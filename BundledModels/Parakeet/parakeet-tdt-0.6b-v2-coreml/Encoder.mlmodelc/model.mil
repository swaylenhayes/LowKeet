program(1.0)
[buildInfo = dict<tensor<string, []>, tensor<string, []>>({{"coremlc-component-MIL", "3500.14.1"}, {"coremlc-version", "3500.32.1"}})]
{
    func main<ios17>(tensor<fp32, [1, 128, 1501]> mel, tensor<int32, [1]> mel_length) {
            tensor<int32, []> var_30 = const()[name = tensor<string, []>("op_30"), val = tensor<int32, []>(-1)];
            tensor<int32, [3]> x_1_perm_0 = const()[name = tensor<string, []>("x_1_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> mel_to_fp16_dtype_0 = const()[name = tensor<string, []>("mel_to_fp16_dtype_0"), val = tensor<string, []>("fp16")];
            tensor<string, []> var_86_to_fp16_dtype_0 = const()[name = tensor<string, []>("op_86_to_fp16_dtype_0"), val = tensor<string, []>("fp16")];
            tensor<fp16, []> var_87_promoted_to_fp16 = const()[name = tensor<string, []>("op_87_promoted_to_fp16"), val = tensor<fp16, []>(-0x1p+0)];
            tensor<fp16, [1]> mel_length_to_fp16 = cast(dtype = var_86_to_fp16_dtype_0, x = mel_length)[name = tensor<string, []>("cast_3")];
            tensor<fp16, [1]> var_88_cast_fp16 = add(x = mel_length_to_fp16, y = var_87_promoted_to_fp16)[name = tensor<string, []>("op_88_cast_fp16")];
            tensor<fp16, []> _inversed_90_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_90_y_0_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1]> _inversed_90_cast_fp16 = mul(x = var_88_cast_fp16, y = _inversed_90_y_0_to_fp16)[name = tensor<string, []>("_inversed_90_cast_fp16")];
            tensor<fp16, []> var_91_to_fp16 = const()[name = tensor<string, []>("op_91_to_fp16"), val = tensor<fp16, []>(0x1p+0)];
            tensor<fp16, [1]> lengths_1_cast_fp16 = add(x = _inversed_90_cast_fp16, y = var_91_to_fp16)[name = tensor<string, []>("lengths_1_cast_fp16")];
            tensor<fp16, [1]> lengths_3_cast_fp16 = floor(x = lengths_1_cast_fp16)[name = tensor<string, []>("lengths_3_cast_fp16")];
            tensor<fp16, []> var_95_promoted_to_fp16 = const()[name = tensor<string, []>("op_95_promoted_to_fp16"), val = tensor<fp16, []>(-0x1p+0)];
            tensor<fp16, [1]> var_96_cast_fp16 = add(x = lengths_3_cast_fp16, y = var_95_promoted_to_fp16)[name = tensor<string, []>("op_96_cast_fp16")];
            tensor<fp16, []> _inversed_98_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_98_y_0_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1]> _inversed_98_cast_fp16 = mul(x = var_96_cast_fp16, y = _inversed_98_y_0_to_fp16)[name = tensor<string, []>("_inversed_98_cast_fp16")];
            tensor<fp16, []> var_99_to_fp16 = const()[name = tensor<string, []>("op_99_to_fp16"), val = tensor<fp16, []>(0x1p+0)];
            tensor<fp16, [1]> lengths_7_cast_fp16 = add(x = _inversed_98_cast_fp16, y = var_99_to_fp16)[name = tensor<string, []>("lengths_7_cast_fp16")];
            tensor<fp16, [1]> lengths_9_cast_fp16 = floor(x = lengths_7_cast_fp16)[name = tensor<string, []>("lengths_9_cast_fp16")];
            tensor<fp16, []> var_103_promoted_to_fp16 = const()[name = tensor<string, []>("op_103_promoted_to_fp16"), val = tensor<fp16, []>(-0x1p+0)];
            tensor<fp16, [1]> var_104_cast_fp16 = add(x = lengths_9_cast_fp16, y = var_103_promoted_to_fp16)[name = tensor<string, []>("op_104_cast_fp16")];
            tensor<fp16, []> _inversed_106_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_106_y_0_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1]> _inversed_106_cast_fp16 = mul(x = var_104_cast_fp16, y = _inversed_106_y_0_to_fp16)[name = tensor<string, []>("_inversed_106_cast_fp16")];
            tensor<fp16, []> var_107_to_fp16 = const()[name = tensor<string, []>("op_107_to_fp16"), val = tensor<fp16, []>(0x1p+0)];
            tensor<fp16, [1]> lengths_13_cast_fp16 = add(x = _inversed_106_cast_fp16, y = var_107_to_fp16)[name = tensor<string, []>("lengths_13_cast_fp16")];
            tensor<fp16, [1]> lengths_cast_fp16 = floor(x = lengths_13_cast_fp16)[name = tensor<string, []>("lengths_cast_fp16")];
            tensor<int32, [1]> input_1_axes_0 = const()[name = tensor<string, []>("input_1_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1, 128, 1501]> mel_to_fp16 = cast(dtype = mel_to_fp16_dtype_0, x = mel)[name = tensor<string, []>("cast_2")];
            tensor<fp16, [1, 1501, 128]> x_1_cast_fp16 = transpose(perm = x_1_perm_0, x = mel_to_fp16)[name = tensor<string, []>("transpose_315")];
            tensor<fp16, [1, 1, 1501, 128]> input_1_cast_fp16 = expand_dims(axes = input_1_axes_0, x = x_1_cast_fp16)[name = tensor<string, []>("input_1_cast_fp16")];
            tensor<string, []> input_3_pad_type_0 = const()[name = tensor<string, []>("input_3_pad_type_0"), val = tensor<string, []>("custom")];
            tensor<int32, [4]> input_3_pad_0 = const()[name = tensor<string, []>("input_3_pad_0"), val = tensor<int32, [4]>([1, 1, 1, 1])];
            tensor<int32, [2]> input_3_strides_0 = const()[name = tensor<string, []>("input_3_strides_0"), val = tensor<int32, [2]>([2, 2])];
            tensor<int32, [2]> input_3_dilations_0 = const()[name = tensor<string, []>("input_3_dilations_0"), val = tensor<int32, [2]>([1, 1])];
            tensor<int32, []> input_3_groups_0 = const()[name = tensor<string, []>("input_3_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [256, 1, 3, 3]> module_pre_encode_conv_0_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1856))), name = tensor<string, []>("module_pre_encode_conv_0_weight_to_fp16_palettized"), shape = tensor<uint32, [4]>([256, 1, 3, 3])];
            tensor<fp16, [256]> module_pre_encode_conv_0_bias_to_fp16 = const()[name = tensor<string, []>("module_pre_encode_conv_0_bias_to_fp16"), val = tensor<fp16, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(2048)))];
            tensor<fp16, [1, 256, 751, 64]> input_3_cast_fp16 = conv(bias = module_pre_encode_conv_0_bias_to_fp16, dilations = input_3_dilations_0, groups = input_3_groups_0, pad = input_3_pad_0, pad_type = input_3_pad_type_0, strides = input_3_strides_0, weight = module_pre_encode_conv_0_weight_to_fp16_palettized, x = input_1_cast_fp16)[name = tensor<string, []>("input_3_cast_fp16")];
            tensor<fp16, [1, 256, 751, 64]> input_5_cast_fp16 = relu(x = input_3_cast_fp16)[name = tensor<string, []>("input_5_cast_fp16")];
            tensor<string, []> input_7_pad_type_0 = const()[name = tensor<string, []>("input_7_pad_type_0"), val = tensor<string, []>("custom")];
            tensor<int32, [4]> input_7_pad_0 = const()[name = tensor<string, []>("input_7_pad_0"), val = tensor<int32, [4]>([1, 1, 1, 1])];
            tensor<int32, [2]> input_7_strides_0 = const()[name = tensor<string, []>("input_7_strides_0"), val = tensor<int32, [2]>([2, 2])];
            tensor<int32, []> input_7_groups_0 = const()[name = tensor<string, []>("input_7_groups_0"), val = tensor<int32, []>(256)];
            tensor<int32, [2]> input_7_dilations_0 = const()[name = tensor<string, []>("input_7_dilations_0"), val = tensor<int32, [2]>([1, 1])];
            tensor<fp16, [256, 1, 3, 3]> module_pre_encode_conv_2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(2624))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4416))), name = tensor<string, []>("module_pre_encode_conv_2_weight_to_fp16_palettized"), shape = tensor<uint32, [4]>([256, 1, 3, 3])];
            tensor<fp16, [256]> module_pre_encode_conv_2_bias_to_fp16 = const()[name = tensor<string, []>("module_pre_encode_conv_2_bias_to_fp16"), val = tensor<fp16, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4608)))];
            tensor<fp16, [1, 256, 376, 32]> input_7_cast_fp16 = conv(bias = module_pre_encode_conv_2_bias_to_fp16, dilations = input_7_dilations_0, groups = input_7_groups_0, pad = input_7_pad_0, pad_type = input_7_pad_type_0, strides = input_7_strides_0, weight = module_pre_encode_conv_2_weight_to_fp16_palettized, x = input_5_cast_fp16)[name = tensor<string, []>("input_7_cast_fp16")];
            tensor<string, []> input_9_pad_type_0 = const()[name = tensor<string, []>("input_9_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [2]> input_9_strides_0 = const()[name = tensor<string, []>("input_9_strides_0"), val = tensor<int32, [2]>([1, 1])];
            tensor<int32, [4]> input_9_pad_0 = const()[name = tensor<string, []>("input_9_pad_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [2]> input_9_dilations_0 = const()[name = tensor<string, []>("input_9_dilations_0"), val = tensor<int32, [2]>([1, 1])];
            tensor<int32, []> input_9_groups_0 = const()[name = tensor<string, []>("input_9_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [256, 256, 1, 1]> module_pre_encode_conv_3_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [49152]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5184))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(54400))), name = tensor<string, []>("module_pre_encode_conv_3_weight_to_fp16_palettized"), shape = tensor<uint32, [4]>([256, 256, 1, 1])];
            tensor<fp16, [256]> module_pre_encode_conv_3_bias_to_fp16 = const()[name = tensor<string, []>("module_pre_encode_conv_3_bias_to_fp16"), val = tensor<fp16, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(54592)))];
            tensor<fp16, [1, 256, 376, 32]> input_9_cast_fp16 = conv(bias = module_pre_encode_conv_3_bias_to_fp16, dilations = input_9_dilations_0, groups = input_9_groups_0, pad = input_9_pad_0, pad_type = input_9_pad_type_0, strides = input_9_strides_0, weight = module_pre_encode_conv_3_weight_to_fp16_palettized, x = input_7_cast_fp16)[name = tensor<string, []>("input_9_cast_fp16")];
            tensor<fp16, [1, 256, 376, 32]> input_11_cast_fp16 = relu(x = input_9_cast_fp16)[name = tensor<string, []>("input_11_cast_fp16")];
            tensor<string, []> input_13_pad_type_0 = const()[name = tensor<string, []>("input_13_pad_type_0"), val = tensor<string, []>("custom")];
            tensor<int32, [4]> input_13_pad_0 = const()[name = tensor<string, []>("input_13_pad_0"), val = tensor<int32, [4]>([1, 1, 1, 1])];
            tensor<int32, [2]> input_13_strides_0 = const()[name = tensor<string, []>("input_13_strides_0"), val = tensor<int32, [2]>([2, 2])];
            tensor<int32, []> input_13_groups_0 = const()[name = tensor<string, []>("input_13_groups_0"), val = tensor<int32, []>(256)];
            tensor<int32, [2]> input_13_dilations_0 = const()[name = tensor<string, []>("input_13_dilations_0"), val = tensor<int32, [2]>([1, 1])];
            tensor<fp16, [256, 1, 3, 3]> module_pre_encode_conv_5_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(55168))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56960))), name = tensor<string, []>("module_pre_encode_conv_5_weight_to_fp16_palettized"), shape = tensor<uint32, [4]>([256, 1, 3, 3])];
            tensor<fp16, [256]> module_pre_encode_conv_5_bias_to_fp16 = const()[name = tensor<string, []>("module_pre_encode_conv_5_bias_to_fp16"), val = tensor<fp16, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(57152)))];
            tensor<fp16, [1, 256, 188, 16]> input_13_cast_fp16 = conv(bias = module_pre_encode_conv_5_bias_to_fp16, dilations = input_13_dilations_0, groups = input_13_groups_0, pad = input_13_pad_0, pad_type = input_13_pad_type_0, strides = input_13_strides_0, weight = module_pre_encode_conv_5_weight_to_fp16_palettized, x = input_11_cast_fp16)[name = tensor<string, []>("input_13_cast_fp16")];
            tensor<string, []> input_15_pad_type_0 = const()[name = tensor<string, []>("input_15_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [2]> input_15_strides_0 = const()[name = tensor<string, []>("input_15_strides_0"), val = tensor<int32, [2]>([1, 1])];
            tensor<int32, [4]> input_15_pad_0 = const()[name = tensor<string, []>("input_15_pad_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [2]> input_15_dilations_0 = const()[name = tensor<string, []>("input_15_dilations_0"), val = tensor<int32, [2]>([1, 1])];
            tensor<int32, []> input_15_groups_0 = const()[name = tensor<string, []>("input_15_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [256, 256, 1, 1]> module_pre_encode_conv_6_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [49152]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(57728))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(106944))), name = tensor<string, []>("module_pre_encode_conv_6_weight_to_fp16_palettized"), shape = tensor<uint32, [4]>([256, 256, 1, 1])];
            tensor<fp16, [256]> module_pre_encode_conv_6_bias_to_fp16 = const()[name = tensor<string, []>("module_pre_encode_conv_6_bias_to_fp16"), val = tensor<fp16, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(107136)))];
            tensor<fp16, [1, 256, 188, 16]> input_15_cast_fp16 = conv(bias = module_pre_encode_conv_6_bias_to_fp16, dilations = input_15_dilations_0, groups = input_15_groups_0, pad = input_15_pad_0, pad_type = input_15_pad_type_0, strides = input_15_strides_0, weight = module_pre_encode_conv_6_weight_to_fp16_palettized, x = input_13_cast_fp16)[name = tensor<string, []>("input_15_cast_fp16")];
            tensor<fp16, [1, 256, 188, 16]> x_3_cast_fp16 = relu(x = input_15_cast_fp16)[name = tensor<string, []>("x_3_cast_fp16")];
            tensor<int32, [4]> var_157_perm_0 = const()[name = tensor<string, []>("op_157_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_158 = const()[name = tensor<string, []>("op_158"), val = tensor<int32, [3]>([1, 188, -1])];
            tensor<fp16, [1, 188, 256, 16]> var_157_cast_fp16 = transpose(perm = var_157_perm_0, x = x_3_cast_fp16)[name = tensor<string, []>("transpose_314")];
            tensor<fp16, [1, 188, 4096]> input_17_cast_fp16 = reshape(shape = var_158, x = var_157_cast_fp16)[name = tensor<string, []>("input_17_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_pre_encode_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(107712))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(3253504))), name = tensor<string, []>("module_pre_encode_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1024]> module_pre_encode_out_bias_to_fp16 = const()[name = tensor<string, []>("module_pre_encode_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(3253696)))];
            tensor<fp16, [1, 188, 1024]> linear_0_cast_fp16 = linear(bias = module_pre_encode_out_bias_to_fp16, weight = module_pre_encode_out_weight_to_fp16_palettized, x = input_17_cast_fp16)[name = tensor<string, []>("linear_0_cast_fp16")];
            tensor<string, []> padding_length_dtype_0 = const()[name = tensor<string, []>("padding_length_dtype_0"), val = tensor<string, []>("int32")];
            tensor<int32, [1, 188]> expand_dims_0 = const()[name = tensor<string, []>("expand_dims_0"), val = tensor<int32, [1, 188]>([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]])];
            tensor<int32, [1]> var_196_axes_0 = const()[name = tensor<string, []>("op_196_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<int32, [1]> encoder_length = cast(dtype = padding_length_dtype_0, x = lengths_cast_fp16)[name = tensor<string, []>("cast_1")];
            tensor<int32, [1, 1]> var_196 = expand_dims(axes = var_196_axes_0, x = encoder_length)[name = tensor<string, []>("op_196")];
            tensor<bool, [1, 188]> pad_mask_1 = less(x = expand_dims_0, y = var_196)[name = tensor<string, []>("pad_mask_1")];
            tensor<int32, [1]> var_198_axes_0 = const()[name = tensor<string, []>("op_198_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<bool, [1, 1, 188]> var_198 = expand_dims(axes = var_198_axes_0, x = pad_mask_1)[name = tensor<string, []>("op_198")];
            tensor<int32, [3]> var_199 = const()[name = tensor<string, []>("op_199"), val = tensor<int32, [3]>([1, 188, 1])];
            tensor<bool, [1, 188, 188]> pad_mask_for_att_mask_1 = tile(reps = var_199, x = var_198)[name = tensor<string, []>("pad_mask_for_att_mask_1")];
            tensor<int32, [3]> var_201_perm_0 = const()[name = tensor<string, []>("op_201_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<bool, [1, 188, 188]> var_201 = transpose(perm = var_201_perm_0, x = pad_mask_for_att_mask_1)[name = tensor<string, []>("transpose_313")];
            tensor<bool, [1, 188, 188]> pad_mask_for_att_mask = logical_and(x = pad_mask_for_att_mask_1, y = var_201)[name = tensor<string, []>("pad_mask_for_att_mask")];
            tensor<bool, [1, 188, 188]> const_7 = const()[name = tensor<string, []>("const_7"), val = tensor<bool, [1, 188, 188]>([[[true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true]]])];
            tensor<bool, [1, 188, 188]> att_mask = logical_and(x = pad_mask_for_att_mask, y = const_7)[name = tensor<string, []>("att_mask")];
            tensor<bool, [1, 188, 188]> mask_1 = logical_not(x = att_mask)[name = tensor<string, []>("mask_1")];
            tensor<bool, [1, 188]> pad_mask = logical_not(x = pad_mask_1)[name = tensor<string, []>("pad_mask")];
            tensor<int32, [1]> input_21_axes_0 = const()[name = tensor<string, []>("input_21_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_0_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_0_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(3255808)))];
            tensor<fp16, [1024]> module_layers_0_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_0_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(3257920)))];
            tensor<fp16, []> var_9_to_fp16 = const()[name = tensor<string, []>("op_9_to_fp16"), val = tensor<fp16, []>(0x1.5p-17)];
            tensor<fp16, [1, 188, 1024]> input_21_cast_fp16 = layer_norm(axes = input_21_axes_0, beta = module_layers_0_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_0_norm_feed_forward1_weight_to_fp16, x = linear_0_cast_fp16)[name = tensor<string, []>("input_21_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_0_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(3260032))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6405824))), name = tensor<string, []>("module_layers_0_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [4096]> linear_1_bias_0_to_fp16 = const()[name = tensor<string, []>("linear_1_bias_0_to_fp16"), val = tensor<fp16, [4096]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6406016)))];
            tensor<fp16, [1, 188, 4096]> linear_1_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_0_feed_forward1_linear1_weight_to_fp16_palettized, x = input_21_cast_fp16)[name = tensor<string, []>("linear_1_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_25_cast_fp16 = silu(x = linear_1_cast_fp16)[name = tensor<string, []>("input_25_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_0_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6414272))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9560064))), name = tensor<string, []>("module_layers_0_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1024]> linear_2_bias_0_to_fp16 = const()[name = tensor<string, []>("linear_2_bias_0_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9560256)))];
            tensor<fp16, [1, 188, 1024]> linear_2_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_0_feed_forward1_linear2_weight_to_fp16_palettized, x = input_25_cast_fp16)[name = tensor<string, []>("linear_2_cast_fp16")];
            tensor<fp16, []> var_232_to_fp16 = const()[name = tensor<string, []>("op_232_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_233_cast_fp16 = mul(x = linear_2_cast_fp16, y = var_232_to_fp16)[name = tensor<string, []>("op_233_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_31_cast_fp16 = add(x = linear_0_cast_fp16, y = var_233_cast_fp16)[name = tensor<string, []>("input_31_cast_fp16")];
            tensor<int32, [1]> query_1_axes_0 = const()[name = tensor<string, []>("query_1_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_0_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_0_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9562368)))];
            tensor<fp16, [1024]> module_layers_0_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_0_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9564480)))];
            tensor<fp16, [1, 188, 1024]> query_1_cast_fp16 = layer_norm(axes = query_1_axes_0, beta = module_layers_0_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_0_norm_self_att_weight_to_fp16, x = input_31_cast_fp16)[name = tensor<string, []>("query_1_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_0_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9566592))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10353088))), name = tensor<string, []>("module_layers_0_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_3_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_0_self_attn_linear_q_weight_to_fp16_palettized, x = query_1_cast_fp16)[name = tensor<string, []>("linear_3_cast_fp16")];
            tensor<int32, [4]> var_249 = const()[name = tensor<string, []>("op_249"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_1_cast_fp16 = reshape(shape = var_249, x = linear_3_cast_fp16)[name = tensor<string, []>("q_1_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_0_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10353280))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11139776))), name = tensor<string, []>("module_layers_0_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_4_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_0_self_attn_linear_k_weight_to_fp16_palettized, x = query_1_cast_fp16)[name = tensor<string, []>("linear_4_cast_fp16")];
            tensor<int32, [4]> var_253 = const()[name = tensor<string, []>("op_253"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_1_cast_fp16 = reshape(shape = var_253, x = linear_4_cast_fp16)[name = tensor<string, []>("k_1_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_0_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11139968))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11926464))), name = tensor<string, []>("module_layers_0_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_5_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_0_self_attn_linear_v_weight_to_fp16_palettized, x = query_1_cast_fp16)[name = tensor<string, []>("linear_5_cast_fp16")];
            tensor<int32, [4]> var_257 = const()[name = tensor<string, []>("op_257"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_1_cast_fp16 = reshape(shape = var_257, x = linear_5_cast_fp16)[name = tensor<string, []>("v_1_cast_fp16")];
            tensor<int32, [4]> value_3_perm_0 = const()[name = tensor<string, []>("value_3_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_0_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_0_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11926656)))];
            tensor<fp16, [1, 188, 8, 128]> var_269_cast_fp16 = add(x = q_1_cast_fp16, y = module_layers_0_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_269_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_0_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_0_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11928768)))];
            tensor<fp16, [1, 188, 8, 128]> var_271_cast_fp16 = add(x = q_1_cast_fp16, y = module_layers_0_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_271_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_1_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_1_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_7_transpose_x_0 = const()[name = tensor<string, []>("x_7_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_7_transpose_y_0 = const()[name = tensor<string, []>("x_7_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_273_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11930880))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12218944))), name = tensor<string, []>("op_273_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_1_cast_fp16 = transpose(perm = q_with_bias_v_1_perm_0, x = var_271_cast_fp16)[name = tensor<string, []>("transpose_312")];
            tensor<fp16, [1, 8, 188, 375]> x_7_cast_fp16 = matmul(transpose_x = x_7_transpose_x_0, transpose_y = x_7_transpose_y_0, x = q_with_bias_v_1_cast_fp16, y = op_273_to_fp16_palettized)[name = tensor<string, []>("x_7_cast_fp16")];
            tensor<int32, [8]> x_9_pad_0 = const()[name = tensor<string, []>("x_9_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_9_mode_0 = const()[name = tensor<string, []>("x_9_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_14_to_fp16 = const()[name = tensor<string, []>("const_14_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_9_cast_fp16 = pad(constant_val = const_14_to_fp16, mode = x_9_mode_0, pad = x_9_pad_0, x = x_7_cast_fp16)[name = tensor<string, []>("x_9_cast_fp16")];
            tensor<int32, [4]> var_281 = const()[name = tensor<string, []>("op_281"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_11_cast_fp16 = reshape(shape = var_281, x = x_9_cast_fp16)[name = tensor<string, []>("x_11_cast_fp16")];
            tensor<int32, [4]> var_285_begin_0 = const()[name = tensor<string, []>("op_285_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_285_end_0 = const()[name = tensor<string, []>("op_285_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_285_end_mask_0 = const()[name = tensor<string, []>("op_285_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_285_cast_fp16 = slice_by_index(begin = var_285_begin_0, end = var_285_end_0, end_mask = var_285_end_mask_0, x = x_11_cast_fp16)[name = tensor<string, []>("op_285_cast_fp16")];
            tensor<int32, [4]> var_286 = const()[name = tensor<string, []>("op_286"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_1_cast_fp16 = reshape(shape = var_286, x = var_285_cast_fp16)[name = tensor<string, []>("matrix_bd_1_cast_fp16")];
            tensor<bool, []> matrix_ac_1_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_1_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_1_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_96_perm_0 = const()[name = tensor<string, []>("transpose_96_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_97_perm_0 = const()[name = tensor<string, []>("transpose_97_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_97 = transpose(perm = transpose_97_perm_0, x = k_1_cast_fp16)[name = tensor<string, []>("transpose_310")];
            tensor<fp16, [1, 8, 188, 128]> transpose_96 = transpose(perm = transpose_96_perm_0, x = var_269_cast_fp16)[name = tensor<string, []>("transpose_311")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_1_cast_fp16 = matmul(transpose_x = matrix_ac_1_transpose_x_0, transpose_y = matrix_ac_1_transpose_y_0, x = transpose_96, y = transpose_97)[name = tensor<string, []>("matrix_ac_1_cast_fp16")];
            tensor<int32, [4]> matrix_bd_3_begin_0 = const()[name = tensor<string, []>("matrix_bd_3_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_3_end_0 = const()[name = tensor<string, []>("matrix_bd_3_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_3_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_3_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_3_cast_fp16 = slice_by_index(begin = matrix_bd_3_begin_0, end = matrix_bd_3_end_0, end_mask = matrix_bd_3_end_mask_0, x = matrix_bd_1_cast_fp16)[name = tensor<string, []>("matrix_bd_3_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_295_cast_fp16 = add(x = matrix_ac_1_cast_fp16, y = matrix_bd_3_cast_fp16)[name = tensor<string, []>("op_295_cast_fp16")];
            tensor<fp16, []> _inversed_scores_1_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_1_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_1_cast_fp16 = mul(x = var_295_cast_fp16, y = _inversed_scores_1_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_1_cast_fp16")];
            tensor<int32, [1]> mask_3_axes_0 = const()[name = tensor<string, []>("mask_3_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<bool, [1, 1, 188, 188]> mask_3 = expand_dims(axes = mask_3_axes_0, x = mask_1)[name = tensor<string, []>("mask_3")];
            tensor<fp16, []> var_12_to_fp16 = const()[name = tensor<string, []>("op_12_to_fp16"), val = tensor<fp16, []>(-0x1.388p+13)];
            tensor<fp16, [1, 8, 188, 188]> scores_3_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_1_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_3_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_301_cast_fp16 = softmax(axis = var_30, x = scores_3_cast_fp16)[name = tensor<string, []>("op_301_cast_fp16")];
            tensor<fp16, []> var_11_to_fp16 = const()[name = tensor<string, []>("op_11_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 188]> input_33_cast_fp16 = select(a = var_11_to_fp16, b = var_301_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_33_cast_fp16")];
            tensor<bool, []> x_13_transpose_x_0 = const()[name = tensor<string, []>("x_13_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_13_transpose_y_0 = const()[name = tensor<string, []>("x_13_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_3_cast_fp16 = transpose(perm = value_3_perm_0, x = v_1_cast_fp16)[name = tensor<string, []>("transpose_309")];
            tensor<fp16, [1, 8, 188, 128]> x_13_cast_fp16 = matmul(transpose_x = x_13_transpose_x_0, transpose_y = x_13_transpose_y_0, x = input_33_cast_fp16, y = value_3_cast_fp16)[name = tensor<string, []>("x_13_cast_fp16")];
            tensor<int32, [4]> var_305_perm_0 = const()[name = tensor<string, []>("op_305_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_306 = const()[name = tensor<string, []>("op_306"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_305_cast_fp16 = transpose(perm = var_305_perm_0, x = x_13_cast_fp16)[name = tensor<string, []>("transpose_308")];
            tensor<fp16, [1, 188, 1024]> input_35_cast_fp16 = reshape(shape = var_306, x = var_305_cast_fp16)[name = tensor<string, []>("input_35_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_0_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12219136))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13005632))), name = tensor<string, []>("module_layers_0_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_7_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_0_self_attn_linear_out_weight_to_fp16_palettized, x = input_35_cast_fp16)[name = tensor<string, []>("linear_7_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_39_cast_fp16 = add(x = input_31_cast_fp16, y = linear_7_cast_fp16)[name = tensor<string, []>("input_39_cast_fp16")];
            tensor<int32, [1]> x_17_axes_0 = const()[name = tensor<string, []>("x_17_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_0_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_0_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13005824)))];
            tensor<fp16, [1024]> module_layers_0_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_0_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13007936)))];
            tensor<fp16, [1, 188, 1024]> x_17_cast_fp16 = layer_norm(axes = x_17_axes_0, beta = module_layers_0_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_0_norm_conv_weight_to_fp16, x = input_39_cast_fp16)[name = tensor<string, []>("x_17_cast_fp16")];
            tensor<int32, [3]> input_41_perm_0 = const()[name = tensor<string, []>("input_41_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_43_pad_type_0 = const()[name = tensor<string, []>("input_43_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_43_strides_0 = const()[name = tensor<string, []>("input_43_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_43_pad_0 = const()[name = tensor<string, []>("input_43_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_43_dilations_0 = const()[name = tensor<string, []>("input_43_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_43_groups_0 = const()[name = tensor<string, []>("input_43_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_0_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13010048))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(14582976))), name = tensor<string, []>("module_layers_0_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_41_cast_fp16 = transpose(perm = input_41_perm_0, x = x_17_cast_fp16)[name = tensor<string, []>("transpose_307")];
            tensor<fp16, [1, 2048, 188]> input_43_cast_fp16 = conv(dilations = input_43_dilations_0, groups = input_43_groups_0, pad = input_43_pad_0, pad_type = input_43_pad_type_0, strides = input_43_strides_0, weight = module_layers_0_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_41_cast_fp16)[name = tensor<string, []>("input_43_cast_fp16")];
            tensor<int32, []> x_19_split_num_splits_0 = const()[name = tensor<string, []>("x_19_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_19_split_axis_0 = const()[name = tensor<string, []>("x_19_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_19_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_19_split_cast_fp16_1 = split(axis = x_19_split_axis_0, num_splits = x_19_split_num_splits_0, x = input_43_cast_fp16)[name = tensor<string, []>("x_19_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_19_split_1_sigmoid_cast_fp16 = sigmoid(x = x_19_split_cast_fp16_1)[name = tensor<string, []>("x_19_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_19_cast_fp16 = mul(x = x_19_split_cast_fp16_0, y = x_19_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_19_cast_fp16")];
            tensor<int32, [1]> var_328_axes_0 = const()[name = tensor<string, []>("op_328_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<bool, [1, 1, 188]> var_328 = expand_dims(axes = var_328_axes_0, x = pad_mask)[name = tensor<string, []>("op_328")];
            tensor<fp16, [1, 1024, 188]> input_45_cast_fp16 = select(a = var_11_to_fp16, b = x_19_cast_fp16, cond = var_328)[name = tensor<string, []>("input_45_cast_fp16")];
            tensor<int32, [6]> input_47_pad_0 = const()[name = tensor<string, []>("input_47_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_47_mode_0 = const()[name = tensor<string, []>("input_47_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_17_to_fp16 = const()[name = tensor<string, []>("const_17_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_47_cast_fp16 = pad(constant_val = const_17_to_fp16, mode = input_47_mode_0, pad = input_47_pad_0, x = input_45_cast_fp16)[name = tensor<string, []>("input_47_cast_fp16")];
            tensor<string, []> input_49_pad_type_0 = const()[name = tensor<string, []>("input_49_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_49_groups_0 = const()[name = tensor<string, []>("input_49_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_49_strides_0 = const()[name = tensor<string, []>("input_49_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_49_pad_0 = const()[name = tensor<string, []>("input_49_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_49_dilations_0 = const()[name = tensor<string, []>("input_49_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_248_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(14583168))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(14590144))), name = tensor<string, []>("const_248_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_249_to_fp16 = const()[name = tensor<string, []>("const_249_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(14590336)))];
            tensor<fp16, [1, 1024, 188]> input_51_cast_fp16 = conv(bias = const_249_to_fp16, dilations = input_49_dilations_0, groups = input_49_groups_0, pad = input_49_pad_0, pad_type = input_49_pad_type_0, strides = input_49_strides_0, weight = const_248_to_fp16_palettized, x = input_47_cast_fp16)[name = tensor<string, []>("input_51_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_53_cast_fp16 = silu(x = input_51_cast_fp16)[name = tensor<string, []>("input_53_cast_fp16")];
            tensor<string, []> x_21_pad_type_0 = const()[name = tensor<string, []>("x_21_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_21_strides_0 = const()[name = tensor<string, []>("x_21_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_21_pad_0 = const()[name = tensor<string, []>("x_21_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_21_dilations_0 = const()[name = tensor<string, []>("x_21_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_21_groups_0 = const()[name = tensor<string, []>("x_21_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_0_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(14592448))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(15378944))), name = tensor<string, []>("module_layers_0_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_21_cast_fp16 = conv(dilations = x_21_dilations_0, groups = x_21_groups_0, pad = x_21_pad_0, pad_type = x_21_pad_type_0, strides = x_21_strides_0, weight = module_layers_0_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_53_cast_fp16)[name = tensor<string, []>("x_21_cast_fp16")];
            tensor<int32, [3]> input_55_perm_0 = const()[name = tensor<string, []>("input_55_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_55_cast_fp16 = transpose(perm = input_55_perm_0, x = x_21_cast_fp16)[name = tensor<string, []>("transpose_306")];
            tensor<fp16, [1, 188, 1024]> input_57_cast_fp16 = add(x = input_39_cast_fp16, y = input_55_cast_fp16)[name = tensor<string, []>("input_57_cast_fp16")];
            tensor<int32, [1]> input_59_axes_0 = const()[name = tensor<string, []>("input_59_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_0_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_0_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(15379136)))];
            tensor<fp16, [1024]> module_layers_0_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_0_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(15381248)))];
            tensor<fp16, [1, 188, 1024]> input_59_cast_fp16 = layer_norm(axes = input_59_axes_0, beta = module_layers_0_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_0_norm_feed_forward2_weight_to_fp16, x = input_57_cast_fp16)[name = tensor<string, []>("input_59_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_0_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(15383360))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(18529152))), name = tensor<string, []>("module_layers_0_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_8_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_0_feed_forward2_linear1_weight_to_fp16_palettized, x = input_59_cast_fp16)[name = tensor<string, []>("linear_8_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_63_cast_fp16 = silu(x = linear_8_cast_fp16)[name = tensor<string, []>("input_63_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_0_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(18529344))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(21675136))), name = tensor<string, []>("module_layers_0_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_9_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_0_feed_forward2_linear2_weight_to_fp16_palettized, x = input_63_cast_fp16)[name = tensor<string, []>("linear_9_cast_fp16")];
            tensor<fp16, []> var_366_to_fp16 = const()[name = tensor<string, []>("op_366_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_367_cast_fp16 = mul(x = linear_9_cast_fp16, y = var_366_to_fp16)[name = tensor<string, []>("op_367_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_69_cast_fp16 = add(x = input_57_cast_fp16, y = var_367_cast_fp16)[name = tensor<string, []>("input_69_cast_fp16")];
            tensor<int32, [1]> input_71_axes_0 = const()[name = tensor<string, []>("input_71_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_0_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_0_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(21675328)))];
            tensor<fp16, [1024]> module_layers_0_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_0_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(21677440)))];
            tensor<fp16, [1, 188, 1024]> input_71_cast_fp16 = layer_norm(axes = input_71_axes_0, beta = module_layers_0_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_0_norm_out_weight_to_fp16, x = input_69_cast_fp16)[name = tensor<string, []>("input_71_cast_fp16")];
            tensor<int32, [1]> input_73_axes_0 = const()[name = tensor<string, []>("input_73_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_1_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_1_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(21679552)))];
            tensor<fp16, [1024]> module_layers_1_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_1_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(21681664)))];
            tensor<fp16, [1, 188, 1024]> input_73_cast_fp16 = layer_norm(axes = input_73_axes_0, beta = module_layers_1_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_1_norm_feed_forward1_weight_to_fp16, x = input_71_cast_fp16)[name = tensor<string, []>("input_73_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_1_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(21683776))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24829568))), name = tensor<string, []>("module_layers_1_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_10_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_1_feed_forward1_linear1_weight_to_fp16_palettized, x = input_73_cast_fp16)[name = tensor<string, []>("linear_10_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_77_cast_fp16 = silu(x = linear_10_cast_fp16)[name = tensor<string, []>("input_77_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_1_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24829760))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27975552))), name = tensor<string, []>("module_layers_1_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_11_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_1_feed_forward1_linear2_weight_to_fp16_palettized, x = input_77_cast_fp16)[name = tensor<string, []>("linear_11_cast_fp16")];
            tensor<fp16, []> var_395_to_fp16 = const()[name = tensor<string, []>("op_395_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_396_cast_fp16 = mul(x = linear_11_cast_fp16, y = var_395_to_fp16)[name = tensor<string, []>("op_396_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_83_cast_fp16 = add(x = input_71_cast_fp16, y = var_396_cast_fp16)[name = tensor<string, []>("input_83_cast_fp16")];
            tensor<int32, [1]> query_3_axes_0 = const()[name = tensor<string, []>("query_3_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_1_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_1_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27975744)))];
            tensor<fp16, [1024]> module_layers_1_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_1_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27977856)))];
            tensor<fp16, [1, 188, 1024]> query_3_cast_fp16 = layer_norm(axes = query_3_axes_0, beta = module_layers_1_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_1_norm_self_att_weight_to_fp16, x = input_83_cast_fp16)[name = tensor<string, []>("query_3_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_1_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27979968))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28766464))), name = tensor<string, []>("module_layers_1_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_12_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_1_self_attn_linear_q_weight_to_fp16_palettized, x = query_3_cast_fp16)[name = tensor<string, []>("linear_12_cast_fp16")];
            tensor<int32, [4]> var_412 = const()[name = tensor<string, []>("op_412"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_7_cast_fp16 = reshape(shape = var_412, x = linear_12_cast_fp16)[name = tensor<string, []>("q_7_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_1_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28766656))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(29553152))), name = tensor<string, []>("module_layers_1_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_13_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_1_self_attn_linear_k_weight_to_fp16_palettized, x = query_3_cast_fp16)[name = tensor<string, []>("linear_13_cast_fp16")];
            tensor<int32, [4]> var_416 = const()[name = tensor<string, []>("op_416"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_5_cast_fp16 = reshape(shape = var_416, x = linear_13_cast_fp16)[name = tensor<string, []>("k_5_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_1_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(29553344))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30339840))), name = tensor<string, []>("module_layers_1_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_14_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_1_self_attn_linear_v_weight_to_fp16_palettized, x = query_3_cast_fp16)[name = tensor<string, []>("linear_14_cast_fp16")];
            tensor<int32, [4]> var_420 = const()[name = tensor<string, []>("op_420"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_3_cast_fp16 = reshape(shape = var_420, x = linear_14_cast_fp16)[name = tensor<string, []>("v_3_cast_fp16")];
            tensor<int32, [4]> value_5_perm_0 = const()[name = tensor<string, []>("value_5_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_1_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_1_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30340032)))];
            tensor<fp16, [1, 188, 8, 128]> var_432_cast_fp16 = add(x = q_7_cast_fp16, y = module_layers_1_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_432_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_1_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_1_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30342144)))];
            tensor<fp16, [1, 188, 8, 128]> var_434_cast_fp16 = add(x = q_7_cast_fp16, y = module_layers_1_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_434_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_3_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_3_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_29_transpose_x_0 = const()[name = tensor<string, []>("x_29_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_29_transpose_y_0 = const()[name = tensor<string, []>("x_29_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_436_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30344256))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30632320))), name = tensor<string, []>("op_436_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_3_cast_fp16 = transpose(perm = q_with_bias_v_3_perm_0, x = var_434_cast_fp16)[name = tensor<string, []>("transpose_305")];
            tensor<fp16, [1, 8, 188, 375]> x_29_cast_fp16 = matmul(transpose_x = x_29_transpose_x_0, transpose_y = x_29_transpose_y_0, x = q_with_bias_v_3_cast_fp16, y = op_436_to_fp16_palettized)[name = tensor<string, []>("x_29_cast_fp16")];
            tensor<int32, [8]> x_31_pad_0 = const()[name = tensor<string, []>("x_31_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_31_mode_0 = const()[name = tensor<string, []>("x_31_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_24_to_fp16 = const()[name = tensor<string, []>("const_24_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_31_cast_fp16 = pad(constant_val = const_24_to_fp16, mode = x_31_mode_0, pad = x_31_pad_0, x = x_29_cast_fp16)[name = tensor<string, []>("x_31_cast_fp16")];
            tensor<int32, [4]> var_444 = const()[name = tensor<string, []>("op_444"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_33_cast_fp16 = reshape(shape = var_444, x = x_31_cast_fp16)[name = tensor<string, []>("x_33_cast_fp16")];
            tensor<int32, [4]> var_448_begin_0 = const()[name = tensor<string, []>("op_448_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_448_end_0 = const()[name = tensor<string, []>("op_448_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_448_end_mask_0 = const()[name = tensor<string, []>("op_448_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_448_cast_fp16 = slice_by_index(begin = var_448_begin_0, end = var_448_end_0, end_mask = var_448_end_mask_0, x = x_33_cast_fp16)[name = tensor<string, []>("op_448_cast_fp16")];
            tensor<int32, [4]> var_449 = const()[name = tensor<string, []>("op_449"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_5_cast_fp16 = reshape(shape = var_449, x = var_448_cast_fp16)[name = tensor<string, []>("matrix_bd_5_cast_fp16")];
            tensor<bool, []> matrix_ac_3_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_3_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_3_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_3_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_98_perm_0 = const()[name = tensor<string, []>("transpose_98_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_99_perm_0 = const()[name = tensor<string, []>("transpose_99_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_99 = transpose(perm = transpose_99_perm_0, x = k_5_cast_fp16)[name = tensor<string, []>("transpose_303")];
            tensor<fp16, [1, 8, 188, 128]> transpose_98 = transpose(perm = transpose_98_perm_0, x = var_432_cast_fp16)[name = tensor<string, []>("transpose_304")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_3_cast_fp16 = matmul(transpose_x = matrix_ac_3_transpose_x_0, transpose_y = matrix_ac_3_transpose_y_0, x = transpose_98, y = transpose_99)[name = tensor<string, []>("matrix_ac_3_cast_fp16")];
            tensor<int32, [4]> matrix_bd_7_begin_0 = const()[name = tensor<string, []>("matrix_bd_7_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_7_end_0 = const()[name = tensor<string, []>("matrix_bd_7_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_7_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_7_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_7_cast_fp16 = slice_by_index(begin = matrix_bd_7_begin_0, end = matrix_bd_7_end_0, end_mask = matrix_bd_7_end_mask_0, x = matrix_bd_5_cast_fp16)[name = tensor<string, []>("matrix_bd_7_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_458_cast_fp16 = add(x = matrix_ac_3_cast_fp16, y = matrix_bd_7_cast_fp16)[name = tensor<string, []>("op_458_cast_fp16")];
            tensor<fp16, []> _inversed_scores_5_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_5_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_5_cast_fp16 = mul(x = var_458_cast_fp16, y = _inversed_scores_5_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_5_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_7_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_5_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_7_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_464_cast_fp16 = softmax(axis = var_30, x = scores_7_cast_fp16)[name = tensor<string, []>("op_464_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_85_cast_fp16 = select(a = var_11_to_fp16, b = var_464_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_85_cast_fp16")];
            tensor<bool, []> x_35_transpose_x_0 = const()[name = tensor<string, []>("x_35_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_35_transpose_y_0 = const()[name = tensor<string, []>("x_35_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_5_cast_fp16 = transpose(perm = value_5_perm_0, x = v_3_cast_fp16)[name = tensor<string, []>("transpose_302")];
            tensor<fp16, [1, 8, 188, 128]> x_35_cast_fp16 = matmul(transpose_x = x_35_transpose_x_0, transpose_y = x_35_transpose_y_0, x = input_85_cast_fp16, y = value_5_cast_fp16)[name = tensor<string, []>("x_35_cast_fp16")];
            tensor<int32, [4]> var_468_perm_0 = const()[name = tensor<string, []>("op_468_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_469 = const()[name = tensor<string, []>("op_469"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_468_cast_fp16 = transpose(perm = var_468_perm_0, x = x_35_cast_fp16)[name = tensor<string, []>("transpose_301")];
            tensor<fp16, [1, 188, 1024]> input_87_cast_fp16 = reshape(shape = var_469, x = var_468_cast_fp16)[name = tensor<string, []>("input_87_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_1_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30632512))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31419008))), name = tensor<string, []>("module_layers_1_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_16_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_1_self_attn_linear_out_weight_to_fp16_palettized, x = input_87_cast_fp16)[name = tensor<string, []>("linear_16_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_91_cast_fp16 = add(x = input_83_cast_fp16, y = linear_16_cast_fp16)[name = tensor<string, []>("input_91_cast_fp16")];
            tensor<int32, [1]> x_39_axes_0 = const()[name = tensor<string, []>("x_39_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_1_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_1_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31419200)))];
            tensor<fp16, [1024]> module_layers_1_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_1_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31421312)))];
            tensor<fp16, [1, 188, 1024]> x_39_cast_fp16 = layer_norm(axes = x_39_axes_0, beta = module_layers_1_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_1_norm_conv_weight_to_fp16, x = input_91_cast_fp16)[name = tensor<string, []>("x_39_cast_fp16")];
            tensor<int32, [3]> input_93_perm_0 = const()[name = tensor<string, []>("input_93_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_95_pad_type_0 = const()[name = tensor<string, []>("input_95_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_95_strides_0 = const()[name = tensor<string, []>("input_95_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_95_pad_0 = const()[name = tensor<string, []>("input_95_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_95_dilations_0 = const()[name = tensor<string, []>("input_95_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_95_groups_0 = const()[name = tensor<string, []>("input_95_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_1_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31423424))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32996352))), name = tensor<string, []>("module_layers_1_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_93_cast_fp16 = transpose(perm = input_93_perm_0, x = x_39_cast_fp16)[name = tensor<string, []>("transpose_300")];
            tensor<fp16, [1, 2048, 188]> input_95_cast_fp16 = conv(dilations = input_95_dilations_0, groups = input_95_groups_0, pad = input_95_pad_0, pad_type = input_95_pad_type_0, strides = input_95_strides_0, weight = module_layers_1_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_93_cast_fp16)[name = tensor<string, []>("input_95_cast_fp16")];
            tensor<int32, []> x_41_split_num_splits_0 = const()[name = tensor<string, []>("x_41_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_41_split_axis_0 = const()[name = tensor<string, []>("x_41_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_41_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_41_split_cast_fp16_1 = split(axis = x_41_split_axis_0, num_splits = x_41_split_num_splits_0, x = input_95_cast_fp16)[name = tensor<string, []>("x_41_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_41_split_1_sigmoid_cast_fp16 = sigmoid(x = x_41_split_cast_fp16_1)[name = tensor<string, []>("x_41_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_41_cast_fp16 = mul(x = x_41_split_cast_fp16_0, y = x_41_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_41_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_97_cast_fp16 = select(a = var_11_to_fp16, b = x_41_cast_fp16, cond = var_328)[name = tensor<string, []>("input_97_cast_fp16")];
            tensor<int32, [6]> input_99_pad_0 = const()[name = tensor<string, []>("input_99_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_99_mode_0 = const()[name = tensor<string, []>("input_99_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_27_to_fp16 = const()[name = tensor<string, []>("const_27_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_99_cast_fp16 = pad(constant_val = const_27_to_fp16, mode = input_99_mode_0, pad = input_99_pad_0, x = input_97_cast_fp16)[name = tensor<string, []>("input_99_cast_fp16")];
            tensor<string, []> input_101_pad_type_0 = const()[name = tensor<string, []>("input_101_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_101_groups_0 = const()[name = tensor<string, []>("input_101_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_101_strides_0 = const()[name = tensor<string, []>("input_101_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_101_pad_0 = const()[name = tensor<string, []>("input_101_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_101_dilations_0 = const()[name = tensor<string, []>("input_101_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_250_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32996544))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33003520))), name = tensor<string, []>("const_250_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_251_to_fp16 = const()[name = tensor<string, []>("const_251_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33003712)))];
            tensor<fp16, [1, 1024, 188]> input_103_cast_fp16 = conv(bias = const_251_to_fp16, dilations = input_101_dilations_0, groups = input_101_groups_0, pad = input_101_pad_0, pad_type = input_101_pad_type_0, strides = input_101_strides_0, weight = const_250_to_fp16_palettized, x = input_99_cast_fp16)[name = tensor<string, []>("input_103_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_105_cast_fp16 = silu(x = input_103_cast_fp16)[name = tensor<string, []>("input_105_cast_fp16")];
            tensor<string, []> x_43_pad_type_0 = const()[name = tensor<string, []>("x_43_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_43_strides_0 = const()[name = tensor<string, []>("x_43_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_43_pad_0 = const()[name = tensor<string, []>("x_43_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_43_dilations_0 = const()[name = tensor<string, []>("x_43_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_43_groups_0 = const()[name = tensor<string, []>("x_43_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_1_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33005824))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33792320))), name = tensor<string, []>("module_layers_1_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_43_cast_fp16 = conv(dilations = x_43_dilations_0, groups = x_43_groups_0, pad = x_43_pad_0, pad_type = x_43_pad_type_0, strides = x_43_strides_0, weight = module_layers_1_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_105_cast_fp16)[name = tensor<string, []>("x_43_cast_fp16")];
            tensor<int32, [3]> input_107_perm_0 = const()[name = tensor<string, []>("input_107_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_107_cast_fp16 = transpose(perm = input_107_perm_0, x = x_43_cast_fp16)[name = tensor<string, []>("transpose_299")];
            tensor<fp16, [1, 188, 1024]> input_109_cast_fp16 = add(x = input_91_cast_fp16, y = input_107_cast_fp16)[name = tensor<string, []>("input_109_cast_fp16")];
            tensor<int32, [1]> input_111_axes_0 = const()[name = tensor<string, []>("input_111_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_1_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_1_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33792512)))];
            tensor<fp16, [1024]> module_layers_1_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_1_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33794624)))];
            tensor<fp16, [1, 188, 1024]> input_111_cast_fp16 = layer_norm(axes = input_111_axes_0, beta = module_layers_1_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_1_norm_feed_forward2_weight_to_fp16, x = input_109_cast_fp16)[name = tensor<string, []>("input_111_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_1_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33796736))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36942528))), name = tensor<string, []>("module_layers_1_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_17_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_1_feed_forward2_linear1_weight_to_fp16_palettized, x = input_111_cast_fp16)[name = tensor<string, []>("linear_17_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_115_cast_fp16 = silu(x = linear_17_cast_fp16)[name = tensor<string, []>("input_115_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_1_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36942720))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40088512))), name = tensor<string, []>("module_layers_1_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_18_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_1_feed_forward2_linear2_weight_to_fp16_palettized, x = input_115_cast_fp16)[name = tensor<string, []>("linear_18_cast_fp16")];
            tensor<fp16, []> var_529_to_fp16 = const()[name = tensor<string, []>("op_529_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_530_cast_fp16 = mul(x = linear_18_cast_fp16, y = var_529_to_fp16)[name = tensor<string, []>("op_530_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_121_cast_fp16 = add(x = input_109_cast_fp16, y = var_530_cast_fp16)[name = tensor<string, []>("input_121_cast_fp16")];
            tensor<int32, [1]> input_123_axes_0 = const()[name = tensor<string, []>("input_123_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_1_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_1_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40088704)))];
            tensor<fp16, [1024]> module_layers_1_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_1_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40090816)))];
            tensor<fp16, [1, 188, 1024]> input_123_cast_fp16 = layer_norm(axes = input_123_axes_0, beta = module_layers_1_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_1_norm_out_weight_to_fp16, x = input_121_cast_fp16)[name = tensor<string, []>("input_123_cast_fp16")];
            tensor<int32, [1]> input_125_axes_0 = const()[name = tensor<string, []>("input_125_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_2_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_2_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40092928)))];
            tensor<fp16, [1024]> module_layers_2_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_2_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40095040)))];
            tensor<fp16, [1, 188, 1024]> input_125_cast_fp16 = layer_norm(axes = input_125_axes_0, beta = module_layers_2_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_2_norm_feed_forward1_weight_to_fp16, x = input_123_cast_fp16)[name = tensor<string, []>("input_125_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_2_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40097152))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43242944))), name = tensor<string, []>("module_layers_2_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_19_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_2_feed_forward1_linear1_weight_to_fp16_palettized, x = input_125_cast_fp16)[name = tensor<string, []>("linear_19_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_129_cast_fp16 = silu(x = linear_19_cast_fp16)[name = tensor<string, []>("input_129_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_2_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43243136))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(46388928))), name = tensor<string, []>("module_layers_2_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_20_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_2_feed_forward1_linear2_weight_to_fp16_palettized, x = input_129_cast_fp16)[name = tensor<string, []>("linear_20_cast_fp16")];
            tensor<fp16, []> var_558_to_fp16 = const()[name = tensor<string, []>("op_558_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_559_cast_fp16 = mul(x = linear_20_cast_fp16, y = var_558_to_fp16)[name = tensor<string, []>("op_559_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_135_cast_fp16 = add(x = input_123_cast_fp16, y = var_559_cast_fp16)[name = tensor<string, []>("input_135_cast_fp16")];
            tensor<int32, [1]> query_5_axes_0 = const()[name = tensor<string, []>("query_5_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_2_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_2_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(46389120)))];
            tensor<fp16, [1024]> module_layers_2_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_2_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(46391232)))];
            tensor<fp16, [1, 188, 1024]> query_5_cast_fp16 = layer_norm(axes = query_5_axes_0, beta = module_layers_2_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_2_norm_self_att_weight_to_fp16, x = input_135_cast_fp16)[name = tensor<string, []>("query_5_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_2_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(46393344))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(47179840))), name = tensor<string, []>("module_layers_2_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_21_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_2_self_attn_linear_q_weight_to_fp16_palettized, x = query_5_cast_fp16)[name = tensor<string, []>("linear_21_cast_fp16")];
            tensor<int32, [4]> var_575 = const()[name = tensor<string, []>("op_575"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_13_cast_fp16 = reshape(shape = var_575, x = linear_21_cast_fp16)[name = tensor<string, []>("q_13_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_2_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(47180032))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(47966528))), name = tensor<string, []>("module_layers_2_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_22_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_2_self_attn_linear_k_weight_to_fp16_palettized, x = query_5_cast_fp16)[name = tensor<string, []>("linear_22_cast_fp16")];
            tensor<int32, [4]> var_579 = const()[name = tensor<string, []>("op_579"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_9_cast_fp16 = reshape(shape = var_579, x = linear_22_cast_fp16)[name = tensor<string, []>("k_9_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_2_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(47966720))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48753216))), name = tensor<string, []>("module_layers_2_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_23_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_2_self_attn_linear_v_weight_to_fp16_palettized, x = query_5_cast_fp16)[name = tensor<string, []>("linear_23_cast_fp16")];
            tensor<int32, [4]> var_583 = const()[name = tensor<string, []>("op_583"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_5_cast_fp16 = reshape(shape = var_583, x = linear_23_cast_fp16)[name = tensor<string, []>("v_5_cast_fp16")];
            tensor<int32, [4]> value_7_perm_0 = const()[name = tensor<string, []>("value_7_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_2_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_2_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48753408)))];
            tensor<fp16, [1, 188, 8, 128]> var_595_cast_fp16 = add(x = q_13_cast_fp16, y = module_layers_2_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_595_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_2_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_2_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48755520)))];
            tensor<fp16, [1, 188, 8, 128]> var_597_cast_fp16 = add(x = q_13_cast_fp16, y = module_layers_2_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_597_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_5_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_5_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_51_transpose_x_0 = const()[name = tensor<string, []>("x_51_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_51_transpose_y_0 = const()[name = tensor<string, []>("x_51_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_599_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48757632))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49045696))), name = tensor<string, []>("op_599_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_5_cast_fp16 = transpose(perm = q_with_bias_v_5_perm_0, x = var_597_cast_fp16)[name = tensor<string, []>("transpose_298")];
            tensor<fp16, [1, 8, 188, 375]> x_51_cast_fp16 = matmul(transpose_x = x_51_transpose_x_0, transpose_y = x_51_transpose_y_0, x = q_with_bias_v_5_cast_fp16, y = op_599_to_fp16_palettized)[name = tensor<string, []>("x_51_cast_fp16")];
            tensor<int32, [8]> x_53_pad_0 = const()[name = tensor<string, []>("x_53_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_53_mode_0 = const()[name = tensor<string, []>("x_53_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_34_to_fp16 = const()[name = tensor<string, []>("const_34_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_53_cast_fp16 = pad(constant_val = const_34_to_fp16, mode = x_53_mode_0, pad = x_53_pad_0, x = x_51_cast_fp16)[name = tensor<string, []>("x_53_cast_fp16")];
            tensor<int32, [4]> var_607 = const()[name = tensor<string, []>("op_607"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_55_cast_fp16 = reshape(shape = var_607, x = x_53_cast_fp16)[name = tensor<string, []>("x_55_cast_fp16")];
            tensor<int32, [4]> var_611_begin_0 = const()[name = tensor<string, []>("op_611_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_611_end_0 = const()[name = tensor<string, []>("op_611_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_611_end_mask_0 = const()[name = tensor<string, []>("op_611_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_611_cast_fp16 = slice_by_index(begin = var_611_begin_0, end = var_611_end_0, end_mask = var_611_end_mask_0, x = x_55_cast_fp16)[name = tensor<string, []>("op_611_cast_fp16")];
            tensor<int32, [4]> var_612 = const()[name = tensor<string, []>("op_612"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_9_cast_fp16 = reshape(shape = var_612, x = var_611_cast_fp16)[name = tensor<string, []>("matrix_bd_9_cast_fp16")];
            tensor<bool, []> matrix_ac_5_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_5_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_5_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_100_perm_0 = const()[name = tensor<string, []>("transpose_100_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_101_perm_0 = const()[name = tensor<string, []>("transpose_101_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_101 = transpose(perm = transpose_101_perm_0, x = k_9_cast_fp16)[name = tensor<string, []>("transpose_296")];
            tensor<fp16, [1, 8, 188, 128]> transpose_100 = transpose(perm = transpose_100_perm_0, x = var_595_cast_fp16)[name = tensor<string, []>("transpose_297")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_5_cast_fp16 = matmul(transpose_x = matrix_ac_5_transpose_x_0, transpose_y = matrix_ac_5_transpose_y_0, x = transpose_100, y = transpose_101)[name = tensor<string, []>("matrix_ac_5_cast_fp16")];
            tensor<int32, [4]> matrix_bd_11_begin_0 = const()[name = tensor<string, []>("matrix_bd_11_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_11_end_0 = const()[name = tensor<string, []>("matrix_bd_11_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_11_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_11_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_11_cast_fp16 = slice_by_index(begin = matrix_bd_11_begin_0, end = matrix_bd_11_end_0, end_mask = matrix_bd_11_end_mask_0, x = matrix_bd_9_cast_fp16)[name = tensor<string, []>("matrix_bd_11_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_621_cast_fp16 = add(x = matrix_ac_5_cast_fp16, y = matrix_bd_11_cast_fp16)[name = tensor<string, []>("op_621_cast_fp16")];
            tensor<fp16, []> _inversed_scores_9_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_9_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_9_cast_fp16 = mul(x = var_621_cast_fp16, y = _inversed_scores_9_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_9_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_11_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_9_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_11_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_627_cast_fp16 = softmax(axis = var_30, x = scores_11_cast_fp16)[name = tensor<string, []>("op_627_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_137_cast_fp16 = select(a = var_11_to_fp16, b = var_627_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_137_cast_fp16")];
            tensor<bool, []> x_57_transpose_x_0 = const()[name = tensor<string, []>("x_57_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_57_transpose_y_0 = const()[name = tensor<string, []>("x_57_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_7_cast_fp16 = transpose(perm = value_7_perm_0, x = v_5_cast_fp16)[name = tensor<string, []>("transpose_295")];
            tensor<fp16, [1, 8, 188, 128]> x_57_cast_fp16 = matmul(transpose_x = x_57_transpose_x_0, transpose_y = x_57_transpose_y_0, x = input_137_cast_fp16, y = value_7_cast_fp16)[name = tensor<string, []>("x_57_cast_fp16")];
            tensor<int32, [4]> var_631_perm_0 = const()[name = tensor<string, []>("op_631_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_632 = const()[name = tensor<string, []>("op_632"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_631_cast_fp16 = transpose(perm = var_631_perm_0, x = x_57_cast_fp16)[name = tensor<string, []>("transpose_294")];
            tensor<fp16, [1, 188, 1024]> input_139_cast_fp16 = reshape(shape = var_632, x = var_631_cast_fp16)[name = tensor<string, []>("input_139_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_2_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49045888))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49832384))), name = tensor<string, []>("module_layers_2_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_25_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_2_self_attn_linear_out_weight_to_fp16_palettized, x = input_139_cast_fp16)[name = tensor<string, []>("linear_25_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_143_cast_fp16 = add(x = input_135_cast_fp16, y = linear_25_cast_fp16)[name = tensor<string, []>("input_143_cast_fp16")];
            tensor<int32, [1]> x_61_axes_0 = const()[name = tensor<string, []>("x_61_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_2_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_2_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49832576)))];
            tensor<fp16, [1024]> module_layers_2_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_2_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49834688)))];
            tensor<fp16, [1, 188, 1024]> x_61_cast_fp16 = layer_norm(axes = x_61_axes_0, beta = module_layers_2_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_2_norm_conv_weight_to_fp16, x = input_143_cast_fp16)[name = tensor<string, []>("x_61_cast_fp16")];
            tensor<int32, [3]> input_145_perm_0 = const()[name = tensor<string, []>("input_145_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_147_pad_type_0 = const()[name = tensor<string, []>("input_147_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_147_strides_0 = const()[name = tensor<string, []>("input_147_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_147_pad_0 = const()[name = tensor<string, []>("input_147_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_147_dilations_0 = const()[name = tensor<string, []>("input_147_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_147_groups_0 = const()[name = tensor<string, []>("input_147_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_2_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49836800))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(51409728))), name = tensor<string, []>("module_layers_2_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_145_cast_fp16 = transpose(perm = input_145_perm_0, x = x_61_cast_fp16)[name = tensor<string, []>("transpose_293")];
            tensor<fp16, [1, 2048, 188]> input_147_cast_fp16 = conv(dilations = input_147_dilations_0, groups = input_147_groups_0, pad = input_147_pad_0, pad_type = input_147_pad_type_0, strides = input_147_strides_0, weight = module_layers_2_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_145_cast_fp16)[name = tensor<string, []>("input_147_cast_fp16")];
            tensor<int32, []> x_63_split_num_splits_0 = const()[name = tensor<string, []>("x_63_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_63_split_axis_0 = const()[name = tensor<string, []>("x_63_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_63_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_63_split_cast_fp16_1 = split(axis = x_63_split_axis_0, num_splits = x_63_split_num_splits_0, x = input_147_cast_fp16)[name = tensor<string, []>("x_63_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_63_split_1_sigmoid_cast_fp16 = sigmoid(x = x_63_split_cast_fp16_1)[name = tensor<string, []>("x_63_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_63_cast_fp16 = mul(x = x_63_split_cast_fp16_0, y = x_63_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_63_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_149_cast_fp16 = select(a = var_11_to_fp16, b = x_63_cast_fp16, cond = var_328)[name = tensor<string, []>("input_149_cast_fp16")];
            tensor<int32, [6]> input_151_pad_0 = const()[name = tensor<string, []>("input_151_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_151_mode_0 = const()[name = tensor<string, []>("input_151_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_37_to_fp16 = const()[name = tensor<string, []>("const_37_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_151_cast_fp16 = pad(constant_val = const_37_to_fp16, mode = input_151_mode_0, pad = input_151_pad_0, x = input_149_cast_fp16)[name = tensor<string, []>("input_151_cast_fp16")];
            tensor<string, []> input_153_pad_type_0 = const()[name = tensor<string, []>("input_153_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_153_groups_0 = const()[name = tensor<string, []>("input_153_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_153_strides_0 = const()[name = tensor<string, []>("input_153_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_153_pad_0 = const()[name = tensor<string, []>("input_153_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_153_dilations_0 = const()[name = tensor<string, []>("input_153_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_252_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(51409920))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(51416896))), name = tensor<string, []>("const_252_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_253_to_fp16 = const()[name = tensor<string, []>("const_253_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(51417088)))];
            tensor<fp16, [1, 1024, 188]> input_155_cast_fp16 = conv(bias = const_253_to_fp16, dilations = input_153_dilations_0, groups = input_153_groups_0, pad = input_153_pad_0, pad_type = input_153_pad_type_0, strides = input_153_strides_0, weight = const_252_to_fp16_palettized, x = input_151_cast_fp16)[name = tensor<string, []>("input_155_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_157_cast_fp16 = silu(x = input_155_cast_fp16)[name = tensor<string, []>("input_157_cast_fp16")];
            tensor<string, []> x_65_pad_type_0 = const()[name = tensor<string, []>("x_65_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_65_strides_0 = const()[name = tensor<string, []>("x_65_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_65_pad_0 = const()[name = tensor<string, []>("x_65_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_65_dilations_0 = const()[name = tensor<string, []>("x_65_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_65_groups_0 = const()[name = tensor<string, []>("x_65_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_2_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(51419200))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52205696))), name = tensor<string, []>("module_layers_2_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_65_cast_fp16 = conv(dilations = x_65_dilations_0, groups = x_65_groups_0, pad = x_65_pad_0, pad_type = x_65_pad_type_0, strides = x_65_strides_0, weight = module_layers_2_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_157_cast_fp16)[name = tensor<string, []>("x_65_cast_fp16")];
            tensor<int32, [3]> input_159_perm_0 = const()[name = tensor<string, []>("input_159_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_159_cast_fp16 = transpose(perm = input_159_perm_0, x = x_65_cast_fp16)[name = tensor<string, []>("transpose_292")];
            tensor<fp16, [1, 188, 1024]> input_161_cast_fp16 = add(x = input_143_cast_fp16, y = input_159_cast_fp16)[name = tensor<string, []>("input_161_cast_fp16")];
            tensor<int32, [1]> input_163_axes_0 = const()[name = tensor<string, []>("input_163_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_2_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_2_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52205888)))];
            tensor<fp16, [1024]> module_layers_2_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_2_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52208000)))];
            tensor<fp16, [1, 188, 1024]> input_163_cast_fp16 = layer_norm(axes = input_163_axes_0, beta = module_layers_2_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_2_norm_feed_forward2_weight_to_fp16, x = input_161_cast_fp16)[name = tensor<string, []>("input_163_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_2_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52210112))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(55355904))), name = tensor<string, []>("module_layers_2_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_26_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_2_feed_forward2_linear1_weight_to_fp16_palettized, x = input_163_cast_fp16)[name = tensor<string, []>("linear_26_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_167_cast_fp16 = silu(x = linear_26_cast_fp16)[name = tensor<string, []>("input_167_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_2_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(55356096))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(58501888))), name = tensor<string, []>("module_layers_2_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_27_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_2_feed_forward2_linear2_weight_to_fp16_palettized, x = input_167_cast_fp16)[name = tensor<string, []>("linear_27_cast_fp16")];
            tensor<fp16, []> var_692_to_fp16 = const()[name = tensor<string, []>("op_692_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_693_cast_fp16 = mul(x = linear_27_cast_fp16, y = var_692_to_fp16)[name = tensor<string, []>("op_693_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_173_cast_fp16 = add(x = input_161_cast_fp16, y = var_693_cast_fp16)[name = tensor<string, []>("input_173_cast_fp16")];
            tensor<int32, [1]> input_175_axes_0 = const()[name = tensor<string, []>("input_175_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_2_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_2_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(58502080)))];
            tensor<fp16, [1024]> module_layers_2_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_2_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(58504192)))];
            tensor<fp16, [1, 188, 1024]> input_175_cast_fp16 = layer_norm(axes = input_175_axes_0, beta = module_layers_2_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_2_norm_out_weight_to_fp16, x = input_173_cast_fp16)[name = tensor<string, []>("input_175_cast_fp16")];
            tensor<int32, [1]> input_177_axes_0 = const()[name = tensor<string, []>("input_177_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_3_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_3_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(58506304)))];
            tensor<fp16, [1024]> module_layers_3_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_3_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(58508416)))];
            tensor<fp16, [1, 188, 1024]> input_177_cast_fp16 = layer_norm(axes = input_177_axes_0, beta = module_layers_3_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_3_norm_feed_forward1_weight_to_fp16, x = input_175_cast_fp16)[name = tensor<string, []>("input_177_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_3_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(58510528))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(61656320))), name = tensor<string, []>("module_layers_3_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_28_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_3_feed_forward1_linear1_weight_to_fp16_palettized, x = input_177_cast_fp16)[name = tensor<string, []>("linear_28_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_181_cast_fp16 = silu(x = linear_28_cast_fp16)[name = tensor<string, []>("input_181_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_3_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(61656512))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64802304))), name = tensor<string, []>("module_layers_3_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_29_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_3_feed_forward1_linear2_weight_to_fp16_palettized, x = input_181_cast_fp16)[name = tensor<string, []>("linear_29_cast_fp16")];
            tensor<fp16, []> var_721_to_fp16 = const()[name = tensor<string, []>("op_721_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_722_cast_fp16 = mul(x = linear_29_cast_fp16, y = var_721_to_fp16)[name = tensor<string, []>("op_722_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_187_cast_fp16 = add(x = input_175_cast_fp16, y = var_722_cast_fp16)[name = tensor<string, []>("input_187_cast_fp16")];
            tensor<int32, [1]> query_7_axes_0 = const()[name = tensor<string, []>("query_7_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_3_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_3_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64802496)))];
            tensor<fp16, [1024]> module_layers_3_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_3_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64804608)))];
            tensor<fp16, [1, 188, 1024]> query_7_cast_fp16 = layer_norm(axes = query_7_axes_0, beta = module_layers_3_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_3_norm_self_att_weight_to_fp16, x = input_187_cast_fp16)[name = tensor<string, []>("query_7_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_3_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64806720))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(65593216))), name = tensor<string, []>("module_layers_3_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_30_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_3_self_attn_linear_q_weight_to_fp16_palettized, x = query_7_cast_fp16)[name = tensor<string, []>("linear_30_cast_fp16")];
            tensor<int32, [4]> var_738 = const()[name = tensor<string, []>("op_738"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_19_cast_fp16 = reshape(shape = var_738, x = linear_30_cast_fp16)[name = tensor<string, []>("q_19_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_3_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(65593408))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(66379904))), name = tensor<string, []>("module_layers_3_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_31_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_3_self_attn_linear_k_weight_to_fp16_palettized, x = query_7_cast_fp16)[name = tensor<string, []>("linear_31_cast_fp16")];
            tensor<int32, [4]> var_742 = const()[name = tensor<string, []>("op_742"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_13_cast_fp16 = reshape(shape = var_742, x = linear_31_cast_fp16)[name = tensor<string, []>("k_13_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_3_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(66380096))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(67166592))), name = tensor<string, []>("module_layers_3_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_32_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_3_self_attn_linear_v_weight_to_fp16_palettized, x = query_7_cast_fp16)[name = tensor<string, []>("linear_32_cast_fp16")];
            tensor<int32, [4]> var_746 = const()[name = tensor<string, []>("op_746"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_7_cast_fp16 = reshape(shape = var_746, x = linear_32_cast_fp16)[name = tensor<string, []>("v_7_cast_fp16")];
            tensor<int32, [4]> value_9_perm_0 = const()[name = tensor<string, []>("value_9_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_3_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_3_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(67166784)))];
            tensor<fp16, [1, 188, 8, 128]> var_758_cast_fp16 = add(x = q_19_cast_fp16, y = module_layers_3_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_758_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_3_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_3_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(67168896)))];
            tensor<fp16, [1, 188, 8, 128]> var_760_cast_fp16 = add(x = q_19_cast_fp16, y = module_layers_3_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_760_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_7_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_7_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_73_transpose_x_0 = const()[name = tensor<string, []>("x_73_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_73_transpose_y_0 = const()[name = tensor<string, []>("x_73_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_762_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(67171008))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(67459072))), name = tensor<string, []>("op_762_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_7_cast_fp16 = transpose(perm = q_with_bias_v_7_perm_0, x = var_760_cast_fp16)[name = tensor<string, []>("transpose_291")];
            tensor<fp16, [1, 8, 188, 375]> x_73_cast_fp16 = matmul(transpose_x = x_73_transpose_x_0, transpose_y = x_73_transpose_y_0, x = q_with_bias_v_7_cast_fp16, y = op_762_to_fp16_palettized)[name = tensor<string, []>("x_73_cast_fp16")];
            tensor<int32, [8]> x_75_pad_0 = const()[name = tensor<string, []>("x_75_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_75_mode_0 = const()[name = tensor<string, []>("x_75_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_44_to_fp16 = const()[name = tensor<string, []>("const_44_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_75_cast_fp16 = pad(constant_val = const_44_to_fp16, mode = x_75_mode_0, pad = x_75_pad_0, x = x_73_cast_fp16)[name = tensor<string, []>("x_75_cast_fp16")];
            tensor<int32, [4]> var_770 = const()[name = tensor<string, []>("op_770"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_77_cast_fp16 = reshape(shape = var_770, x = x_75_cast_fp16)[name = tensor<string, []>("x_77_cast_fp16")];
            tensor<int32, [4]> var_774_begin_0 = const()[name = tensor<string, []>("op_774_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_774_end_0 = const()[name = tensor<string, []>("op_774_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_774_end_mask_0 = const()[name = tensor<string, []>("op_774_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_774_cast_fp16 = slice_by_index(begin = var_774_begin_0, end = var_774_end_0, end_mask = var_774_end_mask_0, x = x_77_cast_fp16)[name = tensor<string, []>("op_774_cast_fp16")];
            tensor<int32, [4]> var_775 = const()[name = tensor<string, []>("op_775"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_13_cast_fp16 = reshape(shape = var_775, x = var_774_cast_fp16)[name = tensor<string, []>("matrix_bd_13_cast_fp16")];
            tensor<bool, []> matrix_ac_7_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_7_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_7_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_7_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_102_perm_0 = const()[name = tensor<string, []>("transpose_102_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_103_perm_0 = const()[name = tensor<string, []>("transpose_103_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_103 = transpose(perm = transpose_103_perm_0, x = k_13_cast_fp16)[name = tensor<string, []>("transpose_289")];
            tensor<fp16, [1, 8, 188, 128]> transpose_102 = transpose(perm = transpose_102_perm_0, x = var_758_cast_fp16)[name = tensor<string, []>("transpose_290")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_7_cast_fp16 = matmul(transpose_x = matrix_ac_7_transpose_x_0, transpose_y = matrix_ac_7_transpose_y_0, x = transpose_102, y = transpose_103)[name = tensor<string, []>("matrix_ac_7_cast_fp16")];
            tensor<int32, [4]> matrix_bd_15_begin_0 = const()[name = tensor<string, []>("matrix_bd_15_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_15_end_0 = const()[name = tensor<string, []>("matrix_bd_15_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_15_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_15_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_15_cast_fp16 = slice_by_index(begin = matrix_bd_15_begin_0, end = matrix_bd_15_end_0, end_mask = matrix_bd_15_end_mask_0, x = matrix_bd_13_cast_fp16)[name = tensor<string, []>("matrix_bd_15_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_784_cast_fp16 = add(x = matrix_ac_7_cast_fp16, y = matrix_bd_15_cast_fp16)[name = tensor<string, []>("op_784_cast_fp16")];
            tensor<fp16, []> _inversed_scores_13_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_13_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_13_cast_fp16 = mul(x = var_784_cast_fp16, y = _inversed_scores_13_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_13_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_15_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_13_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_15_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_790_cast_fp16 = softmax(axis = var_30, x = scores_15_cast_fp16)[name = tensor<string, []>("op_790_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_189_cast_fp16 = select(a = var_11_to_fp16, b = var_790_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_189_cast_fp16")];
            tensor<bool, []> x_79_transpose_x_0 = const()[name = tensor<string, []>("x_79_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_79_transpose_y_0 = const()[name = tensor<string, []>("x_79_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_9_cast_fp16 = transpose(perm = value_9_perm_0, x = v_7_cast_fp16)[name = tensor<string, []>("transpose_288")];
            tensor<fp16, [1, 8, 188, 128]> x_79_cast_fp16 = matmul(transpose_x = x_79_transpose_x_0, transpose_y = x_79_transpose_y_0, x = input_189_cast_fp16, y = value_9_cast_fp16)[name = tensor<string, []>("x_79_cast_fp16")];
            tensor<int32, [4]> var_794_perm_0 = const()[name = tensor<string, []>("op_794_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_795 = const()[name = tensor<string, []>("op_795"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_794_cast_fp16 = transpose(perm = var_794_perm_0, x = x_79_cast_fp16)[name = tensor<string, []>("transpose_287")];
            tensor<fp16, [1, 188, 1024]> input_191_cast_fp16 = reshape(shape = var_795, x = var_794_cast_fp16)[name = tensor<string, []>("input_191_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_3_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(67459264))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(68245760))), name = tensor<string, []>("module_layers_3_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_34_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_3_self_attn_linear_out_weight_to_fp16_palettized, x = input_191_cast_fp16)[name = tensor<string, []>("linear_34_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_195_cast_fp16 = add(x = input_187_cast_fp16, y = linear_34_cast_fp16)[name = tensor<string, []>("input_195_cast_fp16")];
            tensor<int32, [1]> x_83_axes_0 = const()[name = tensor<string, []>("x_83_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_3_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_3_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(68245952)))];
            tensor<fp16, [1024]> module_layers_3_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_3_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(68248064)))];
            tensor<fp16, [1, 188, 1024]> x_83_cast_fp16 = layer_norm(axes = x_83_axes_0, beta = module_layers_3_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_3_norm_conv_weight_to_fp16, x = input_195_cast_fp16)[name = tensor<string, []>("x_83_cast_fp16")];
            tensor<int32, [3]> input_197_perm_0 = const()[name = tensor<string, []>("input_197_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_199_pad_type_0 = const()[name = tensor<string, []>("input_199_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_199_strides_0 = const()[name = tensor<string, []>("input_199_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_199_pad_0 = const()[name = tensor<string, []>("input_199_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_199_dilations_0 = const()[name = tensor<string, []>("input_199_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_199_groups_0 = const()[name = tensor<string, []>("input_199_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_3_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(68250176))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(69823104))), name = tensor<string, []>("module_layers_3_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_197_cast_fp16 = transpose(perm = input_197_perm_0, x = x_83_cast_fp16)[name = tensor<string, []>("transpose_286")];
            tensor<fp16, [1, 2048, 188]> input_199_cast_fp16 = conv(dilations = input_199_dilations_0, groups = input_199_groups_0, pad = input_199_pad_0, pad_type = input_199_pad_type_0, strides = input_199_strides_0, weight = module_layers_3_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_197_cast_fp16)[name = tensor<string, []>("input_199_cast_fp16")];
            tensor<int32, []> x_85_split_num_splits_0 = const()[name = tensor<string, []>("x_85_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_85_split_axis_0 = const()[name = tensor<string, []>("x_85_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_85_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_85_split_cast_fp16_1 = split(axis = x_85_split_axis_0, num_splits = x_85_split_num_splits_0, x = input_199_cast_fp16)[name = tensor<string, []>("x_85_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_85_split_1_sigmoid_cast_fp16 = sigmoid(x = x_85_split_cast_fp16_1)[name = tensor<string, []>("x_85_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_85_cast_fp16 = mul(x = x_85_split_cast_fp16_0, y = x_85_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_85_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_201_cast_fp16 = select(a = var_11_to_fp16, b = x_85_cast_fp16, cond = var_328)[name = tensor<string, []>("input_201_cast_fp16")];
            tensor<int32, [6]> input_203_pad_0 = const()[name = tensor<string, []>("input_203_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_203_mode_0 = const()[name = tensor<string, []>("input_203_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_47_to_fp16 = const()[name = tensor<string, []>("const_47_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_203_cast_fp16 = pad(constant_val = const_47_to_fp16, mode = input_203_mode_0, pad = input_203_pad_0, x = input_201_cast_fp16)[name = tensor<string, []>("input_203_cast_fp16")];
            tensor<string, []> input_205_pad_type_0 = const()[name = tensor<string, []>("input_205_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_205_groups_0 = const()[name = tensor<string, []>("input_205_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_205_strides_0 = const()[name = tensor<string, []>("input_205_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_205_pad_0 = const()[name = tensor<string, []>("input_205_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_205_dilations_0 = const()[name = tensor<string, []>("input_205_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_254_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(69823296))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(69830272))), name = tensor<string, []>("const_254_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_255_to_fp16 = const()[name = tensor<string, []>("const_255_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(69830464)))];
            tensor<fp16, [1, 1024, 188]> input_207_cast_fp16 = conv(bias = const_255_to_fp16, dilations = input_205_dilations_0, groups = input_205_groups_0, pad = input_205_pad_0, pad_type = input_205_pad_type_0, strides = input_205_strides_0, weight = const_254_to_fp16_palettized, x = input_203_cast_fp16)[name = tensor<string, []>("input_207_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_209_cast_fp16 = silu(x = input_207_cast_fp16)[name = tensor<string, []>("input_209_cast_fp16")];
            tensor<string, []> x_87_pad_type_0 = const()[name = tensor<string, []>("x_87_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_87_strides_0 = const()[name = tensor<string, []>("x_87_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_87_pad_0 = const()[name = tensor<string, []>("x_87_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_87_dilations_0 = const()[name = tensor<string, []>("x_87_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_87_groups_0 = const()[name = tensor<string, []>("x_87_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_3_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(69832576))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(70619072))), name = tensor<string, []>("module_layers_3_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_87_cast_fp16 = conv(dilations = x_87_dilations_0, groups = x_87_groups_0, pad = x_87_pad_0, pad_type = x_87_pad_type_0, strides = x_87_strides_0, weight = module_layers_3_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_209_cast_fp16)[name = tensor<string, []>("x_87_cast_fp16")];
            tensor<int32, [3]> input_211_perm_0 = const()[name = tensor<string, []>("input_211_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_211_cast_fp16 = transpose(perm = input_211_perm_0, x = x_87_cast_fp16)[name = tensor<string, []>("transpose_285")];
            tensor<fp16, [1, 188, 1024]> input_213_cast_fp16 = add(x = input_195_cast_fp16, y = input_211_cast_fp16)[name = tensor<string, []>("input_213_cast_fp16")];
            tensor<int32, [1]> input_215_axes_0 = const()[name = tensor<string, []>("input_215_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_3_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_3_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(70619264)))];
            tensor<fp16, [1024]> module_layers_3_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_3_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(70621376)))];
            tensor<fp16, [1, 188, 1024]> input_215_cast_fp16 = layer_norm(axes = input_215_axes_0, beta = module_layers_3_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_3_norm_feed_forward2_weight_to_fp16, x = input_213_cast_fp16)[name = tensor<string, []>("input_215_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_3_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(70623488))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(73769280))), name = tensor<string, []>("module_layers_3_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_35_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_3_feed_forward2_linear1_weight_to_fp16_palettized, x = input_215_cast_fp16)[name = tensor<string, []>("linear_35_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_219_cast_fp16 = silu(x = linear_35_cast_fp16)[name = tensor<string, []>("input_219_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_3_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(73769472))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(76915264))), name = tensor<string, []>("module_layers_3_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_36_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_3_feed_forward2_linear2_weight_to_fp16_palettized, x = input_219_cast_fp16)[name = tensor<string, []>("linear_36_cast_fp16")];
            tensor<fp16, []> var_855_to_fp16 = const()[name = tensor<string, []>("op_855_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_856_cast_fp16 = mul(x = linear_36_cast_fp16, y = var_855_to_fp16)[name = tensor<string, []>("op_856_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_225_cast_fp16 = add(x = input_213_cast_fp16, y = var_856_cast_fp16)[name = tensor<string, []>("input_225_cast_fp16")];
            tensor<int32, [1]> input_227_axes_0 = const()[name = tensor<string, []>("input_227_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_3_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_3_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(76915456)))];
            tensor<fp16, [1024]> module_layers_3_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_3_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(76917568)))];
            tensor<fp16, [1, 188, 1024]> input_227_cast_fp16 = layer_norm(axes = input_227_axes_0, beta = module_layers_3_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_3_norm_out_weight_to_fp16, x = input_225_cast_fp16)[name = tensor<string, []>("input_227_cast_fp16")];
            tensor<int32, [1]> input_229_axes_0 = const()[name = tensor<string, []>("input_229_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_4_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_4_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(76919680)))];
            tensor<fp16, [1024]> module_layers_4_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_4_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(76921792)))];
            tensor<fp16, [1, 188, 1024]> input_229_cast_fp16 = layer_norm(axes = input_229_axes_0, beta = module_layers_4_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_4_norm_feed_forward1_weight_to_fp16, x = input_227_cast_fp16)[name = tensor<string, []>("input_229_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_4_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(76923904))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(80069696))), name = tensor<string, []>("module_layers_4_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_37_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_4_feed_forward1_linear1_weight_to_fp16_palettized, x = input_229_cast_fp16)[name = tensor<string, []>("linear_37_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_233_cast_fp16 = silu(x = linear_37_cast_fp16)[name = tensor<string, []>("input_233_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_4_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(80069888))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(83215680))), name = tensor<string, []>("module_layers_4_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_38_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_4_feed_forward1_linear2_weight_to_fp16_palettized, x = input_233_cast_fp16)[name = tensor<string, []>("linear_38_cast_fp16")];
            tensor<fp16, []> var_884_to_fp16 = const()[name = tensor<string, []>("op_884_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_885_cast_fp16 = mul(x = linear_38_cast_fp16, y = var_884_to_fp16)[name = tensor<string, []>("op_885_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_239_cast_fp16 = add(x = input_227_cast_fp16, y = var_885_cast_fp16)[name = tensor<string, []>("input_239_cast_fp16")];
            tensor<int32, [1]> query_9_axes_0 = const()[name = tensor<string, []>("query_9_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_4_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_4_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(83215872)))];
            tensor<fp16, [1024]> module_layers_4_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_4_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(83217984)))];
            tensor<fp16, [1, 188, 1024]> query_9_cast_fp16 = layer_norm(axes = query_9_axes_0, beta = module_layers_4_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_4_norm_self_att_weight_to_fp16, x = input_239_cast_fp16)[name = tensor<string, []>("query_9_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_4_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(83220096))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(84006592))), name = tensor<string, []>("module_layers_4_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_39_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_4_self_attn_linear_q_weight_to_fp16_palettized, x = query_9_cast_fp16)[name = tensor<string, []>("linear_39_cast_fp16")];
            tensor<int32, [4]> var_901 = const()[name = tensor<string, []>("op_901"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_25_cast_fp16 = reshape(shape = var_901, x = linear_39_cast_fp16)[name = tensor<string, []>("q_25_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_4_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(84006784))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(84793280))), name = tensor<string, []>("module_layers_4_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_40_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_4_self_attn_linear_k_weight_to_fp16_palettized, x = query_9_cast_fp16)[name = tensor<string, []>("linear_40_cast_fp16")];
            tensor<int32, [4]> var_905 = const()[name = tensor<string, []>("op_905"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_17_cast_fp16 = reshape(shape = var_905, x = linear_40_cast_fp16)[name = tensor<string, []>("k_17_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_4_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(84793472))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(85579968))), name = tensor<string, []>("module_layers_4_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_41_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_4_self_attn_linear_v_weight_to_fp16_palettized, x = query_9_cast_fp16)[name = tensor<string, []>("linear_41_cast_fp16")];
            tensor<int32, [4]> var_909 = const()[name = tensor<string, []>("op_909"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_9_cast_fp16 = reshape(shape = var_909, x = linear_41_cast_fp16)[name = tensor<string, []>("v_9_cast_fp16")];
            tensor<int32, [4]> value_11_perm_0 = const()[name = tensor<string, []>("value_11_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_4_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_4_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(85580160)))];
            tensor<fp16, [1, 188, 8, 128]> var_921_cast_fp16 = add(x = q_25_cast_fp16, y = module_layers_4_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_921_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_4_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_4_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(85582272)))];
            tensor<fp16, [1, 188, 8, 128]> var_923_cast_fp16 = add(x = q_25_cast_fp16, y = module_layers_4_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_923_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_9_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_9_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_95_transpose_x_0 = const()[name = tensor<string, []>("x_95_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_95_transpose_y_0 = const()[name = tensor<string, []>("x_95_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_925_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(85584384))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(85872448))), name = tensor<string, []>("op_925_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_9_cast_fp16 = transpose(perm = q_with_bias_v_9_perm_0, x = var_923_cast_fp16)[name = tensor<string, []>("transpose_284")];
            tensor<fp16, [1, 8, 188, 375]> x_95_cast_fp16 = matmul(transpose_x = x_95_transpose_x_0, transpose_y = x_95_transpose_y_0, x = q_with_bias_v_9_cast_fp16, y = op_925_to_fp16_palettized)[name = tensor<string, []>("x_95_cast_fp16")];
            tensor<int32, [8]> x_97_pad_0 = const()[name = tensor<string, []>("x_97_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_97_mode_0 = const()[name = tensor<string, []>("x_97_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_54_to_fp16 = const()[name = tensor<string, []>("const_54_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_97_cast_fp16 = pad(constant_val = const_54_to_fp16, mode = x_97_mode_0, pad = x_97_pad_0, x = x_95_cast_fp16)[name = tensor<string, []>("x_97_cast_fp16")];
            tensor<int32, [4]> var_933 = const()[name = tensor<string, []>("op_933"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_99_cast_fp16 = reshape(shape = var_933, x = x_97_cast_fp16)[name = tensor<string, []>("x_99_cast_fp16")];
            tensor<int32, [4]> var_937_begin_0 = const()[name = tensor<string, []>("op_937_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_937_end_0 = const()[name = tensor<string, []>("op_937_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_937_end_mask_0 = const()[name = tensor<string, []>("op_937_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_937_cast_fp16 = slice_by_index(begin = var_937_begin_0, end = var_937_end_0, end_mask = var_937_end_mask_0, x = x_99_cast_fp16)[name = tensor<string, []>("op_937_cast_fp16")];
            tensor<int32, [4]> var_938 = const()[name = tensor<string, []>("op_938"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_17_cast_fp16 = reshape(shape = var_938, x = var_937_cast_fp16)[name = tensor<string, []>("matrix_bd_17_cast_fp16")];
            tensor<bool, []> matrix_ac_9_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_9_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_9_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_9_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_104_perm_0 = const()[name = tensor<string, []>("transpose_104_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_105_perm_0 = const()[name = tensor<string, []>("transpose_105_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_105 = transpose(perm = transpose_105_perm_0, x = k_17_cast_fp16)[name = tensor<string, []>("transpose_282")];
            tensor<fp16, [1, 8, 188, 128]> transpose_104 = transpose(perm = transpose_104_perm_0, x = var_921_cast_fp16)[name = tensor<string, []>("transpose_283")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_9_cast_fp16 = matmul(transpose_x = matrix_ac_9_transpose_x_0, transpose_y = matrix_ac_9_transpose_y_0, x = transpose_104, y = transpose_105)[name = tensor<string, []>("matrix_ac_9_cast_fp16")];
            tensor<int32, [4]> matrix_bd_19_begin_0 = const()[name = tensor<string, []>("matrix_bd_19_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_19_end_0 = const()[name = tensor<string, []>("matrix_bd_19_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_19_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_19_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_19_cast_fp16 = slice_by_index(begin = matrix_bd_19_begin_0, end = matrix_bd_19_end_0, end_mask = matrix_bd_19_end_mask_0, x = matrix_bd_17_cast_fp16)[name = tensor<string, []>("matrix_bd_19_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_947_cast_fp16 = add(x = matrix_ac_9_cast_fp16, y = matrix_bd_19_cast_fp16)[name = tensor<string, []>("op_947_cast_fp16")];
            tensor<fp16, []> _inversed_scores_17_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_17_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_17_cast_fp16 = mul(x = var_947_cast_fp16, y = _inversed_scores_17_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_17_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_19_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_17_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_19_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_953_cast_fp16 = softmax(axis = var_30, x = scores_19_cast_fp16)[name = tensor<string, []>("op_953_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_241_cast_fp16 = select(a = var_11_to_fp16, b = var_953_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_241_cast_fp16")];
            tensor<bool, []> x_101_transpose_x_0 = const()[name = tensor<string, []>("x_101_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_101_transpose_y_0 = const()[name = tensor<string, []>("x_101_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_11_cast_fp16 = transpose(perm = value_11_perm_0, x = v_9_cast_fp16)[name = tensor<string, []>("transpose_281")];
            tensor<fp16, [1, 8, 188, 128]> x_101_cast_fp16 = matmul(transpose_x = x_101_transpose_x_0, transpose_y = x_101_transpose_y_0, x = input_241_cast_fp16, y = value_11_cast_fp16)[name = tensor<string, []>("x_101_cast_fp16")];
            tensor<int32, [4]> var_957_perm_0 = const()[name = tensor<string, []>("op_957_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_958 = const()[name = tensor<string, []>("op_958"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_957_cast_fp16 = transpose(perm = var_957_perm_0, x = x_101_cast_fp16)[name = tensor<string, []>("transpose_280")];
            tensor<fp16, [1, 188, 1024]> input_243_cast_fp16 = reshape(shape = var_958, x = var_957_cast_fp16)[name = tensor<string, []>("input_243_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_4_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(85872640))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(86659136))), name = tensor<string, []>("module_layers_4_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_43_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_4_self_attn_linear_out_weight_to_fp16_palettized, x = input_243_cast_fp16)[name = tensor<string, []>("linear_43_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_247_cast_fp16 = add(x = input_239_cast_fp16, y = linear_43_cast_fp16)[name = tensor<string, []>("input_247_cast_fp16")];
            tensor<int32, [1]> x_105_axes_0 = const()[name = tensor<string, []>("x_105_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_4_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_4_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(86659328)))];
            tensor<fp16, [1024]> module_layers_4_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_4_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(86661440)))];
            tensor<fp16, [1, 188, 1024]> x_105_cast_fp16 = layer_norm(axes = x_105_axes_0, beta = module_layers_4_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_4_norm_conv_weight_to_fp16, x = input_247_cast_fp16)[name = tensor<string, []>("x_105_cast_fp16")];
            tensor<int32, [3]> input_249_perm_0 = const()[name = tensor<string, []>("input_249_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_251_pad_type_0 = const()[name = tensor<string, []>("input_251_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_251_strides_0 = const()[name = tensor<string, []>("input_251_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_251_pad_0 = const()[name = tensor<string, []>("input_251_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_251_dilations_0 = const()[name = tensor<string, []>("input_251_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_251_groups_0 = const()[name = tensor<string, []>("input_251_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_4_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(86663552))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(88236480))), name = tensor<string, []>("module_layers_4_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_249_cast_fp16 = transpose(perm = input_249_perm_0, x = x_105_cast_fp16)[name = tensor<string, []>("transpose_279")];
            tensor<fp16, [1, 2048, 188]> input_251_cast_fp16 = conv(dilations = input_251_dilations_0, groups = input_251_groups_0, pad = input_251_pad_0, pad_type = input_251_pad_type_0, strides = input_251_strides_0, weight = module_layers_4_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_249_cast_fp16)[name = tensor<string, []>("input_251_cast_fp16")];
            tensor<int32, []> x_107_split_num_splits_0 = const()[name = tensor<string, []>("x_107_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_107_split_axis_0 = const()[name = tensor<string, []>("x_107_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_107_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_107_split_cast_fp16_1 = split(axis = x_107_split_axis_0, num_splits = x_107_split_num_splits_0, x = input_251_cast_fp16)[name = tensor<string, []>("x_107_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_107_split_1_sigmoid_cast_fp16 = sigmoid(x = x_107_split_cast_fp16_1)[name = tensor<string, []>("x_107_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_107_cast_fp16 = mul(x = x_107_split_cast_fp16_0, y = x_107_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_107_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_253_cast_fp16 = select(a = var_11_to_fp16, b = x_107_cast_fp16, cond = var_328)[name = tensor<string, []>("input_253_cast_fp16")];
            tensor<int32, [6]> input_255_pad_0 = const()[name = tensor<string, []>("input_255_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_255_mode_0 = const()[name = tensor<string, []>("input_255_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_57_to_fp16 = const()[name = tensor<string, []>("const_57_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_255_cast_fp16 = pad(constant_val = const_57_to_fp16, mode = input_255_mode_0, pad = input_255_pad_0, x = input_253_cast_fp16)[name = tensor<string, []>("input_255_cast_fp16")];
            tensor<string, []> input_257_pad_type_0 = const()[name = tensor<string, []>("input_257_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_257_groups_0 = const()[name = tensor<string, []>("input_257_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_257_strides_0 = const()[name = tensor<string, []>("input_257_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_257_pad_0 = const()[name = tensor<string, []>("input_257_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_257_dilations_0 = const()[name = tensor<string, []>("input_257_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_256_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(88236672))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(88243648))), name = tensor<string, []>("const_256_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_257_to_fp16 = const()[name = tensor<string, []>("const_257_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(88243840)))];
            tensor<fp16, [1, 1024, 188]> input_259_cast_fp16 = conv(bias = const_257_to_fp16, dilations = input_257_dilations_0, groups = input_257_groups_0, pad = input_257_pad_0, pad_type = input_257_pad_type_0, strides = input_257_strides_0, weight = const_256_to_fp16_palettized, x = input_255_cast_fp16)[name = tensor<string, []>("input_259_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_261_cast_fp16 = silu(x = input_259_cast_fp16)[name = tensor<string, []>("input_261_cast_fp16")];
            tensor<string, []> x_109_pad_type_0 = const()[name = tensor<string, []>("x_109_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_109_strides_0 = const()[name = tensor<string, []>("x_109_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_109_pad_0 = const()[name = tensor<string, []>("x_109_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_109_dilations_0 = const()[name = tensor<string, []>("x_109_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_109_groups_0 = const()[name = tensor<string, []>("x_109_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_4_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(88245952))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(89032448))), name = tensor<string, []>("module_layers_4_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_109_cast_fp16 = conv(dilations = x_109_dilations_0, groups = x_109_groups_0, pad = x_109_pad_0, pad_type = x_109_pad_type_0, strides = x_109_strides_0, weight = module_layers_4_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_261_cast_fp16)[name = tensor<string, []>("x_109_cast_fp16")];
            tensor<int32, [3]> input_263_perm_0 = const()[name = tensor<string, []>("input_263_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_263_cast_fp16 = transpose(perm = input_263_perm_0, x = x_109_cast_fp16)[name = tensor<string, []>("transpose_278")];
            tensor<fp16, [1, 188, 1024]> input_265_cast_fp16 = add(x = input_247_cast_fp16, y = input_263_cast_fp16)[name = tensor<string, []>("input_265_cast_fp16")];
            tensor<int32, [1]> input_267_axes_0 = const()[name = tensor<string, []>("input_267_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_4_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_4_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(89032640)))];
            tensor<fp16, [1024]> module_layers_4_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_4_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(89034752)))];
            tensor<fp16, [1, 188, 1024]> input_267_cast_fp16 = layer_norm(axes = input_267_axes_0, beta = module_layers_4_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_4_norm_feed_forward2_weight_to_fp16, x = input_265_cast_fp16)[name = tensor<string, []>("input_267_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_4_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(89036864))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(92182656))), name = tensor<string, []>("module_layers_4_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_44_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_4_feed_forward2_linear1_weight_to_fp16_palettized, x = input_267_cast_fp16)[name = tensor<string, []>("linear_44_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_271_cast_fp16 = silu(x = linear_44_cast_fp16)[name = tensor<string, []>("input_271_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_4_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(92182848))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(95328640))), name = tensor<string, []>("module_layers_4_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_45_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_4_feed_forward2_linear2_weight_to_fp16_palettized, x = input_271_cast_fp16)[name = tensor<string, []>("linear_45_cast_fp16")];
            tensor<fp16, []> var_1018_to_fp16 = const()[name = tensor<string, []>("op_1018_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1019_cast_fp16 = mul(x = linear_45_cast_fp16, y = var_1018_to_fp16)[name = tensor<string, []>("op_1019_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_277_cast_fp16 = add(x = input_265_cast_fp16, y = var_1019_cast_fp16)[name = tensor<string, []>("input_277_cast_fp16")];
            tensor<int32, [1]> input_279_axes_0 = const()[name = tensor<string, []>("input_279_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_4_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_4_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(95328832)))];
            tensor<fp16, [1024]> module_layers_4_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_4_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(95330944)))];
            tensor<fp16, [1, 188, 1024]> input_279_cast_fp16 = layer_norm(axes = input_279_axes_0, beta = module_layers_4_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_4_norm_out_weight_to_fp16, x = input_277_cast_fp16)[name = tensor<string, []>("input_279_cast_fp16")];
            tensor<int32, [1]> input_281_axes_0 = const()[name = tensor<string, []>("input_281_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_5_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_5_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(95333056)))];
            tensor<fp16, [1024]> module_layers_5_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_5_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(95335168)))];
            tensor<fp16, [1, 188, 1024]> input_281_cast_fp16 = layer_norm(axes = input_281_axes_0, beta = module_layers_5_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_5_norm_feed_forward1_weight_to_fp16, x = input_279_cast_fp16)[name = tensor<string, []>("input_281_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_5_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(95337280))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(98483072))), name = tensor<string, []>("module_layers_5_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_46_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_5_feed_forward1_linear1_weight_to_fp16_palettized, x = input_281_cast_fp16)[name = tensor<string, []>("linear_46_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_285_cast_fp16 = silu(x = linear_46_cast_fp16)[name = tensor<string, []>("input_285_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_5_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(98483264))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(101629056))), name = tensor<string, []>("module_layers_5_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_47_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_5_feed_forward1_linear2_weight_to_fp16_palettized, x = input_285_cast_fp16)[name = tensor<string, []>("linear_47_cast_fp16")];
            tensor<fp16, []> var_1047_to_fp16 = const()[name = tensor<string, []>("op_1047_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1048_cast_fp16 = mul(x = linear_47_cast_fp16, y = var_1047_to_fp16)[name = tensor<string, []>("op_1048_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_291_cast_fp16 = add(x = input_279_cast_fp16, y = var_1048_cast_fp16)[name = tensor<string, []>("input_291_cast_fp16")];
            tensor<int32, [1]> query_11_axes_0 = const()[name = tensor<string, []>("query_11_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_5_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_5_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(101629248)))];
            tensor<fp16, [1024]> module_layers_5_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_5_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(101631360)))];
            tensor<fp16, [1, 188, 1024]> query_11_cast_fp16 = layer_norm(axes = query_11_axes_0, beta = module_layers_5_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_5_norm_self_att_weight_to_fp16, x = input_291_cast_fp16)[name = tensor<string, []>("query_11_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_5_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(101633472))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(102419968))), name = tensor<string, []>("module_layers_5_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_48_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_5_self_attn_linear_q_weight_to_fp16_palettized, x = query_11_cast_fp16)[name = tensor<string, []>("linear_48_cast_fp16")];
            tensor<int32, [4]> var_1064 = const()[name = tensor<string, []>("op_1064"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_31_cast_fp16 = reshape(shape = var_1064, x = linear_48_cast_fp16)[name = tensor<string, []>("q_31_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_5_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(102420160))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(103206656))), name = tensor<string, []>("module_layers_5_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_49_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_5_self_attn_linear_k_weight_to_fp16_palettized, x = query_11_cast_fp16)[name = tensor<string, []>("linear_49_cast_fp16")];
            tensor<int32, [4]> var_1068 = const()[name = tensor<string, []>("op_1068"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_21_cast_fp16 = reshape(shape = var_1068, x = linear_49_cast_fp16)[name = tensor<string, []>("k_21_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_5_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(103206848))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(103993344))), name = tensor<string, []>("module_layers_5_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_50_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_5_self_attn_linear_v_weight_to_fp16_palettized, x = query_11_cast_fp16)[name = tensor<string, []>("linear_50_cast_fp16")];
            tensor<int32, [4]> var_1072 = const()[name = tensor<string, []>("op_1072"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_11_cast_fp16 = reshape(shape = var_1072, x = linear_50_cast_fp16)[name = tensor<string, []>("v_11_cast_fp16")];
            tensor<int32, [4]> value_13_perm_0 = const()[name = tensor<string, []>("value_13_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_5_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_5_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(103993536)))];
            tensor<fp16, [1, 188, 8, 128]> var_1084_cast_fp16 = add(x = q_31_cast_fp16, y = module_layers_5_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_1084_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_5_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_5_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(103995648)))];
            tensor<fp16, [1, 188, 8, 128]> var_1086_cast_fp16 = add(x = q_31_cast_fp16, y = module_layers_5_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_1086_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_11_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_11_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_117_transpose_x_0 = const()[name = tensor<string, []>("x_117_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_117_transpose_y_0 = const()[name = tensor<string, []>("x_117_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_1088_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(103997760))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(104285824))), name = tensor<string, []>("op_1088_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_11_cast_fp16 = transpose(perm = q_with_bias_v_11_perm_0, x = var_1086_cast_fp16)[name = tensor<string, []>("transpose_277")];
            tensor<fp16, [1, 8, 188, 375]> x_117_cast_fp16 = matmul(transpose_x = x_117_transpose_x_0, transpose_y = x_117_transpose_y_0, x = q_with_bias_v_11_cast_fp16, y = op_1088_to_fp16_palettized)[name = tensor<string, []>("x_117_cast_fp16")];
            tensor<int32, [8]> x_119_pad_0 = const()[name = tensor<string, []>("x_119_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_119_mode_0 = const()[name = tensor<string, []>("x_119_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_64_to_fp16 = const()[name = tensor<string, []>("const_64_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_119_cast_fp16 = pad(constant_val = const_64_to_fp16, mode = x_119_mode_0, pad = x_119_pad_0, x = x_117_cast_fp16)[name = tensor<string, []>("x_119_cast_fp16")];
            tensor<int32, [4]> var_1096 = const()[name = tensor<string, []>("op_1096"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_121_cast_fp16 = reshape(shape = var_1096, x = x_119_cast_fp16)[name = tensor<string, []>("x_121_cast_fp16")];
            tensor<int32, [4]> var_1100_begin_0 = const()[name = tensor<string, []>("op_1100_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_1100_end_0 = const()[name = tensor<string, []>("op_1100_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_1100_end_mask_0 = const()[name = tensor<string, []>("op_1100_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_1100_cast_fp16 = slice_by_index(begin = var_1100_begin_0, end = var_1100_end_0, end_mask = var_1100_end_mask_0, x = x_121_cast_fp16)[name = tensor<string, []>("op_1100_cast_fp16")];
            tensor<int32, [4]> var_1101 = const()[name = tensor<string, []>("op_1101"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_21_cast_fp16 = reshape(shape = var_1101, x = var_1100_cast_fp16)[name = tensor<string, []>("matrix_bd_21_cast_fp16")];
            tensor<bool, []> matrix_ac_11_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_11_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_11_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_11_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_106_perm_0 = const()[name = tensor<string, []>("transpose_106_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_107_perm_0 = const()[name = tensor<string, []>("transpose_107_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_107 = transpose(perm = transpose_107_perm_0, x = k_21_cast_fp16)[name = tensor<string, []>("transpose_275")];
            tensor<fp16, [1, 8, 188, 128]> transpose_106 = transpose(perm = transpose_106_perm_0, x = var_1084_cast_fp16)[name = tensor<string, []>("transpose_276")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_11_cast_fp16 = matmul(transpose_x = matrix_ac_11_transpose_x_0, transpose_y = matrix_ac_11_transpose_y_0, x = transpose_106, y = transpose_107)[name = tensor<string, []>("matrix_ac_11_cast_fp16")];
            tensor<int32, [4]> matrix_bd_23_begin_0 = const()[name = tensor<string, []>("matrix_bd_23_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_23_end_0 = const()[name = tensor<string, []>("matrix_bd_23_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_23_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_23_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_23_cast_fp16 = slice_by_index(begin = matrix_bd_23_begin_0, end = matrix_bd_23_end_0, end_mask = matrix_bd_23_end_mask_0, x = matrix_bd_21_cast_fp16)[name = tensor<string, []>("matrix_bd_23_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1110_cast_fp16 = add(x = matrix_ac_11_cast_fp16, y = matrix_bd_23_cast_fp16)[name = tensor<string, []>("op_1110_cast_fp16")];
            tensor<fp16, []> _inversed_scores_21_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_21_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_21_cast_fp16 = mul(x = var_1110_cast_fp16, y = _inversed_scores_21_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_21_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_23_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_21_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_23_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1116_cast_fp16 = softmax(axis = var_30, x = scores_23_cast_fp16)[name = tensor<string, []>("op_1116_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_293_cast_fp16 = select(a = var_11_to_fp16, b = var_1116_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_293_cast_fp16")];
            tensor<bool, []> x_123_transpose_x_0 = const()[name = tensor<string, []>("x_123_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_123_transpose_y_0 = const()[name = tensor<string, []>("x_123_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_13_cast_fp16 = transpose(perm = value_13_perm_0, x = v_11_cast_fp16)[name = tensor<string, []>("transpose_274")];
            tensor<fp16, [1, 8, 188, 128]> x_123_cast_fp16 = matmul(transpose_x = x_123_transpose_x_0, transpose_y = x_123_transpose_y_0, x = input_293_cast_fp16, y = value_13_cast_fp16)[name = tensor<string, []>("x_123_cast_fp16")];
            tensor<int32, [4]> var_1120_perm_0 = const()[name = tensor<string, []>("op_1120_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_1121 = const()[name = tensor<string, []>("op_1121"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_1120_cast_fp16 = transpose(perm = var_1120_perm_0, x = x_123_cast_fp16)[name = tensor<string, []>("transpose_273")];
            tensor<fp16, [1, 188, 1024]> input_295_cast_fp16 = reshape(shape = var_1121, x = var_1120_cast_fp16)[name = tensor<string, []>("input_295_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_5_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(104286016))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(105072512))), name = tensor<string, []>("module_layers_5_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_52_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_5_self_attn_linear_out_weight_to_fp16_palettized, x = input_295_cast_fp16)[name = tensor<string, []>("linear_52_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_299_cast_fp16 = add(x = input_291_cast_fp16, y = linear_52_cast_fp16)[name = tensor<string, []>("input_299_cast_fp16")];
            tensor<int32, [1]> x_127_axes_0 = const()[name = tensor<string, []>("x_127_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_5_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_5_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(105072704)))];
            tensor<fp16, [1024]> module_layers_5_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_5_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(105074816)))];
            tensor<fp16, [1, 188, 1024]> x_127_cast_fp16 = layer_norm(axes = x_127_axes_0, beta = module_layers_5_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_5_norm_conv_weight_to_fp16, x = input_299_cast_fp16)[name = tensor<string, []>("x_127_cast_fp16")];
            tensor<int32, [3]> input_301_perm_0 = const()[name = tensor<string, []>("input_301_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_303_pad_type_0 = const()[name = tensor<string, []>("input_303_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_303_strides_0 = const()[name = tensor<string, []>("input_303_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_303_pad_0 = const()[name = tensor<string, []>("input_303_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_303_dilations_0 = const()[name = tensor<string, []>("input_303_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_303_groups_0 = const()[name = tensor<string, []>("input_303_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_5_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(105076928))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(106649856))), name = tensor<string, []>("module_layers_5_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_301_cast_fp16 = transpose(perm = input_301_perm_0, x = x_127_cast_fp16)[name = tensor<string, []>("transpose_272")];
            tensor<fp16, [1, 2048, 188]> input_303_cast_fp16 = conv(dilations = input_303_dilations_0, groups = input_303_groups_0, pad = input_303_pad_0, pad_type = input_303_pad_type_0, strides = input_303_strides_0, weight = module_layers_5_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_301_cast_fp16)[name = tensor<string, []>("input_303_cast_fp16")];
            tensor<int32, []> x_129_split_num_splits_0 = const()[name = tensor<string, []>("x_129_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_129_split_axis_0 = const()[name = tensor<string, []>("x_129_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_129_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_129_split_cast_fp16_1 = split(axis = x_129_split_axis_0, num_splits = x_129_split_num_splits_0, x = input_303_cast_fp16)[name = tensor<string, []>("x_129_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_129_split_1_sigmoid_cast_fp16 = sigmoid(x = x_129_split_cast_fp16_1)[name = tensor<string, []>("x_129_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_129_cast_fp16 = mul(x = x_129_split_cast_fp16_0, y = x_129_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_129_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_305_cast_fp16 = select(a = var_11_to_fp16, b = x_129_cast_fp16, cond = var_328)[name = tensor<string, []>("input_305_cast_fp16")];
            tensor<int32, [6]> input_307_pad_0 = const()[name = tensor<string, []>("input_307_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_307_mode_0 = const()[name = tensor<string, []>("input_307_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_67_to_fp16 = const()[name = tensor<string, []>("const_67_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_307_cast_fp16 = pad(constant_val = const_67_to_fp16, mode = input_307_mode_0, pad = input_307_pad_0, x = input_305_cast_fp16)[name = tensor<string, []>("input_307_cast_fp16")];
            tensor<string, []> input_309_pad_type_0 = const()[name = tensor<string, []>("input_309_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_309_groups_0 = const()[name = tensor<string, []>("input_309_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_309_strides_0 = const()[name = tensor<string, []>("input_309_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_309_pad_0 = const()[name = tensor<string, []>("input_309_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_309_dilations_0 = const()[name = tensor<string, []>("input_309_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_258_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(106650048))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(106657024))), name = tensor<string, []>("const_258_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_259_to_fp16 = const()[name = tensor<string, []>("const_259_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(106657216)))];
            tensor<fp16, [1, 1024, 188]> input_311_cast_fp16 = conv(bias = const_259_to_fp16, dilations = input_309_dilations_0, groups = input_309_groups_0, pad = input_309_pad_0, pad_type = input_309_pad_type_0, strides = input_309_strides_0, weight = const_258_to_fp16_palettized, x = input_307_cast_fp16)[name = tensor<string, []>("input_311_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_313_cast_fp16 = silu(x = input_311_cast_fp16)[name = tensor<string, []>("input_313_cast_fp16")];
            tensor<string, []> x_131_pad_type_0 = const()[name = tensor<string, []>("x_131_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_131_strides_0 = const()[name = tensor<string, []>("x_131_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_131_pad_0 = const()[name = tensor<string, []>("x_131_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_131_dilations_0 = const()[name = tensor<string, []>("x_131_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_131_groups_0 = const()[name = tensor<string, []>("x_131_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_5_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(106659328))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(107445824))), name = tensor<string, []>("module_layers_5_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_131_cast_fp16 = conv(dilations = x_131_dilations_0, groups = x_131_groups_0, pad = x_131_pad_0, pad_type = x_131_pad_type_0, strides = x_131_strides_0, weight = module_layers_5_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_313_cast_fp16)[name = tensor<string, []>("x_131_cast_fp16")];
            tensor<int32, [3]> input_315_perm_0 = const()[name = tensor<string, []>("input_315_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_315_cast_fp16 = transpose(perm = input_315_perm_0, x = x_131_cast_fp16)[name = tensor<string, []>("transpose_271")];
            tensor<fp16, [1, 188, 1024]> input_317_cast_fp16 = add(x = input_299_cast_fp16, y = input_315_cast_fp16)[name = tensor<string, []>("input_317_cast_fp16")];
            tensor<int32, [1]> input_319_axes_0 = const()[name = tensor<string, []>("input_319_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_5_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_5_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(107446016)))];
            tensor<fp16, [1024]> module_layers_5_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_5_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(107448128)))];
            tensor<fp16, [1, 188, 1024]> input_319_cast_fp16 = layer_norm(axes = input_319_axes_0, beta = module_layers_5_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_5_norm_feed_forward2_weight_to_fp16, x = input_317_cast_fp16)[name = tensor<string, []>("input_319_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_5_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(107450240))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(110596032))), name = tensor<string, []>("module_layers_5_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_53_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_5_feed_forward2_linear1_weight_to_fp16_palettized, x = input_319_cast_fp16)[name = tensor<string, []>("linear_53_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_323_cast_fp16 = silu(x = linear_53_cast_fp16)[name = tensor<string, []>("input_323_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_5_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(110596224))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(113742016))), name = tensor<string, []>("module_layers_5_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_54_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_5_feed_forward2_linear2_weight_to_fp16_palettized, x = input_323_cast_fp16)[name = tensor<string, []>("linear_54_cast_fp16")];
            tensor<fp16, []> var_1181_to_fp16 = const()[name = tensor<string, []>("op_1181_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1182_cast_fp16 = mul(x = linear_54_cast_fp16, y = var_1181_to_fp16)[name = tensor<string, []>("op_1182_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_329_cast_fp16 = add(x = input_317_cast_fp16, y = var_1182_cast_fp16)[name = tensor<string, []>("input_329_cast_fp16")];
            tensor<int32, [1]> input_331_axes_0 = const()[name = tensor<string, []>("input_331_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_5_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_5_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(113742208)))];
            tensor<fp16, [1024]> module_layers_5_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_5_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(113744320)))];
            tensor<fp16, [1, 188, 1024]> input_331_cast_fp16 = layer_norm(axes = input_331_axes_0, beta = module_layers_5_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_5_norm_out_weight_to_fp16, x = input_329_cast_fp16)[name = tensor<string, []>("input_331_cast_fp16")];
            tensor<int32, [1]> input_333_axes_0 = const()[name = tensor<string, []>("input_333_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_6_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_6_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(113746432)))];
            tensor<fp16, [1024]> module_layers_6_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_6_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(113748544)))];
            tensor<fp16, [1, 188, 1024]> input_333_cast_fp16 = layer_norm(axes = input_333_axes_0, beta = module_layers_6_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_6_norm_feed_forward1_weight_to_fp16, x = input_331_cast_fp16)[name = tensor<string, []>("input_333_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_6_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(113750656))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(116896448))), name = tensor<string, []>("module_layers_6_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_55_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_6_feed_forward1_linear1_weight_to_fp16_palettized, x = input_333_cast_fp16)[name = tensor<string, []>("linear_55_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_337_cast_fp16 = silu(x = linear_55_cast_fp16)[name = tensor<string, []>("input_337_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_6_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(116896640))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(120042432))), name = tensor<string, []>("module_layers_6_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_56_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_6_feed_forward1_linear2_weight_to_fp16_palettized, x = input_337_cast_fp16)[name = tensor<string, []>("linear_56_cast_fp16")];
            tensor<fp16, []> var_1210_to_fp16 = const()[name = tensor<string, []>("op_1210_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1211_cast_fp16 = mul(x = linear_56_cast_fp16, y = var_1210_to_fp16)[name = tensor<string, []>("op_1211_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_343_cast_fp16 = add(x = input_331_cast_fp16, y = var_1211_cast_fp16)[name = tensor<string, []>("input_343_cast_fp16")];
            tensor<int32, [1]> query_13_axes_0 = const()[name = tensor<string, []>("query_13_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_6_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_6_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(120042624)))];
            tensor<fp16, [1024]> module_layers_6_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_6_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(120044736)))];
            tensor<fp16, [1, 188, 1024]> query_13_cast_fp16 = layer_norm(axes = query_13_axes_0, beta = module_layers_6_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_6_norm_self_att_weight_to_fp16, x = input_343_cast_fp16)[name = tensor<string, []>("query_13_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_6_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(120046848))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(120833344))), name = tensor<string, []>("module_layers_6_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_57_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_6_self_attn_linear_q_weight_to_fp16_palettized, x = query_13_cast_fp16)[name = tensor<string, []>("linear_57_cast_fp16")];
            tensor<int32, [4]> var_1227 = const()[name = tensor<string, []>("op_1227"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_37_cast_fp16 = reshape(shape = var_1227, x = linear_57_cast_fp16)[name = tensor<string, []>("q_37_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_6_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(120833536))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(121620032))), name = tensor<string, []>("module_layers_6_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_58_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_6_self_attn_linear_k_weight_to_fp16_palettized, x = query_13_cast_fp16)[name = tensor<string, []>("linear_58_cast_fp16")];
            tensor<int32, [4]> var_1231 = const()[name = tensor<string, []>("op_1231"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_25_cast_fp16 = reshape(shape = var_1231, x = linear_58_cast_fp16)[name = tensor<string, []>("k_25_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_6_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(121620224))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(122406720))), name = tensor<string, []>("module_layers_6_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_59_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_6_self_attn_linear_v_weight_to_fp16_palettized, x = query_13_cast_fp16)[name = tensor<string, []>("linear_59_cast_fp16")];
            tensor<int32, [4]> var_1235 = const()[name = tensor<string, []>("op_1235"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_13_cast_fp16 = reshape(shape = var_1235, x = linear_59_cast_fp16)[name = tensor<string, []>("v_13_cast_fp16")];
            tensor<int32, [4]> value_15_perm_0 = const()[name = tensor<string, []>("value_15_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_6_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_6_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(122406912)))];
            tensor<fp16, [1, 188, 8, 128]> var_1247_cast_fp16 = add(x = q_37_cast_fp16, y = module_layers_6_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_1247_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_6_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_6_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(122409024)))];
            tensor<fp16, [1, 188, 8, 128]> var_1249_cast_fp16 = add(x = q_37_cast_fp16, y = module_layers_6_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_1249_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_13_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_13_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_139_transpose_x_0 = const()[name = tensor<string, []>("x_139_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_139_transpose_y_0 = const()[name = tensor<string, []>("x_139_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_1251_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(122411136))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(122699200))), name = tensor<string, []>("op_1251_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_13_cast_fp16 = transpose(perm = q_with_bias_v_13_perm_0, x = var_1249_cast_fp16)[name = tensor<string, []>("transpose_270")];
            tensor<fp16, [1, 8, 188, 375]> x_139_cast_fp16 = matmul(transpose_x = x_139_transpose_x_0, transpose_y = x_139_transpose_y_0, x = q_with_bias_v_13_cast_fp16, y = op_1251_to_fp16_palettized)[name = tensor<string, []>("x_139_cast_fp16")];
            tensor<int32, [8]> x_141_pad_0 = const()[name = tensor<string, []>("x_141_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_141_mode_0 = const()[name = tensor<string, []>("x_141_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_74_to_fp16 = const()[name = tensor<string, []>("const_74_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_141_cast_fp16 = pad(constant_val = const_74_to_fp16, mode = x_141_mode_0, pad = x_141_pad_0, x = x_139_cast_fp16)[name = tensor<string, []>("x_141_cast_fp16")];
            tensor<int32, [4]> var_1259 = const()[name = tensor<string, []>("op_1259"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_143_cast_fp16 = reshape(shape = var_1259, x = x_141_cast_fp16)[name = tensor<string, []>("x_143_cast_fp16")];
            tensor<int32, [4]> var_1263_begin_0 = const()[name = tensor<string, []>("op_1263_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_1263_end_0 = const()[name = tensor<string, []>("op_1263_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_1263_end_mask_0 = const()[name = tensor<string, []>("op_1263_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_1263_cast_fp16 = slice_by_index(begin = var_1263_begin_0, end = var_1263_end_0, end_mask = var_1263_end_mask_0, x = x_143_cast_fp16)[name = tensor<string, []>("op_1263_cast_fp16")];
            tensor<int32, [4]> var_1264 = const()[name = tensor<string, []>("op_1264"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_25_cast_fp16 = reshape(shape = var_1264, x = var_1263_cast_fp16)[name = tensor<string, []>("matrix_bd_25_cast_fp16")];
            tensor<bool, []> matrix_ac_13_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_13_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_13_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_13_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_108_perm_0 = const()[name = tensor<string, []>("transpose_108_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_109_perm_0 = const()[name = tensor<string, []>("transpose_109_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_109 = transpose(perm = transpose_109_perm_0, x = k_25_cast_fp16)[name = tensor<string, []>("transpose_268")];
            tensor<fp16, [1, 8, 188, 128]> transpose_108 = transpose(perm = transpose_108_perm_0, x = var_1247_cast_fp16)[name = tensor<string, []>("transpose_269")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_13_cast_fp16 = matmul(transpose_x = matrix_ac_13_transpose_x_0, transpose_y = matrix_ac_13_transpose_y_0, x = transpose_108, y = transpose_109)[name = tensor<string, []>("matrix_ac_13_cast_fp16")];
            tensor<int32, [4]> matrix_bd_27_begin_0 = const()[name = tensor<string, []>("matrix_bd_27_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_27_end_0 = const()[name = tensor<string, []>("matrix_bd_27_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_27_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_27_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_27_cast_fp16 = slice_by_index(begin = matrix_bd_27_begin_0, end = matrix_bd_27_end_0, end_mask = matrix_bd_27_end_mask_0, x = matrix_bd_25_cast_fp16)[name = tensor<string, []>("matrix_bd_27_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1273_cast_fp16 = add(x = matrix_ac_13_cast_fp16, y = matrix_bd_27_cast_fp16)[name = tensor<string, []>("op_1273_cast_fp16")];
            tensor<fp16, []> _inversed_scores_25_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_25_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_25_cast_fp16 = mul(x = var_1273_cast_fp16, y = _inversed_scores_25_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_25_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_27_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_25_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_27_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1279_cast_fp16 = softmax(axis = var_30, x = scores_27_cast_fp16)[name = tensor<string, []>("op_1279_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_345_cast_fp16 = select(a = var_11_to_fp16, b = var_1279_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_345_cast_fp16")];
            tensor<bool, []> x_145_transpose_x_0 = const()[name = tensor<string, []>("x_145_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_145_transpose_y_0 = const()[name = tensor<string, []>("x_145_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_15_cast_fp16 = transpose(perm = value_15_perm_0, x = v_13_cast_fp16)[name = tensor<string, []>("transpose_267")];
            tensor<fp16, [1, 8, 188, 128]> x_145_cast_fp16 = matmul(transpose_x = x_145_transpose_x_0, transpose_y = x_145_transpose_y_0, x = input_345_cast_fp16, y = value_15_cast_fp16)[name = tensor<string, []>("x_145_cast_fp16")];
            tensor<int32, [4]> var_1283_perm_0 = const()[name = tensor<string, []>("op_1283_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_1284 = const()[name = tensor<string, []>("op_1284"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_1283_cast_fp16 = transpose(perm = var_1283_perm_0, x = x_145_cast_fp16)[name = tensor<string, []>("transpose_266")];
            tensor<fp16, [1, 188, 1024]> input_347_cast_fp16 = reshape(shape = var_1284, x = var_1283_cast_fp16)[name = tensor<string, []>("input_347_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_6_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(122699392))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(123485888))), name = tensor<string, []>("module_layers_6_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_61_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_6_self_attn_linear_out_weight_to_fp16_palettized, x = input_347_cast_fp16)[name = tensor<string, []>("linear_61_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_351_cast_fp16 = add(x = input_343_cast_fp16, y = linear_61_cast_fp16)[name = tensor<string, []>("input_351_cast_fp16")];
            tensor<int32, [1]> x_149_axes_0 = const()[name = tensor<string, []>("x_149_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_6_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_6_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(123486080)))];
            tensor<fp16, [1024]> module_layers_6_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_6_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(123488192)))];
            tensor<fp16, [1, 188, 1024]> x_149_cast_fp16 = layer_norm(axes = x_149_axes_0, beta = module_layers_6_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_6_norm_conv_weight_to_fp16, x = input_351_cast_fp16)[name = tensor<string, []>("x_149_cast_fp16")];
            tensor<int32, [3]> input_353_perm_0 = const()[name = tensor<string, []>("input_353_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_355_pad_type_0 = const()[name = tensor<string, []>("input_355_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_355_strides_0 = const()[name = tensor<string, []>("input_355_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_355_pad_0 = const()[name = tensor<string, []>("input_355_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_355_dilations_0 = const()[name = tensor<string, []>("input_355_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_355_groups_0 = const()[name = tensor<string, []>("input_355_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_6_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(123490304))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(125063232))), name = tensor<string, []>("module_layers_6_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_353_cast_fp16 = transpose(perm = input_353_perm_0, x = x_149_cast_fp16)[name = tensor<string, []>("transpose_265")];
            tensor<fp16, [1, 2048, 188]> input_355_cast_fp16 = conv(dilations = input_355_dilations_0, groups = input_355_groups_0, pad = input_355_pad_0, pad_type = input_355_pad_type_0, strides = input_355_strides_0, weight = module_layers_6_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_353_cast_fp16)[name = tensor<string, []>("input_355_cast_fp16")];
            tensor<int32, []> x_151_split_num_splits_0 = const()[name = tensor<string, []>("x_151_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_151_split_axis_0 = const()[name = tensor<string, []>("x_151_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_151_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_151_split_cast_fp16_1 = split(axis = x_151_split_axis_0, num_splits = x_151_split_num_splits_0, x = input_355_cast_fp16)[name = tensor<string, []>("x_151_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_151_split_1_sigmoid_cast_fp16 = sigmoid(x = x_151_split_cast_fp16_1)[name = tensor<string, []>("x_151_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_151_cast_fp16 = mul(x = x_151_split_cast_fp16_0, y = x_151_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_151_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_357_cast_fp16 = select(a = var_11_to_fp16, b = x_151_cast_fp16, cond = var_328)[name = tensor<string, []>("input_357_cast_fp16")];
            tensor<int32, [6]> input_359_pad_0 = const()[name = tensor<string, []>("input_359_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_359_mode_0 = const()[name = tensor<string, []>("input_359_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_77_to_fp16 = const()[name = tensor<string, []>("const_77_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_359_cast_fp16 = pad(constant_val = const_77_to_fp16, mode = input_359_mode_0, pad = input_359_pad_0, x = input_357_cast_fp16)[name = tensor<string, []>("input_359_cast_fp16")];
            tensor<string, []> input_361_pad_type_0 = const()[name = tensor<string, []>("input_361_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_361_groups_0 = const()[name = tensor<string, []>("input_361_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_361_strides_0 = const()[name = tensor<string, []>("input_361_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_361_pad_0 = const()[name = tensor<string, []>("input_361_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_361_dilations_0 = const()[name = tensor<string, []>("input_361_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_260_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(125063424))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(125070400))), name = tensor<string, []>("const_260_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_261_to_fp16 = const()[name = tensor<string, []>("const_261_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(125070592)))];
            tensor<fp16, [1, 1024, 188]> input_363_cast_fp16 = conv(bias = const_261_to_fp16, dilations = input_361_dilations_0, groups = input_361_groups_0, pad = input_361_pad_0, pad_type = input_361_pad_type_0, strides = input_361_strides_0, weight = const_260_to_fp16_palettized, x = input_359_cast_fp16)[name = tensor<string, []>("input_363_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_365_cast_fp16 = silu(x = input_363_cast_fp16)[name = tensor<string, []>("input_365_cast_fp16")];
            tensor<string, []> x_153_pad_type_0 = const()[name = tensor<string, []>("x_153_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_153_strides_0 = const()[name = tensor<string, []>("x_153_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_153_pad_0 = const()[name = tensor<string, []>("x_153_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_153_dilations_0 = const()[name = tensor<string, []>("x_153_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_153_groups_0 = const()[name = tensor<string, []>("x_153_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_6_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(125072704))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(125859200))), name = tensor<string, []>("module_layers_6_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_153_cast_fp16 = conv(dilations = x_153_dilations_0, groups = x_153_groups_0, pad = x_153_pad_0, pad_type = x_153_pad_type_0, strides = x_153_strides_0, weight = module_layers_6_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_365_cast_fp16)[name = tensor<string, []>("x_153_cast_fp16")];
            tensor<int32, [3]> input_367_perm_0 = const()[name = tensor<string, []>("input_367_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_367_cast_fp16 = transpose(perm = input_367_perm_0, x = x_153_cast_fp16)[name = tensor<string, []>("transpose_264")];
            tensor<fp16, [1, 188, 1024]> input_369_cast_fp16 = add(x = input_351_cast_fp16, y = input_367_cast_fp16)[name = tensor<string, []>("input_369_cast_fp16")];
            tensor<int32, [1]> input_371_axes_0 = const()[name = tensor<string, []>("input_371_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_6_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_6_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(125859392)))];
            tensor<fp16, [1024]> module_layers_6_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_6_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(125861504)))];
            tensor<fp16, [1, 188, 1024]> input_371_cast_fp16 = layer_norm(axes = input_371_axes_0, beta = module_layers_6_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_6_norm_feed_forward2_weight_to_fp16, x = input_369_cast_fp16)[name = tensor<string, []>("input_371_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_6_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(125863616))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(129009408))), name = tensor<string, []>("module_layers_6_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_62_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_6_feed_forward2_linear1_weight_to_fp16_palettized, x = input_371_cast_fp16)[name = tensor<string, []>("linear_62_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_375_cast_fp16 = silu(x = linear_62_cast_fp16)[name = tensor<string, []>("input_375_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_6_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(129009600))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(132155392))), name = tensor<string, []>("module_layers_6_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_63_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_6_feed_forward2_linear2_weight_to_fp16_palettized, x = input_375_cast_fp16)[name = tensor<string, []>("linear_63_cast_fp16")];
            tensor<fp16, []> var_1344_to_fp16 = const()[name = tensor<string, []>("op_1344_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1345_cast_fp16 = mul(x = linear_63_cast_fp16, y = var_1344_to_fp16)[name = tensor<string, []>("op_1345_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_381_cast_fp16 = add(x = input_369_cast_fp16, y = var_1345_cast_fp16)[name = tensor<string, []>("input_381_cast_fp16")];
            tensor<int32, [1]> input_383_axes_0 = const()[name = tensor<string, []>("input_383_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_6_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_6_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(132155584)))];
            tensor<fp16, [1024]> module_layers_6_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_6_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(132157696)))];
            tensor<fp16, [1, 188, 1024]> input_383_cast_fp16 = layer_norm(axes = input_383_axes_0, beta = module_layers_6_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_6_norm_out_weight_to_fp16, x = input_381_cast_fp16)[name = tensor<string, []>("input_383_cast_fp16")];
            tensor<int32, [1]> input_385_axes_0 = const()[name = tensor<string, []>("input_385_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_7_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_7_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(132159808)))];
            tensor<fp16, [1024]> module_layers_7_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_7_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(132161920)))];
            tensor<fp16, [1, 188, 1024]> input_385_cast_fp16 = layer_norm(axes = input_385_axes_0, beta = module_layers_7_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_7_norm_feed_forward1_weight_to_fp16, x = input_383_cast_fp16)[name = tensor<string, []>("input_385_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_7_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(132164032))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(135309824))), name = tensor<string, []>("module_layers_7_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_64_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_7_feed_forward1_linear1_weight_to_fp16_palettized, x = input_385_cast_fp16)[name = tensor<string, []>("linear_64_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_389_cast_fp16 = silu(x = linear_64_cast_fp16)[name = tensor<string, []>("input_389_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_7_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(135310016))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(138455808))), name = tensor<string, []>("module_layers_7_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_65_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_7_feed_forward1_linear2_weight_to_fp16_palettized, x = input_389_cast_fp16)[name = tensor<string, []>("linear_65_cast_fp16")];
            tensor<fp16, []> var_1373_to_fp16 = const()[name = tensor<string, []>("op_1373_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1374_cast_fp16 = mul(x = linear_65_cast_fp16, y = var_1373_to_fp16)[name = tensor<string, []>("op_1374_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_395_cast_fp16 = add(x = input_383_cast_fp16, y = var_1374_cast_fp16)[name = tensor<string, []>("input_395_cast_fp16")];
            tensor<int32, [1]> query_15_axes_0 = const()[name = tensor<string, []>("query_15_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_7_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_7_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(138456000)))];
            tensor<fp16, [1024]> module_layers_7_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_7_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(138458112)))];
            tensor<fp16, [1, 188, 1024]> query_15_cast_fp16 = layer_norm(axes = query_15_axes_0, beta = module_layers_7_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_7_norm_self_att_weight_to_fp16, x = input_395_cast_fp16)[name = tensor<string, []>("query_15_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_7_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(138460224))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(139246720))), name = tensor<string, []>("module_layers_7_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_66_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_7_self_attn_linear_q_weight_to_fp16_palettized, x = query_15_cast_fp16)[name = tensor<string, []>("linear_66_cast_fp16")];
            tensor<int32, [4]> var_1390 = const()[name = tensor<string, []>("op_1390"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_43_cast_fp16 = reshape(shape = var_1390, x = linear_66_cast_fp16)[name = tensor<string, []>("q_43_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_7_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(139246912))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(140033408))), name = tensor<string, []>("module_layers_7_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_67_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_7_self_attn_linear_k_weight_to_fp16_palettized, x = query_15_cast_fp16)[name = tensor<string, []>("linear_67_cast_fp16")];
            tensor<int32, [4]> var_1394 = const()[name = tensor<string, []>("op_1394"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_29_cast_fp16 = reshape(shape = var_1394, x = linear_67_cast_fp16)[name = tensor<string, []>("k_29_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_7_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(140033600))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(140820096))), name = tensor<string, []>("module_layers_7_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_68_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_7_self_attn_linear_v_weight_to_fp16_palettized, x = query_15_cast_fp16)[name = tensor<string, []>("linear_68_cast_fp16")];
            tensor<int32, [4]> var_1398 = const()[name = tensor<string, []>("op_1398"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_15_cast_fp16 = reshape(shape = var_1398, x = linear_68_cast_fp16)[name = tensor<string, []>("v_15_cast_fp16")];
            tensor<int32, [4]> value_17_perm_0 = const()[name = tensor<string, []>("value_17_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_7_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_7_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(140820288)))];
            tensor<fp16, [1, 188, 8, 128]> var_1410_cast_fp16 = add(x = q_43_cast_fp16, y = module_layers_7_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_1410_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_7_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_7_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(140822400)))];
            tensor<fp16, [1, 188, 8, 128]> var_1412_cast_fp16 = add(x = q_43_cast_fp16, y = module_layers_7_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_1412_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_15_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_15_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_161_transpose_x_0 = const()[name = tensor<string, []>("x_161_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_161_transpose_y_0 = const()[name = tensor<string, []>("x_161_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_1414_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(140824512))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(141112576))), name = tensor<string, []>("op_1414_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_15_cast_fp16 = transpose(perm = q_with_bias_v_15_perm_0, x = var_1412_cast_fp16)[name = tensor<string, []>("transpose_263")];
            tensor<fp16, [1, 8, 188, 375]> x_161_cast_fp16 = matmul(transpose_x = x_161_transpose_x_0, transpose_y = x_161_transpose_y_0, x = q_with_bias_v_15_cast_fp16, y = op_1414_to_fp16_palettized)[name = tensor<string, []>("x_161_cast_fp16")];
            tensor<int32, [8]> x_163_pad_0 = const()[name = tensor<string, []>("x_163_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_163_mode_0 = const()[name = tensor<string, []>("x_163_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_84_to_fp16 = const()[name = tensor<string, []>("const_84_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_163_cast_fp16 = pad(constant_val = const_84_to_fp16, mode = x_163_mode_0, pad = x_163_pad_0, x = x_161_cast_fp16)[name = tensor<string, []>("x_163_cast_fp16")];
            tensor<int32, [4]> var_1422 = const()[name = tensor<string, []>("op_1422"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_165_cast_fp16 = reshape(shape = var_1422, x = x_163_cast_fp16)[name = tensor<string, []>("x_165_cast_fp16")];
            tensor<int32, [4]> var_1426_begin_0 = const()[name = tensor<string, []>("op_1426_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_1426_end_0 = const()[name = tensor<string, []>("op_1426_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_1426_end_mask_0 = const()[name = tensor<string, []>("op_1426_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_1426_cast_fp16 = slice_by_index(begin = var_1426_begin_0, end = var_1426_end_0, end_mask = var_1426_end_mask_0, x = x_165_cast_fp16)[name = tensor<string, []>("op_1426_cast_fp16")];
            tensor<int32, [4]> var_1427 = const()[name = tensor<string, []>("op_1427"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_29_cast_fp16 = reshape(shape = var_1427, x = var_1426_cast_fp16)[name = tensor<string, []>("matrix_bd_29_cast_fp16")];
            tensor<bool, []> matrix_ac_15_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_15_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_15_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_15_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_110_perm_0 = const()[name = tensor<string, []>("transpose_110_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_111_perm_0 = const()[name = tensor<string, []>("transpose_111_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_111 = transpose(perm = transpose_111_perm_0, x = k_29_cast_fp16)[name = tensor<string, []>("transpose_261")];
            tensor<fp16, [1, 8, 188, 128]> transpose_110 = transpose(perm = transpose_110_perm_0, x = var_1410_cast_fp16)[name = tensor<string, []>("transpose_262")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_15_cast_fp16 = matmul(transpose_x = matrix_ac_15_transpose_x_0, transpose_y = matrix_ac_15_transpose_y_0, x = transpose_110, y = transpose_111)[name = tensor<string, []>("matrix_ac_15_cast_fp16")];
            tensor<int32, [4]> matrix_bd_31_begin_0 = const()[name = tensor<string, []>("matrix_bd_31_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_31_end_0 = const()[name = tensor<string, []>("matrix_bd_31_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_31_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_31_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_31_cast_fp16 = slice_by_index(begin = matrix_bd_31_begin_0, end = matrix_bd_31_end_0, end_mask = matrix_bd_31_end_mask_0, x = matrix_bd_29_cast_fp16)[name = tensor<string, []>("matrix_bd_31_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1436_cast_fp16 = add(x = matrix_ac_15_cast_fp16, y = matrix_bd_31_cast_fp16)[name = tensor<string, []>("op_1436_cast_fp16")];
            tensor<fp16, []> _inversed_scores_29_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_29_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_29_cast_fp16 = mul(x = var_1436_cast_fp16, y = _inversed_scores_29_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_29_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_31_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_29_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_31_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1442_cast_fp16 = softmax(axis = var_30, x = scores_31_cast_fp16)[name = tensor<string, []>("op_1442_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_397_cast_fp16 = select(a = var_11_to_fp16, b = var_1442_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_397_cast_fp16")];
            tensor<bool, []> x_167_transpose_x_0 = const()[name = tensor<string, []>("x_167_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_167_transpose_y_0 = const()[name = tensor<string, []>("x_167_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_17_cast_fp16 = transpose(perm = value_17_perm_0, x = v_15_cast_fp16)[name = tensor<string, []>("transpose_260")];
            tensor<fp16, [1, 8, 188, 128]> x_167_cast_fp16 = matmul(transpose_x = x_167_transpose_x_0, transpose_y = x_167_transpose_y_0, x = input_397_cast_fp16, y = value_17_cast_fp16)[name = tensor<string, []>("x_167_cast_fp16")];
            tensor<int32, [4]> var_1446_perm_0 = const()[name = tensor<string, []>("op_1446_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_1447 = const()[name = tensor<string, []>("op_1447"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_1446_cast_fp16 = transpose(perm = var_1446_perm_0, x = x_167_cast_fp16)[name = tensor<string, []>("transpose_259")];
            tensor<fp16, [1, 188, 1024]> input_399_cast_fp16 = reshape(shape = var_1447, x = var_1446_cast_fp16)[name = tensor<string, []>("input_399_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_7_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(141112768))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(141899264))), name = tensor<string, []>("module_layers_7_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_70_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_7_self_attn_linear_out_weight_to_fp16_palettized, x = input_399_cast_fp16)[name = tensor<string, []>("linear_70_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_403_cast_fp16 = add(x = input_395_cast_fp16, y = linear_70_cast_fp16)[name = tensor<string, []>("input_403_cast_fp16")];
            tensor<int32, [1]> x_171_axes_0 = const()[name = tensor<string, []>("x_171_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_7_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_7_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(141899456)))];
            tensor<fp16, [1024]> module_layers_7_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_7_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(141901568)))];
            tensor<fp16, [1, 188, 1024]> x_171_cast_fp16 = layer_norm(axes = x_171_axes_0, beta = module_layers_7_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_7_norm_conv_weight_to_fp16, x = input_403_cast_fp16)[name = tensor<string, []>("x_171_cast_fp16")];
            tensor<int32, [3]> input_405_perm_0 = const()[name = tensor<string, []>("input_405_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_407_pad_type_0 = const()[name = tensor<string, []>("input_407_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_407_strides_0 = const()[name = tensor<string, []>("input_407_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_407_pad_0 = const()[name = tensor<string, []>("input_407_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_407_dilations_0 = const()[name = tensor<string, []>("input_407_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_407_groups_0 = const()[name = tensor<string, []>("input_407_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_7_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(141903680))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(143476608))), name = tensor<string, []>("module_layers_7_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_405_cast_fp16 = transpose(perm = input_405_perm_0, x = x_171_cast_fp16)[name = tensor<string, []>("transpose_258")];
            tensor<fp16, [1, 2048, 188]> input_407_cast_fp16 = conv(dilations = input_407_dilations_0, groups = input_407_groups_0, pad = input_407_pad_0, pad_type = input_407_pad_type_0, strides = input_407_strides_0, weight = module_layers_7_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_405_cast_fp16)[name = tensor<string, []>("input_407_cast_fp16")];
            tensor<int32, []> x_173_split_num_splits_0 = const()[name = tensor<string, []>("x_173_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_173_split_axis_0 = const()[name = tensor<string, []>("x_173_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_173_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_173_split_cast_fp16_1 = split(axis = x_173_split_axis_0, num_splits = x_173_split_num_splits_0, x = input_407_cast_fp16)[name = tensor<string, []>("x_173_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_173_split_1_sigmoid_cast_fp16 = sigmoid(x = x_173_split_cast_fp16_1)[name = tensor<string, []>("x_173_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_173_cast_fp16 = mul(x = x_173_split_cast_fp16_0, y = x_173_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_173_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_409_cast_fp16 = select(a = var_11_to_fp16, b = x_173_cast_fp16, cond = var_328)[name = tensor<string, []>("input_409_cast_fp16")];
            tensor<int32, [6]> input_411_pad_0 = const()[name = tensor<string, []>("input_411_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_411_mode_0 = const()[name = tensor<string, []>("input_411_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_87_to_fp16 = const()[name = tensor<string, []>("const_87_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_411_cast_fp16 = pad(constant_val = const_87_to_fp16, mode = input_411_mode_0, pad = input_411_pad_0, x = input_409_cast_fp16)[name = tensor<string, []>("input_411_cast_fp16")];
            tensor<string, []> input_413_pad_type_0 = const()[name = tensor<string, []>("input_413_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_413_groups_0 = const()[name = tensor<string, []>("input_413_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_413_strides_0 = const()[name = tensor<string, []>("input_413_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_413_pad_0 = const()[name = tensor<string, []>("input_413_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_413_dilations_0 = const()[name = tensor<string, []>("input_413_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_262_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(143476800))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(143483776))), name = tensor<string, []>("const_262_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_263_to_fp16 = const()[name = tensor<string, []>("const_263_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(143483968)))];
            tensor<fp16, [1, 1024, 188]> input_415_cast_fp16 = conv(bias = const_263_to_fp16, dilations = input_413_dilations_0, groups = input_413_groups_0, pad = input_413_pad_0, pad_type = input_413_pad_type_0, strides = input_413_strides_0, weight = const_262_to_fp16_palettized, x = input_411_cast_fp16)[name = tensor<string, []>("input_415_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_417_cast_fp16 = silu(x = input_415_cast_fp16)[name = tensor<string, []>("input_417_cast_fp16")];
            tensor<string, []> x_175_pad_type_0 = const()[name = tensor<string, []>("x_175_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_175_strides_0 = const()[name = tensor<string, []>("x_175_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_175_pad_0 = const()[name = tensor<string, []>("x_175_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_175_dilations_0 = const()[name = tensor<string, []>("x_175_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_175_groups_0 = const()[name = tensor<string, []>("x_175_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_7_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(143486080))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(144272576))), name = tensor<string, []>("module_layers_7_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_175_cast_fp16 = conv(dilations = x_175_dilations_0, groups = x_175_groups_0, pad = x_175_pad_0, pad_type = x_175_pad_type_0, strides = x_175_strides_0, weight = module_layers_7_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_417_cast_fp16)[name = tensor<string, []>("x_175_cast_fp16")];
            tensor<int32, [3]> input_419_perm_0 = const()[name = tensor<string, []>("input_419_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_419_cast_fp16 = transpose(perm = input_419_perm_0, x = x_175_cast_fp16)[name = tensor<string, []>("transpose_257")];
            tensor<fp16, [1, 188, 1024]> input_421_cast_fp16 = add(x = input_403_cast_fp16, y = input_419_cast_fp16)[name = tensor<string, []>("input_421_cast_fp16")];
            tensor<int32, [1]> input_423_axes_0 = const()[name = tensor<string, []>("input_423_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_7_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_7_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(144272768)))];
            tensor<fp16, [1024]> module_layers_7_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_7_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(144274880)))];
            tensor<fp16, [1, 188, 1024]> input_423_cast_fp16 = layer_norm(axes = input_423_axes_0, beta = module_layers_7_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_7_norm_feed_forward2_weight_to_fp16, x = input_421_cast_fp16)[name = tensor<string, []>("input_423_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_7_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(144276992))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(147422784))), name = tensor<string, []>("module_layers_7_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_71_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_7_feed_forward2_linear1_weight_to_fp16_palettized, x = input_423_cast_fp16)[name = tensor<string, []>("linear_71_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_427_cast_fp16 = silu(x = linear_71_cast_fp16)[name = tensor<string, []>("input_427_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_7_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(147422976))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(150568768))), name = tensor<string, []>("module_layers_7_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_72_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_7_feed_forward2_linear2_weight_to_fp16_palettized, x = input_427_cast_fp16)[name = tensor<string, []>("linear_72_cast_fp16")];
            tensor<fp16, []> var_1507_to_fp16 = const()[name = tensor<string, []>("op_1507_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1508_cast_fp16 = mul(x = linear_72_cast_fp16, y = var_1507_to_fp16)[name = tensor<string, []>("op_1508_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_433_cast_fp16 = add(x = input_421_cast_fp16, y = var_1508_cast_fp16)[name = tensor<string, []>("input_433_cast_fp16")];
            tensor<int32, [1]> input_435_axes_0 = const()[name = tensor<string, []>("input_435_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_7_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_7_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(150568960)))];
            tensor<fp16, [1024]> module_layers_7_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_7_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(150571072)))];
            tensor<fp16, [1, 188, 1024]> input_435_cast_fp16 = layer_norm(axes = input_435_axes_0, beta = module_layers_7_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_7_norm_out_weight_to_fp16, x = input_433_cast_fp16)[name = tensor<string, []>("input_435_cast_fp16")];
            tensor<int32, [1]> input_437_axes_0 = const()[name = tensor<string, []>("input_437_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_8_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_8_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(150573184)))];
            tensor<fp16, [1024]> module_layers_8_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_8_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(150575296)))];
            tensor<fp16, [1, 188, 1024]> input_437_cast_fp16 = layer_norm(axes = input_437_axes_0, beta = module_layers_8_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_8_norm_feed_forward1_weight_to_fp16, x = input_435_cast_fp16)[name = tensor<string, []>("input_437_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_8_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(150577408))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(153723200))), name = tensor<string, []>("module_layers_8_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_73_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_8_feed_forward1_linear1_weight_to_fp16_palettized, x = input_437_cast_fp16)[name = tensor<string, []>("linear_73_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_441_cast_fp16 = silu(x = linear_73_cast_fp16)[name = tensor<string, []>("input_441_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_8_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(153723392))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(156869184))), name = tensor<string, []>("module_layers_8_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_74_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_8_feed_forward1_linear2_weight_to_fp16_palettized, x = input_441_cast_fp16)[name = tensor<string, []>("linear_74_cast_fp16")];
            tensor<fp16, []> var_1536_to_fp16 = const()[name = tensor<string, []>("op_1536_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1537_cast_fp16 = mul(x = linear_74_cast_fp16, y = var_1536_to_fp16)[name = tensor<string, []>("op_1537_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_447_cast_fp16 = add(x = input_435_cast_fp16, y = var_1537_cast_fp16)[name = tensor<string, []>("input_447_cast_fp16")];
            tensor<int32, [1]> query_17_axes_0 = const()[name = tensor<string, []>("query_17_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_8_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_8_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(156869376)))];
            tensor<fp16, [1024]> module_layers_8_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_8_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(156871488)))];
            tensor<fp16, [1, 188, 1024]> query_17_cast_fp16 = layer_norm(axes = query_17_axes_0, beta = module_layers_8_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_8_norm_self_att_weight_to_fp16, x = input_447_cast_fp16)[name = tensor<string, []>("query_17_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_8_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(156873600))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(157660096))), name = tensor<string, []>("module_layers_8_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_75_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_8_self_attn_linear_q_weight_to_fp16_palettized, x = query_17_cast_fp16)[name = tensor<string, []>("linear_75_cast_fp16")];
            tensor<int32, [4]> var_1553 = const()[name = tensor<string, []>("op_1553"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_49_cast_fp16 = reshape(shape = var_1553, x = linear_75_cast_fp16)[name = tensor<string, []>("q_49_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_8_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(157660288))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(158446784))), name = tensor<string, []>("module_layers_8_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_76_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_8_self_attn_linear_k_weight_to_fp16_palettized, x = query_17_cast_fp16)[name = tensor<string, []>("linear_76_cast_fp16")];
            tensor<int32, [4]> var_1557 = const()[name = tensor<string, []>("op_1557"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_33_cast_fp16 = reshape(shape = var_1557, x = linear_76_cast_fp16)[name = tensor<string, []>("k_33_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_8_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(158446976))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(159233472))), name = tensor<string, []>("module_layers_8_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_77_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_8_self_attn_linear_v_weight_to_fp16_palettized, x = query_17_cast_fp16)[name = tensor<string, []>("linear_77_cast_fp16")];
            tensor<int32, [4]> var_1561 = const()[name = tensor<string, []>("op_1561"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_17_cast_fp16 = reshape(shape = var_1561, x = linear_77_cast_fp16)[name = tensor<string, []>("v_17_cast_fp16")];
            tensor<int32, [4]> value_19_perm_0 = const()[name = tensor<string, []>("value_19_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_8_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_8_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(159233664)))];
            tensor<fp16, [1, 188, 8, 128]> var_1573_cast_fp16 = add(x = q_49_cast_fp16, y = module_layers_8_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_1573_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_8_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_8_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(159235776)))];
            tensor<fp16, [1, 188, 8, 128]> var_1575_cast_fp16 = add(x = q_49_cast_fp16, y = module_layers_8_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_1575_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_17_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_17_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_183_transpose_x_0 = const()[name = tensor<string, []>("x_183_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_183_transpose_y_0 = const()[name = tensor<string, []>("x_183_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_1577_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(159237888))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(159525952))), name = tensor<string, []>("op_1577_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_17_cast_fp16 = transpose(perm = q_with_bias_v_17_perm_0, x = var_1575_cast_fp16)[name = tensor<string, []>("transpose_256")];
            tensor<fp16, [1, 8, 188, 375]> x_183_cast_fp16 = matmul(transpose_x = x_183_transpose_x_0, transpose_y = x_183_transpose_y_0, x = q_with_bias_v_17_cast_fp16, y = op_1577_to_fp16_palettized)[name = tensor<string, []>("x_183_cast_fp16")];
            tensor<int32, [8]> x_185_pad_0 = const()[name = tensor<string, []>("x_185_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_185_mode_0 = const()[name = tensor<string, []>("x_185_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_94_to_fp16 = const()[name = tensor<string, []>("const_94_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_185_cast_fp16 = pad(constant_val = const_94_to_fp16, mode = x_185_mode_0, pad = x_185_pad_0, x = x_183_cast_fp16)[name = tensor<string, []>("x_185_cast_fp16")];
            tensor<int32, [4]> var_1585 = const()[name = tensor<string, []>("op_1585"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_187_cast_fp16 = reshape(shape = var_1585, x = x_185_cast_fp16)[name = tensor<string, []>("x_187_cast_fp16")];
            tensor<int32, [4]> var_1589_begin_0 = const()[name = tensor<string, []>("op_1589_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_1589_end_0 = const()[name = tensor<string, []>("op_1589_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_1589_end_mask_0 = const()[name = tensor<string, []>("op_1589_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_1589_cast_fp16 = slice_by_index(begin = var_1589_begin_0, end = var_1589_end_0, end_mask = var_1589_end_mask_0, x = x_187_cast_fp16)[name = tensor<string, []>("op_1589_cast_fp16")];
            tensor<int32, [4]> var_1590 = const()[name = tensor<string, []>("op_1590"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_33_cast_fp16 = reshape(shape = var_1590, x = var_1589_cast_fp16)[name = tensor<string, []>("matrix_bd_33_cast_fp16")];
            tensor<bool, []> matrix_ac_17_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_17_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_17_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_17_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_112_perm_0 = const()[name = tensor<string, []>("transpose_112_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_113_perm_0 = const()[name = tensor<string, []>("transpose_113_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_113 = transpose(perm = transpose_113_perm_0, x = k_33_cast_fp16)[name = tensor<string, []>("transpose_254")];
            tensor<fp16, [1, 8, 188, 128]> transpose_112 = transpose(perm = transpose_112_perm_0, x = var_1573_cast_fp16)[name = tensor<string, []>("transpose_255")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_17_cast_fp16 = matmul(transpose_x = matrix_ac_17_transpose_x_0, transpose_y = matrix_ac_17_transpose_y_0, x = transpose_112, y = transpose_113)[name = tensor<string, []>("matrix_ac_17_cast_fp16")];
            tensor<int32, [4]> matrix_bd_35_begin_0 = const()[name = tensor<string, []>("matrix_bd_35_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_35_end_0 = const()[name = tensor<string, []>("matrix_bd_35_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_35_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_35_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_35_cast_fp16 = slice_by_index(begin = matrix_bd_35_begin_0, end = matrix_bd_35_end_0, end_mask = matrix_bd_35_end_mask_0, x = matrix_bd_33_cast_fp16)[name = tensor<string, []>("matrix_bd_35_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1599_cast_fp16 = add(x = matrix_ac_17_cast_fp16, y = matrix_bd_35_cast_fp16)[name = tensor<string, []>("op_1599_cast_fp16")];
            tensor<fp16, []> _inversed_scores_33_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_33_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_33_cast_fp16 = mul(x = var_1599_cast_fp16, y = _inversed_scores_33_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_33_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_35_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_33_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_35_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1605_cast_fp16 = softmax(axis = var_30, x = scores_35_cast_fp16)[name = tensor<string, []>("op_1605_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_449_cast_fp16 = select(a = var_11_to_fp16, b = var_1605_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_449_cast_fp16")];
            tensor<bool, []> x_189_transpose_x_0 = const()[name = tensor<string, []>("x_189_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_189_transpose_y_0 = const()[name = tensor<string, []>("x_189_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_19_cast_fp16 = transpose(perm = value_19_perm_0, x = v_17_cast_fp16)[name = tensor<string, []>("transpose_253")];
            tensor<fp16, [1, 8, 188, 128]> x_189_cast_fp16 = matmul(transpose_x = x_189_transpose_x_0, transpose_y = x_189_transpose_y_0, x = input_449_cast_fp16, y = value_19_cast_fp16)[name = tensor<string, []>("x_189_cast_fp16")];
            tensor<int32, [4]> var_1609_perm_0 = const()[name = tensor<string, []>("op_1609_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_1610 = const()[name = tensor<string, []>("op_1610"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_1609_cast_fp16 = transpose(perm = var_1609_perm_0, x = x_189_cast_fp16)[name = tensor<string, []>("transpose_252")];
            tensor<fp16, [1, 188, 1024]> input_451_cast_fp16 = reshape(shape = var_1610, x = var_1609_cast_fp16)[name = tensor<string, []>("input_451_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_8_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(159526144))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(160312640))), name = tensor<string, []>("module_layers_8_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_79_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_8_self_attn_linear_out_weight_to_fp16_palettized, x = input_451_cast_fp16)[name = tensor<string, []>("linear_79_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_455_cast_fp16 = add(x = input_447_cast_fp16, y = linear_79_cast_fp16)[name = tensor<string, []>("input_455_cast_fp16")];
            tensor<int32, [1]> x_193_axes_0 = const()[name = tensor<string, []>("x_193_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_8_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_8_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(160312832)))];
            tensor<fp16, [1024]> module_layers_8_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_8_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(160314944)))];
            tensor<fp16, [1, 188, 1024]> x_193_cast_fp16 = layer_norm(axes = x_193_axes_0, beta = module_layers_8_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_8_norm_conv_weight_to_fp16, x = input_455_cast_fp16)[name = tensor<string, []>("x_193_cast_fp16")];
            tensor<int32, [3]> input_457_perm_0 = const()[name = tensor<string, []>("input_457_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_459_pad_type_0 = const()[name = tensor<string, []>("input_459_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_459_strides_0 = const()[name = tensor<string, []>("input_459_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_459_pad_0 = const()[name = tensor<string, []>("input_459_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_459_dilations_0 = const()[name = tensor<string, []>("input_459_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_459_groups_0 = const()[name = tensor<string, []>("input_459_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_8_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(160317056))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(161889984))), name = tensor<string, []>("module_layers_8_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_457_cast_fp16 = transpose(perm = input_457_perm_0, x = x_193_cast_fp16)[name = tensor<string, []>("transpose_251")];
            tensor<fp16, [1, 2048, 188]> input_459_cast_fp16 = conv(dilations = input_459_dilations_0, groups = input_459_groups_0, pad = input_459_pad_0, pad_type = input_459_pad_type_0, strides = input_459_strides_0, weight = module_layers_8_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_457_cast_fp16)[name = tensor<string, []>("input_459_cast_fp16")];
            tensor<int32, []> x_195_split_num_splits_0 = const()[name = tensor<string, []>("x_195_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_195_split_axis_0 = const()[name = tensor<string, []>("x_195_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_195_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_195_split_cast_fp16_1 = split(axis = x_195_split_axis_0, num_splits = x_195_split_num_splits_0, x = input_459_cast_fp16)[name = tensor<string, []>("x_195_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_195_split_1_sigmoid_cast_fp16 = sigmoid(x = x_195_split_cast_fp16_1)[name = tensor<string, []>("x_195_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_195_cast_fp16 = mul(x = x_195_split_cast_fp16_0, y = x_195_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_195_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_461_cast_fp16 = select(a = var_11_to_fp16, b = x_195_cast_fp16, cond = var_328)[name = tensor<string, []>("input_461_cast_fp16")];
            tensor<int32, [6]> input_463_pad_0 = const()[name = tensor<string, []>("input_463_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_463_mode_0 = const()[name = tensor<string, []>("input_463_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_97_to_fp16 = const()[name = tensor<string, []>("const_97_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_463_cast_fp16 = pad(constant_val = const_97_to_fp16, mode = input_463_mode_0, pad = input_463_pad_0, x = input_461_cast_fp16)[name = tensor<string, []>("input_463_cast_fp16")];
            tensor<string, []> input_465_pad_type_0 = const()[name = tensor<string, []>("input_465_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_465_groups_0 = const()[name = tensor<string, []>("input_465_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_465_strides_0 = const()[name = tensor<string, []>("input_465_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_465_pad_0 = const()[name = tensor<string, []>("input_465_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_465_dilations_0 = const()[name = tensor<string, []>("input_465_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_264_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(161890176))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(161897152))), name = tensor<string, []>("const_264_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_265_to_fp16 = const()[name = tensor<string, []>("const_265_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(161897344)))];
            tensor<fp16, [1, 1024, 188]> input_467_cast_fp16 = conv(bias = const_265_to_fp16, dilations = input_465_dilations_0, groups = input_465_groups_0, pad = input_465_pad_0, pad_type = input_465_pad_type_0, strides = input_465_strides_0, weight = const_264_to_fp16_palettized, x = input_463_cast_fp16)[name = tensor<string, []>("input_467_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_469_cast_fp16 = silu(x = input_467_cast_fp16)[name = tensor<string, []>("input_469_cast_fp16")];
            tensor<string, []> x_197_pad_type_0 = const()[name = tensor<string, []>("x_197_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_197_strides_0 = const()[name = tensor<string, []>("x_197_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_197_pad_0 = const()[name = tensor<string, []>("x_197_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_197_dilations_0 = const()[name = tensor<string, []>("x_197_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_197_groups_0 = const()[name = tensor<string, []>("x_197_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_8_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(161899456))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(162685952))), name = tensor<string, []>("module_layers_8_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_197_cast_fp16 = conv(dilations = x_197_dilations_0, groups = x_197_groups_0, pad = x_197_pad_0, pad_type = x_197_pad_type_0, strides = x_197_strides_0, weight = module_layers_8_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_469_cast_fp16)[name = tensor<string, []>("x_197_cast_fp16")];
            tensor<int32, [3]> input_471_perm_0 = const()[name = tensor<string, []>("input_471_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_471_cast_fp16 = transpose(perm = input_471_perm_0, x = x_197_cast_fp16)[name = tensor<string, []>("transpose_250")];
            tensor<fp16, [1, 188, 1024]> input_473_cast_fp16 = add(x = input_455_cast_fp16, y = input_471_cast_fp16)[name = tensor<string, []>("input_473_cast_fp16")];
            tensor<int32, [1]> input_475_axes_0 = const()[name = tensor<string, []>("input_475_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_8_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_8_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(162686144)))];
            tensor<fp16, [1024]> module_layers_8_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_8_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(162688256)))];
            tensor<fp16, [1, 188, 1024]> input_475_cast_fp16 = layer_norm(axes = input_475_axes_0, beta = module_layers_8_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_8_norm_feed_forward2_weight_to_fp16, x = input_473_cast_fp16)[name = tensor<string, []>("input_475_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_8_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(162690368))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(165836160))), name = tensor<string, []>("module_layers_8_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_80_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_8_feed_forward2_linear1_weight_to_fp16_palettized, x = input_475_cast_fp16)[name = tensor<string, []>("linear_80_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_479_cast_fp16 = silu(x = linear_80_cast_fp16)[name = tensor<string, []>("input_479_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_8_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(165836352))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(168982144))), name = tensor<string, []>("module_layers_8_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_81_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_8_feed_forward2_linear2_weight_to_fp16_palettized, x = input_479_cast_fp16)[name = tensor<string, []>("linear_81_cast_fp16")];
            tensor<fp16, []> var_1670_to_fp16 = const()[name = tensor<string, []>("op_1670_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1671_cast_fp16 = mul(x = linear_81_cast_fp16, y = var_1670_to_fp16)[name = tensor<string, []>("op_1671_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_485_cast_fp16 = add(x = input_473_cast_fp16, y = var_1671_cast_fp16)[name = tensor<string, []>("input_485_cast_fp16")];
            tensor<int32, [1]> input_487_axes_0 = const()[name = tensor<string, []>("input_487_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_8_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_8_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(168982336)))];
            tensor<fp16, [1024]> module_layers_8_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_8_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(168984448)))];
            tensor<fp16, [1, 188, 1024]> input_487_cast_fp16 = layer_norm(axes = input_487_axes_0, beta = module_layers_8_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_8_norm_out_weight_to_fp16, x = input_485_cast_fp16)[name = tensor<string, []>("input_487_cast_fp16")];
            tensor<int32, [1]> input_489_axes_0 = const()[name = tensor<string, []>("input_489_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_9_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_9_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(168986560)))];
            tensor<fp16, [1024]> module_layers_9_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_9_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(168988672)))];
            tensor<fp16, [1, 188, 1024]> input_489_cast_fp16 = layer_norm(axes = input_489_axes_0, beta = module_layers_9_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_9_norm_feed_forward1_weight_to_fp16, x = input_487_cast_fp16)[name = tensor<string, []>("input_489_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_9_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(168990784))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(172136576))), name = tensor<string, []>("module_layers_9_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_82_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_9_feed_forward1_linear1_weight_to_fp16_palettized, x = input_489_cast_fp16)[name = tensor<string, []>("linear_82_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_493_cast_fp16 = silu(x = linear_82_cast_fp16)[name = tensor<string, []>("input_493_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_9_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(172136768))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(175282560))), name = tensor<string, []>("module_layers_9_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_83_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_9_feed_forward1_linear2_weight_to_fp16_palettized, x = input_493_cast_fp16)[name = tensor<string, []>("linear_83_cast_fp16")];
            tensor<fp16, []> var_1699_to_fp16 = const()[name = tensor<string, []>("op_1699_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1700_cast_fp16 = mul(x = linear_83_cast_fp16, y = var_1699_to_fp16)[name = tensor<string, []>("op_1700_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_499_cast_fp16 = add(x = input_487_cast_fp16, y = var_1700_cast_fp16)[name = tensor<string, []>("input_499_cast_fp16")];
            tensor<int32, [1]> query_19_axes_0 = const()[name = tensor<string, []>("query_19_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_9_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_9_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(175282752)))];
            tensor<fp16, [1024]> module_layers_9_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_9_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(175284864)))];
            tensor<fp16, [1, 188, 1024]> query_19_cast_fp16 = layer_norm(axes = query_19_axes_0, beta = module_layers_9_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_9_norm_self_att_weight_to_fp16, x = input_499_cast_fp16)[name = tensor<string, []>("query_19_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_9_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(175286976))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(176073472))), name = tensor<string, []>("module_layers_9_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_84_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_9_self_attn_linear_q_weight_to_fp16_palettized, x = query_19_cast_fp16)[name = tensor<string, []>("linear_84_cast_fp16")];
            tensor<int32, [4]> var_1716 = const()[name = tensor<string, []>("op_1716"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_55_cast_fp16 = reshape(shape = var_1716, x = linear_84_cast_fp16)[name = tensor<string, []>("q_55_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_9_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(176073664))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(176860160))), name = tensor<string, []>("module_layers_9_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_85_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_9_self_attn_linear_k_weight_to_fp16_palettized, x = query_19_cast_fp16)[name = tensor<string, []>("linear_85_cast_fp16")];
            tensor<int32, [4]> var_1720 = const()[name = tensor<string, []>("op_1720"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_37_cast_fp16 = reshape(shape = var_1720, x = linear_85_cast_fp16)[name = tensor<string, []>("k_37_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_9_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(176860352))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(177646848))), name = tensor<string, []>("module_layers_9_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_86_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_9_self_attn_linear_v_weight_to_fp16_palettized, x = query_19_cast_fp16)[name = tensor<string, []>("linear_86_cast_fp16")];
            tensor<int32, [4]> var_1724 = const()[name = tensor<string, []>("op_1724"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_19_cast_fp16 = reshape(shape = var_1724, x = linear_86_cast_fp16)[name = tensor<string, []>("v_19_cast_fp16")];
            tensor<int32, [4]> value_21_perm_0 = const()[name = tensor<string, []>("value_21_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_9_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_9_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(177647040)))];
            tensor<fp16, [1, 188, 8, 128]> var_1736_cast_fp16 = add(x = q_55_cast_fp16, y = module_layers_9_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_1736_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_9_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_9_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(177649152)))];
            tensor<fp16, [1, 188, 8, 128]> var_1738_cast_fp16 = add(x = q_55_cast_fp16, y = module_layers_9_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_1738_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_19_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_19_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_205_transpose_x_0 = const()[name = tensor<string, []>("x_205_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_205_transpose_y_0 = const()[name = tensor<string, []>("x_205_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_1740_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(177651264))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(177939328))), name = tensor<string, []>("op_1740_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_19_cast_fp16 = transpose(perm = q_with_bias_v_19_perm_0, x = var_1738_cast_fp16)[name = tensor<string, []>("transpose_249")];
            tensor<fp16, [1, 8, 188, 375]> x_205_cast_fp16 = matmul(transpose_x = x_205_transpose_x_0, transpose_y = x_205_transpose_y_0, x = q_with_bias_v_19_cast_fp16, y = op_1740_to_fp16_palettized)[name = tensor<string, []>("x_205_cast_fp16")];
            tensor<int32, [8]> x_207_pad_0 = const()[name = tensor<string, []>("x_207_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_207_mode_0 = const()[name = tensor<string, []>("x_207_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_104_to_fp16 = const()[name = tensor<string, []>("const_104_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_207_cast_fp16 = pad(constant_val = const_104_to_fp16, mode = x_207_mode_0, pad = x_207_pad_0, x = x_205_cast_fp16)[name = tensor<string, []>("x_207_cast_fp16")];
            tensor<int32, [4]> var_1748 = const()[name = tensor<string, []>("op_1748"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_209_cast_fp16 = reshape(shape = var_1748, x = x_207_cast_fp16)[name = tensor<string, []>("x_209_cast_fp16")];
            tensor<int32, [4]> var_1752_begin_0 = const()[name = tensor<string, []>("op_1752_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_1752_end_0 = const()[name = tensor<string, []>("op_1752_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_1752_end_mask_0 = const()[name = tensor<string, []>("op_1752_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_1752_cast_fp16 = slice_by_index(begin = var_1752_begin_0, end = var_1752_end_0, end_mask = var_1752_end_mask_0, x = x_209_cast_fp16)[name = tensor<string, []>("op_1752_cast_fp16")];
            tensor<int32, [4]> var_1753 = const()[name = tensor<string, []>("op_1753"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_37_cast_fp16 = reshape(shape = var_1753, x = var_1752_cast_fp16)[name = tensor<string, []>("matrix_bd_37_cast_fp16")];
            tensor<bool, []> matrix_ac_19_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_19_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_19_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_19_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_114_perm_0 = const()[name = tensor<string, []>("transpose_114_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_115_perm_0 = const()[name = tensor<string, []>("transpose_115_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_115 = transpose(perm = transpose_115_perm_0, x = k_37_cast_fp16)[name = tensor<string, []>("transpose_247")];
            tensor<fp16, [1, 8, 188, 128]> transpose_114 = transpose(perm = transpose_114_perm_0, x = var_1736_cast_fp16)[name = tensor<string, []>("transpose_248")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_19_cast_fp16 = matmul(transpose_x = matrix_ac_19_transpose_x_0, transpose_y = matrix_ac_19_transpose_y_0, x = transpose_114, y = transpose_115)[name = tensor<string, []>("matrix_ac_19_cast_fp16")];
            tensor<int32, [4]> matrix_bd_39_begin_0 = const()[name = tensor<string, []>("matrix_bd_39_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_39_end_0 = const()[name = tensor<string, []>("matrix_bd_39_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_39_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_39_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_39_cast_fp16 = slice_by_index(begin = matrix_bd_39_begin_0, end = matrix_bd_39_end_0, end_mask = matrix_bd_39_end_mask_0, x = matrix_bd_37_cast_fp16)[name = tensor<string, []>("matrix_bd_39_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1762_cast_fp16 = add(x = matrix_ac_19_cast_fp16, y = matrix_bd_39_cast_fp16)[name = tensor<string, []>("op_1762_cast_fp16")];
            tensor<fp16, []> _inversed_scores_37_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_37_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_37_cast_fp16 = mul(x = var_1762_cast_fp16, y = _inversed_scores_37_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_37_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_39_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_37_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_39_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1768_cast_fp16 = softmax(axis = var_30, x = scores_39_cast_fp16)[name = tensor<string, []>("op_1768_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_501_cast_fp16 = select(a = var_11_to_fp16, b = var_1768_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_501_cast_fp16")];
            tensor<bool, []> x_211_transpose_x_0 = const()[name = tensor<string, []>("x_211_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_211_transpose_y_0 = const()[name = tensor<string, []>("x_211_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_21_cast_fp16 = transpose(perm = value_21_perm_0, x = v_19_cast_fp16)[name = tensor<string, []>("transpose_246")];
            tensor<fp16, [1, 8, 188, 128]> x_211_cast_fp16 = matmul(transpose_x = x_211_transpose_x_0, transpose_y = x_211_transpose_y_0, x = input_501_cast_fp16, y = value_21_cast_fp16)[name = tensor<string, []>("x_211_cast_fp16")];
            tensor<int32, [4]> var_1772_perm_0 = const()[name = tensor<string, []>("op_1772_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_1773 = const()[name = tensor<string, []>("op_1773"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_1772_cast_fp16 = transpose(perm = var_1772_perm_0, x = x_211_cast_fp16)[name = tensor<string, []>("transpose_245")];
            tensor<fp16, [1, 188, 1024]> input_503_cast_fp16 = reshape(shape = var_1773, x = var_1772_cast_fp16)[name = tensor<string, []>("input_503_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_9_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(177939520))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(178726016))), name = tensor<string, []>("module_layers_9_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_88_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_9_self_attn_linear_out_weight_to_fp16_palettized, x = input_503_cast_fp16)[name = tensor<string, []>("linear_88_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_507_cast_fp16 = add(x = input_499_cast_fp16, y = linear_88_cast_fp16)[name = tensor<string, []>("input_507_cast_fp16")];
            tensor<int32, [1]> x_215_axes_0 = const()[name = tensor<string, []>("x_215_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_9_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_9_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(178726208)))];
            tensor<fp16, [1024]> module_layers_9_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_9_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(178728320)))];
            tensor<fp16, [1, 188, 1024]> x_215_cast_fp16 = layer_norm(axes = x_215_axes_0, beta = module_layers_9_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_9_norm_conv_weight_to_fp16, x = input_507_cast_fp16)[name = tensor<string, []>("x_215_cast_fp16")];
            tensor<int32, [3]> input_509_perm_0 = const()[name = tensor<string, []>("input_509_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_511_pad_type_0 = const()[name = tensor<string, []>("input_511_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_511_strides_0 = const()[name = tensor<string, []>("input_511_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_511_pad_0 = const()[name = tensor<string, []>("input_511_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_511_dilations_0 = const()[name = tensor<string, []>("input_511_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_511_groups_0 = const()[name = tensor<string, []>("input_511_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_9_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(178730432))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(180303360))), name = tensor<string, []>("module_layers_9_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_509_cast_fp16 = transpose(perm = input_509_perm_0, x = x_215_cast_fp16)[name = tensor<string, []>("transpose_244")];
            tensor<fp16, [1, 2048, 188]> input_511_cast_fp16 = conv(dilations = input_511_dilations_0, groups = input_511_groups_0, pad = input_511_pad_0, pad_type = input_511_pad_type_0, strides = input_511_strides_0, weight = module_layers_9_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_509_cast_fp16)[name = tensor<string, []>("input_511_cast_fp16")];
            tensor<int32, []> x_217_split_num_splits_0 = const()[name = tensor<string, []>("x_217_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_217_split_axis_0 = const()[name = tensor<string, []>("x_217_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_217_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_217_split_cast_fp16_1 = split(axis = x_217_split_axis_0, num_splits = x_217_split_num_splits_0, x = input_511_cast_fp16)[name = tensor<string, []>("x_217_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_217_split_1_sigmoid_cast_fp16 = sigmoid(x = x_217_split_cast_fp16_1)[name = tensor<string, []>("x_217_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_217_cast_fp16 = mul(x = x_217_split_cast_fp16_0, y = x_217_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_217_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_513_cast_fp16 = select(a = var_11_to_fp16, b = x_217_cast_fp16, cond = var_328)[name = tensor<string, []>("input_513_cast_fp16")];
            tensor<int32, [6]> input_515_pad_0 = const()[name = tensor<string, []>("input_515_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_515_mode_0 = const()[name = tensor<string, []>("input_515_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_107_to_fp16 = const()[name = tensor<string, []>("const_107_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_515_cast_fp16 = pad(constant_val = const_107_to_fp16, mode = input_515_mode_0, pad = input_515_pad_0, x = input_513_cast_fp16)[name = tensor<string, []>("input_515_cast_fp16")];
            tensor<string, []> input_517_pad_type_0 = const()[name = tensor<string, []>("input_517_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_517_groups_0 = const()[name = tensor<string, []>("input_517_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_517_strides_0 = const()[name = tensor<string, []>("input_517_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_517_pad_0 = const()[name = tensor<string, []>("input_517_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_517_dilations_0 = const()[name = tensor<string, []>("input_517_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_266_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(180303552))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(180310528))), name = tensor<string, []>("const_266_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_267_to_fp16 = const()[name = tensor<string, []>("const_267_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(180310720)))];
            tensor<fp16, [1, 1024, 188]> input_519_cast_fp16 = conv(bias = const_267_to_fp16, dilations = input_517_dilations_0, groups = input_517_groups_0, pad = input_517_pad_0, pad_type = input_517_pad_type_0, strides = input_517_strides_0, weight = const_266_to_fp16_palettized, x = input_515_cast_fp16)[name = tensor<string, []>("input_519_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_521_cast_fp16 = silu(x = input_519_cast_fp16)[name = tensor<string, []>("input_521_cast_fp16")];
            tensor<string, []> x_219_pad_type_0 = const()[name = tensor<string, []>("x_219_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_219_strides_0 = const()[name = tensor<string, []>("x_219_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_219_pad_0 = const()[name = tensor<string, []>("x_219_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_219_dilations_0 = const()[name = tensor<string, []>("x_219_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_219_groups_0 = const()[name = tensor<string, []>("x_219_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_9_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(180312832))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(181099328))), name = tensor<string, []>("module_layers_9_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_219_cast_fp16 = conv(dilations = x_219_dilations_0, groups = x_219_groups_0, pad = x_219_pad_0, pad_type = x_219_pad_type_0, strides = x_219_strides_0, weight = module_layers_9_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_521_cast_fp16)[name = tensor<string, []>("x_219_cast_fp16")];
            tensor<int32, [3]> input_523_perm_0 = const()[name = tensor<string, []>("input_523_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_523_cast_fp16 = transpose(perm = input_523_perm_0, x = x_219_cast_fp16)[name = tensor<string, []>("transpose_243")];
            tensor<fp16, [1, 188, 1024]> input_525_cast_fp16 = add(x = input_507_cast_fp16, y = input_523_cast_fp16)[name = tensor<string, []>("input_525_cast_fp16")];
            tensor<int32, [1]> input_527_axes_0 = const()[name = tensor<string, []>("input_527_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_9_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_9_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(181099520)))];
            tensor<fp16, [1024]> module_layers_9_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_9_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(181101632)))];
            tensor<fp16, [1, 188, 1024]> input_527_cast_fp16 = layer_norm(axes = input_527_axes_0, beta = module_layers_9_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_9_norm_feed_forward2_weight_to_fp16, x = input_525_cast_fp16)[name = tensor<string, []>("input_527_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_9_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(181103744))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(184249536))), name = tensor<string, []>("module_layers_9_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_89_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_9_feed_forward2_linear1_weight_to_fp16_palettized, x = input_527_cast_fp16)[name = tensor<string, []>("linear_89_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_531_cast_fp16 = silu(x = linear_89_cast_fp16)[name = tensor<string, []>("input_531_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_9_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(184249728))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(187395520))), name = tensor<string, []>("module_layers_9_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_90_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_9_feed_forward2_linear2_weight_to_fp16_palettized, x = input_531_cast_fp16)[name = tensor<string, []>("linear_90_cast_fp16")];
            tensor<fp16, []> var_1833_to_fp16 = const()[name = tensor<string, []>("op_1833_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1834_cast_fp16 = mul(x = linear_90_cast_fp16, y = var_1833_to_fp16)[name = tensor<string, []>("op_1834_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_537_cast_fp16 = add(x = input_525_cast_fp16, y = var_1834_cast_fp16)[name = tensor<string, []>("input_537_cast_fp16")];
            tensor<int32, [1]> input_539_axes_0 = const()[name = tensor<string, []>("input_539_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_9_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_9_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(187395712)))];
            tensor<fp16, [1024]> module_layers_9_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_9_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(187397824)))];
            tensor<fp16, [1, 188, 1024]> input_539_cast_fp16 = layer_norm(axes = input_539_axes_0, beta = module_layers_9_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_9_norm_out_weight_to_fp16, x = input_537_cast_fp16)[name = tensor<string, []>("input_539_cast_fp16")];
            tensor<int32, [1]> input_541_axes_0 = const()[name = tensor<string, []>("input_541_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_10_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_10_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(187399936)))];
            tensor<fp16, [1024]> module_layers_10_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_10_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(187402048)))];
            tensor<fp16, [1, 188, 1024]> input_541_cast_fp16 = layer_norm(axes = input_541_axes_0, beta = module_layers_10_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_10_norm_feed_forward1_weight_to_fp16, x = input_539_cast_fp16)[name = tensor<string, []>("input_541_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_10_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(187404160))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(190549952))), name = tensor<string, []>("module_layers_10_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_91_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_10_feed_forward1_linear1_weight_to_fp16_palettized, x = input_541_cast_fp16)[name = tensor<string, []>("linear_91_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_545_cast_fp16 = silu(x = linear_91_cast_fp16)[name = tensor<string, []>("input_545_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_10_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(190550144))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(193695936))), name = tensor<string, []>("module_layers_10_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_92_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_10_feed_forward1_linear2_weight_to_fp16_palettized, x = input_545_cast_fp16)[name = tensor<string, []>("linear_92_cast_fp16")];
            tensor<fp16, []> var_1862_to_fp16 = const()[name = tensor<string, []>("op_1862_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1863_cast_fp16 = mul(x = linear_92_cast_fp16, y = var_1862_to_fp16)[name = tensor<string, []>("op_1863_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_551_cast_fp16 = add(x = input_539_cast_fp16, y = var_1863_cast_fp16)[name = tensor<string, []>("input_551_cast_fp16")];
            tensor<int32, [1]> query_21_axes_0 = const()[name = tensor<string, []>("query_21_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_10_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_10_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(193696128)))];
            tensor<fp16, [1024]> module_layers_10_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_10_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(193698240)))];
            tensor<fp16, [1, 188, 1024]> query_21_cast_fp16 = layer_norm(axes = query_21_axes_0, beta = module_layers_10_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_10_norm_self_att_weight_to_fp16, x = input_551_cast_fp16)[name = tensor<string, []>("query_21_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_10_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(193700352))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(194486848))), name = tensor<string, []>("module_layers_10_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_93_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_10_self_attn_linear_q_weight_to_fp16_palettized, x = query_21_cast_fp16)[name = tensor<string, []>("linear_93_cast_fp16")];
            tensor<int32, [4]> var_1879 = const()[name = tensor<string, []>("op_1879"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_61_cast_fp16 = reshape(shape = var_1879, x = linear_93_cast_fp16)[name = tensor<string, []>("q_61_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_10_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(194487040))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(195273536))), name = tensor<string, []>("module_layers_10_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_94_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_10_self_attn_linear_k_weight_to_fp16_palettized, x = query_21_cast_fp16)[name = tensor<string, []>("linear_94_cast_fp16")];
            tensor<int32, [4]> var_1883 = const()[name = tensor<string, []>("op_1883"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_41_cast_fp16 = reshape(shape = var_1883, x = linear_94_cast_fp16)[name = tensor<string, []>("k_41_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_10_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(195273728))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(196060224))), name = tensor<string, []>("module_layers_10_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_95_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_10_self_attn_linear_v_weight_to_fp16_palettized, x = query_21_cast_fp16)[name = tensor<string, []>("linear_95_cast_fp16")];
            tensor<int32, [4]> var_1887 = const()[name = tensor<string, []>("op_1887"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_21_cast_fp16 = reshape(shape = var_1887, x = linear_95_cast_fp16)[name = tensor<string, []>("v_21_cast_fp16")];
            tensor<int32, [4]> value_23_perm_0 = const()[name = tensor<string, []>("value_23_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_10_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_10_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(196060416)))];
            tensor<fp16, [1, 188, 8, 128]> var_1899_cast_fp16 = add(x = q_61_cast_fp16, y = module_layers_10_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_1899_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_10_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_10_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(196062528)))];
            tensor<fp16, [1, 188, 8, 128]> var_1901_cast_fp16 = add(x = q_61_cast_fp16, y = module_layers_10_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_1901_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_21_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_21_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_227_transpose_x_0 = const()[name = tensor<string, []>("x_227_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_227_transpose_y_0 = const()[name = tensor<string, []>("x_227_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_1903_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(196064640))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(196352704))), name = tensor<string, []>("op_1903_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_21_cast_fp16 = transpose(perm = q_with_bias_v_21_perm_0, x = var_1901_cast_fp16)[name = tensor<string, []>("transpose_242")];
            tensor<fp16, [1, 8, 188, 375]> x_227_cast_fp16 = matmul(transpose_x = x_227_transpose_x_0, transpose_y = x_227_transpose_y_0, x = q_with_bias_v_21_cast_fp16, y = op_1903_to_fp16_palettized)[name = tensor<string, []>("x_227_cast_fp16")];
            tensor<int32, [8]> x_229_pad_0 = const()[name = tensor<string, []>("x_229_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_229_mode_0 = const()[name = tensor<string, []>("x_229_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_114_to_fp16 = const()[name = tensor<string, []>("const_114_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_229_cast_fp16 = pad(constant_val = const_114_to_fp16, mode = x_229_mode_0, pad = x_229_pad_0, x = x_227_cast_fp16)[name = tensor<string, []>("x_229_cast_fp16")];
            tensor<int32, [4]> var_1911 = const()[name = tensor<string, []>("op_1911"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_231_cast_fp16 = reshape(shape = var_1911, x = x_229_cast_fp16)[name = tensor<string, []>("x_231_cast_fp16")];
            tensor<int32, [4]> var_1915_begin_0 = const()[name = tensor<string, []>("op_1915_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_1915_end_0 = const()[name = tensor<string, []>("op_1915_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_1915_end_mask_0 = const()[name = tensor<string, []>("op_1915_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_1915_cast_fp16 = slice_by_index(begin = var_1915_begin_0, end = var_1915_end_0, end_mask = var_1915_end_mask_0, x = x_231_cast_fp16)[name = tensor<string, []>("op_1915_cast_fp16")];
            tensor<int32, [4]> var_1916 = const()[name = tensor<string, []>("op_1916"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_41_cast_fp16 = reshape(shape = var_1916, x = var_1915_cast_fp16)[name = tensor<string, []>("matrix_bd_41_cast_fp16")];
            tensor<bool, []> matrix_ac_21_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_21_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_21_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_21_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_116_perm_0 = const()[name = tensor<string, []>("transpose_116_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_117_perm_0 = const()[name = tensor<string, []>("transpose_117_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_117 = transpose(perm = transpose_117_perm_0, x = k_41_cast_fp16)[name = tensor<string, []>("transpose_240")];
            tensor<fp16, [1, 8, 188, 128]> transpose_116 = transpose(perm = transpose_116_perm_0, x = var_1899_cast_fp16)[name = tensor<string, []>("transpose_241")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_21_cast_fp16 = matmul(transpose_x = matrix_ac_21_transpose_x_0, transpose_y = matrix_ac_21_transpose_y_0, x = transpose_116, y = transpose_117)[name = tensor<string, []>("matrix_ac_21_cast_fp16")];
            tensor<int32, [4]> matrix_bd_43_begin_0 = const()[name = tensor<string, []>("matrix_bd_43_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_43_end_0 = const()[name = tensor<string, []>("matrix_bd_43_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_43_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_43_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_43_cast_fp16 = slice_by_index(begin = matrix_bd_43_begin_0, end = matrix_bd_43_end_0, end_mask = matrix_bd_43_end_mask_0, x = matrix_bd_41_cast_fp16)[name = tensor<string, []>("matrix_bd_43_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1925_cast_fp16 = add(x = matrix_ac_21_cast_fp16, y = matrix_bd_43_cast_fp16)[name = tensor<string, []>("op_1925_cast_fp16")];
            tensor<fp16, []> _inversed_scores_41_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_41_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_41_cast_fp16 = mul(x = var_1925_cast_fp16, y = _inversed_scores_41_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_41_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_43_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_41_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_43_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_1931_cast_fp16 = softmax(axis = var_30, x = scores_43_cast_fp16)[name = tensor<string, []>("op_1931_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_553_cast_fp16 = select(a = var_11_to_fp16, b = var_1931_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_553_cast_fp16")];
            tensor<bool, []> x_233_transpose_x_0 = const()[name = tensor<string, []>("x_233_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_233_transpose_y_0 = const()[name = tensor<string, []>("x_233_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_23_cast_fp16 = transpose(perm = value_23_perm_0, x = v_21_cast_fp16)[name = tensor<string, []>("transpose_239")];
            tensor<fp16, [1, 8, 188, 128]> x_233_cast_fp16 = matmul(transpose_x = x_233_transpose_x_0, transpose_y = x_233_transpose_y_0, x = input_553_cast_fp16, y = value_23_cast_fp16)[name = tensor<string, []>("x_233_cast_fp16")];
            tensor<int32, [4]> var_1935_perm_0 = const()[name = tensor<string, []>("op_1935_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_1936 = const()[name = tensor<string, []>("op_1936"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_1935_cast_fp16 = transpose(perm = var_1935_perm_0, x = x_233_cast_fp16)[name = tensor<string, []>("transpose_238")];
            tensor<fp16, [1, 188, 1024]> input_555_cast_fp16 = reshape(shape = var_1936, x = var_1935_cast_fp16)[name = tensor<string, []>("input_555_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_10_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(196352896))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(197139392))), name = tensor<string, []>("module_layers_10_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_97_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_10_self_attn_linear_out_weight_to_fp16_palettized, x = input_555_cast_fp16)[name = tensor<string, []>("linear_97_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_559_cast_fp16 = add(x = input_551_cast_fp16, y = linear_97_cast_fp16)[name = tensor<string, []>("input_559_cast_fp16")];
            tensor<int32, [1]> x_237_axes_0 = const()[name = tensor<string, []>("x_237_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_10_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_10_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(197139584)))];
            tensor<fp16, [1024]> module_layers_10_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_10_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(197141696)))];
            tensor<fp16, [1, 188, 1024]> x_237_cast_fp16 = layer_norm(axes = x_237_axes_0, beta = module_layers_10_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_10_norm_conv_weight_to_fp16, x = input_559_cast_fp16)[name = tensor<string, []>("x_237_cast_fp16")];
            tensor<int32, [3]> input_561_perm_0 = const()[name = tensor<string, []>("input_561_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_563_pad_type_0 = const()[name = tensor<string, []>("input_563_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_563_strides_0 = const()[name = tensor<string, []>("input_563_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_563_pad_0 = const()[name = tensor<string, []>("input_563_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_563_dilations_0 = const()[name = tensor<string, []>("input_563_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_563_groups_0 = const()[name = tensor<string, []>("input_563_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_10_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(197143808))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(198716736))), name = tensor<string, []>("module_layers_10_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_561_cast_fp16 = transpose(perm = input_561_perm_0, x = x_237_cast_fp16)[name = tensor<string, []>("transpose_237")];
            tensor<fp16, [1, 2048, 188]> input_563_cast_fp16 = conv(dilations = input_563_dilations_0, groups = input_563_groups_0, pad = input_563_pad_0, pad_type = input_563_pad_type_0, strides = input_563_strides_0, weight = module_layers_10_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_561_cast_fp16)[name = tensor<string, []>("input_563_cast_fp16")];
            tensor<int32, []> x_239_split_num_splits_0 = const()[name = tensor<string, []>("x_239_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_239_split_axis_0 = const()[name = tensor<string, []>("x_239_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_239_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_239_split_cast_fp16_1 = split(axis = x_239_split_axis_0, num_splits = x_239_split_num_splits_0, x = input_563_cast_fp16)[name = tensor<string, []>("x_239_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_239_split_1_sigmoid_cast_fp16 = sigmoid(x = x_239_split_cast_fp16_1)[name = tensor<string, []>("x_239_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_239_cast_fp16 = mul(x = x_239_split_cast_fp16_0, y = x_239_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_239_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_565_cast_fp16 = select(a = var_11_to_fp16, b = x_239_cast_fp16, cond = var_328)[name = tensor<string, []>("input_565_cast_fp16")];
            tensor<int32, [6]> input_567_pad_0 = const()[name = tensor<string, []>("input_567_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_567_mode_0 = const()[name = tensor<string, []>("input_567_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_117_to_fp16 = const()[name = tensor<string, []>("const_117_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_567_cast_fp16 = pad(constant_val = const_117_to_fp16, mode = input_567_mode_0, pad = input_567_pad_0, x = input_565_cast_fp16)[name = tensor<string, []>("input_567_cast_fp16")];
            tensor<string, []> input_569_pad_type_0 = const()[name = tensor<string, []>("input_569_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_569_groups_0 = const()[name = tensor<string, []>("input_569_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_569_strides_0 = const()[name = tensor<string, []>("input_569_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_569_pad_0 = const()[name = tensor<string, []>("input_569_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_569_dilations_0 = const()[name = tensor<string, []>("input_569_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_268_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(198716928))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(198723904))), name = tensor<string, []>("const_268_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_269_to_fp16 = const()[name = tensor<string, []>("const_269_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(198724096)))];
            tensor<fp16, [1, 1024, 188]> input_571_cast_fp16 = conv(bias = const_269_to_fp16, dilations = input_569_dilations_0, groups = input_569_groups_0, pad = input_569_pad_0, pad_type = input_569_pad_type_0, strides = input_569_strides_0, weight = const_268_to_fp16_palettized, x = input_567_cast_fp16)[name = tensor<string, []>("input_571_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_573_cast_fp16 = silu(x = input_571_cast_fp16)[name = tensor<string, []>("input_573_cast_fp16")];
            tensor<string, []> x_241_pad_type_0 = const()[name = tensor<string, []>("x_241_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_241_strides_0 = const()[name = tensor<string, []>("x_241_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_241_pad_0 = const()[name = tensor<string, []>("x_241_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_241_dilations_0 = const()[name = tensor<string, []>("x_241_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_241_groups_0 = const()[name = tensor<string, []>("x_241_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_10_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(198726208))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(199512704))), name = tensor<string, []>("module_layers_10_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_241_cast_fp16 = conv(dilations = x_241_dilations_0, groups = x_241_groups_0, pad = x_241_pad_0, pad_type = x_241_pad_type_0, strides = x_241_strides_0, weight = module_layers_10_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_573_cast_fp16)[name = tensor<string, []>("x_241_cast_fp16")];
            tensor<int32, [3]> input_575_perm_0 = const()[name = tensor<string, []>("input_575_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_575_cast_fp16 = transpose(perm = input_575_perm_0, x = x_241_cast_fp16)[name = tensor<string, []>("transpose_236")];
            tensor<fp16, [1, 188, 1024]> input_577_cast_fp16 = add(x = input_559_cast_fp16, y = input_575_cast_fp16)[name = tensor<string, []>("input_577_cast_fp16")];
            tensor<int32, [1]> input_579_axes_0 = const()[name = tensor<string, []>("input_579_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_10_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_10_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(199512896)))];
            tensor<fp16, [1024]> module_layers_10_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_10_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(199515008)))];
            tensor<fp16, [1, 188, 1024]> input_579_cast_fp16 = layer_norm(axes = input_579_axes_0, beta = module_layers_10_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_10_norm_feed_forward2_weight_to_fp16, x = input_577_cast_fp16)[name = tensor<string, []>("input_579_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_10_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(199517120))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(202662912))), name = tensor<string, []>("module_layers_10_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_98_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_10_feed_forward2_linear1_weight_to_fp16_palettized, x = input_579_cast_fp16)[name = tensor<string, []>("linear_98_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_583_cast_fp16 = silu(x = linear_98_cast_fp16)[name = tensor<string, []>("input_583_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_10_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(202663104))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(205808896))), name = tensor<string, []>("module_layers_10_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_99_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_10_feed_forward2_linear2_weight_to_fp16_palettized, x = input_583_cast_fp16)[name = tensor<string, []>("linear_99_cast_fp16")];
            tensor<fp16, []> var_1996_to_fp16 = const()[name = tensor<string, []>("op_1996_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_1997_cast_fp16 = mul(x = linear_99_cast_fp16, y = var_1996_to_fp16)[name = tensor<string, []>("op_1997_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_589_cast_fp16 = add(x = input_577_cast_fp16, y = var_1997_cast_fp16)[name = tensor<string, []>("input_589_cast_fp16")];
            tensor<int32, [1]> input_591_axes_0 = const()[name = tensor<string, []>("input_591_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_10_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_10_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(205809088)))];
            tensor<fp16, [1024]> module_layers_10_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_10_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(205811200)))];
            tensor<fp16, [1, 188, 1024]> input_591_cast_fp16 = layer_norm(axes = input_591_axes_0, beta = module_layers_10_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_10_norm_out_weight_to_fp16, x = input_589_cast_fp16)[name = tensor<string, []>("input_591_cast_fp16")];
            tensor<int32, [1]> input_593_axes_0 = const()[name = tensor<string, []>("input_593_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_11_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_11_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(205813312)))];
            tensor<fp16, [1024]> module_layers_11_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_11_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(205815424)))];
            tensor<fp16, [1, 188, 1024]> input_593_cast_fp16 = layer_norm(axes = input_593_axes_0, beta = module_layers_11_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_11_norm_feed_forward1_weight_to_fp16, x = input_591_cast_fp16)[name = tensor<string, []>("input_593_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_11_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(205817536))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(208963328))), name = tensor<string, []>("module_layers_11_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_100_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_11_feed_forward1_linear1_weight_to_fp16_palettized, x = input_593_cast_fp16)[name = tensor<string, []>("linear_100_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_597_cast_fp16 = silu(x = linear_100_cast_fp16)[name = tensor<string, []>("input_597_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_11_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(208963520))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(212109312))), name = tensor<string, []>("module_layers_11_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_101_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_11_feed_forward1_linear2_weight_to_fp16_palettized, x = input_597_cast_fp16)[name = tensor<string, []>("linear_101_cast_fp16")];
            tensor<fp16, []> var_2025_to_fp16 = const()[name = tensor<string, []>("op_2025_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2026_cast_fp16 = mul(x = linear_101_cast_fp16, y = var_2025_to_fp16)[name = tensor<string, []>("op_2026_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_603_cast_fp16 = add(x = input_591_cast_fp16, y = var_2026_cast_fp16)[name = tensor<string, []>("input_603_cast_fp16")];
            tensor<int32, [1]> query_23_axes_0 = const()[name = tensor<string, []>("query_23_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_11_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_11_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(212109504)))];
            tensor<fp16, [1024]> module_layers_11_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_11_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(212111616)))];
            tensor<fp16, [1, 188, 1024]> query_23_cast_fp16 = layer_norm(axes = query_23_axes_0, beta = module_layers_11_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_11_norm_self_att_weight_to_fp16, x = input_603_cast_fp16)[name = tensor<string, []>("query_23_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_11_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(212113728))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(212900224))), name = tensor<string, []>("module_layers_11_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_102_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_11_self_attn_linear_q_weight_to_fp16_palettized, x = query_23_cast_fp16)[name = tensor<string, []>("linear_102_cast_fp16")];
            tensor<int32, [4]> var_2042 = const()[name = tensor<string, []>("op_2042"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_67_cast_fp16 = reshape(shape = var_2042, x = linear_102_cast_fp16)[name = tensor<string, []>("q_67_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_11_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(212900416))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(213686912))), name = tensor<string, []>("module_layers_11_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_103_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_11_self_attn_linear_k_weight_to_fp16_palettized, x = query_23_cast_fp16)[name = tensor<string, []>("linear_103_cast_fp16")];
            tensor<int32, [4]> var_2046 = const()[name = tensor<string, []>("op_2046"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_45_cast_fp16 = reshape(shape = var_2046, x = linear_103_cast_fp16)[name = tensor<string, []>("k_45_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_11_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(213687104))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(214473600))), name = tensor<string, []>("module_layers_11_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_104_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_11_self_attn_linear_v_weight_to_fp16_palettized, x = query_23_cast_fp16)[name = tensor<string, []>("linear_104_cast_fp16")];
            tensor<int32, [4]> var_2050 = const()[name = tensor<string, []>("op_2050"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_23_cast_fp16 = reshape(shape = var_2050, x = linear_104_cast_fp16)[name = tensor<string, []>("v_23_cast_fp16")];
            tensor<int32, [4]> value_25_perm_0 = const()[name = tensor<string, []>("value_25_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_11_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_11_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(214473792)))];
            tensor<fp16, [1, 188, 8, 128]> var_2062_cast_fp16 = add(x = q_67_cast_fp16, y = module_layers_11_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_2062_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_11_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_11_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(214475904)))];
            tensor<fp16, [1, 188, 8, 128]> var_2064_cast_fp16 = add(x = q_67_cast_fp16, y = module_layers_11_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_2064_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_23_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_23_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_249_transpose_x_0 = const()[name = tensor<string, []>("x_249_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_249_transpose_y_0 = const()[name = tensor<string, []>("x_249_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_2066_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(214478016))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(214766080))), name = tensor<string, []>("op_2066_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_23_cast_fp16 = transpose(perm = q_with_bias_v_23_perm_0, x = var_2064_cast_fp16)[name = tensor<string, []>("transpose_235")];
            tensor<fp16, [1, 8, 188, 375]> x_249_cast_fp16 = matmul(transpose_x = x_249_transpose_x_0, transpose_y = x_249_transpose_y_0, x = q_with_bias_v_23_cast_fp16, y = op_2066_to_fp16_palettized)[name = tensor<string, []>("x_249_cast_fp16")];
            tensor<int32, [8]> x_251_pad_0 = const()[name = tensor<string, []>("x_251_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_251_mode_0 = const()[name = tensor<string, []>("x_251_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_124_to_fp16 = const()[name = tensor<string, []>("const_124_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_251_cast_fp16 = pad(constant_val = const_124_to_fp16, mode = x_251_mode_0, pad = x_251_pad_0, x = x_249_cast_fp16)[name = tensor<string, []>("x_251_cast_fp16")];
            tensor<int32, [4]> var_2074 = const()[name = tensor<string, []>("op_2074"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_253_cast_fp16 = reshape(shape = var_2074, x = x_251_cast_fp16)[name = tensor<string, []>("x_253_cast_fp16")];
            tensor<int32, [4]> var_2078_begin_0 = const()[name = tensor<string, []>("op_2078_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_2078_end_0 = const()[name = tensor<string, []>("op_2078_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_2078_end_mask_0 = const()[name = tensor<string, []>("op_2078_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_2078_cast_fp16 = slice_by_index(begin = var_2078_begin_0, end = var_2078_end_0, end_mask = var_2078_end_mask_0, x = x_253_cast_fp16)[name = tensor<string, []>("op_2078_cast_fp16")];
            tensor<int32, [4]> var_2079 = const()[name = tensor<string, []>("op_2079"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_45_cast_fp16 = reshape(shape = var_2079, x = var_2078_cast_fp16)[name = tensor<string, []>("matrix_bd_45_cast_fp16")];
            tensor<bool, []> matrix_ac_23_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_23_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_23_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_23_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_118_perm_0 = const()[name = tensor<string, []>("transpose_118_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_119_perm_0 = const()[name = tensor<string, []>("transpose_119_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_119 = transpose(perm = transpose_119_perm_0, x = k_45_cast_fp16)[name = tensor<string, []>("transpose_233")];
            tensor<fp16, [1, 8, 188, 128]> transpose_118 = transpose(perm = transpose_118_perm_0, x = var_2062_cast_fp16)[name = tensor<string, []>("transpose_234")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_23_cast_fp16 = matmul(transpose_x = matrix_ac_23_transpose_x_0, transpose_y = matrix_ac_23_transpose_y_0, x = transpose_118, y = transpose_119)[name = tensor<string, []>("matrix_ac_23_cast_fp16")];
            tensor<int32, [4]> matrix_bd_47_begin_0 = const()[name = tensor<string, []>("matrix_bd_47_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_47_end_0 = const()[name = tensor<string, []>("matrix_bd_47_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_47_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_47_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_47_cast_fp16 = slice_by_index(begin = matrix_bd_47_begin_0, end = matrix_bd_47_end_0, end_mask = matrix_bd_47_end_mask_0, x = matrix_bd_45_cast_fp16)[name = tensor<string, []>("matrix_bd_47_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2088_cast_fp16 = add(x = matrix_ac_23_cast_fp16, y = matrix_bd_47_cast_fp16)[name = tensor<string, []>("op_2088_cast_fp16")];
            tensor<fp16, []> _inversed_scores_45_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_45_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_45_cast_fp16 = mul(x = var_2088_cast_fp16, y = _inversed_scores_45_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_45_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_47_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_45_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_47_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2094_cast_fp16 = softmax(axis = var_30, x = scores_47_cast_fp16)[name = tensor<string, []>("op_2094_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_605_cast_fp16 = select(a = var_11_to_fp16, b = var_2094_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_605_cast_fp16")];
            tensor<bool, []> x_255_transpose_x_0 = const()[name = tensor<string, []>("x_255_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_255_transpose_y_0 = const()[name = tensor<string, []>("x_255_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_25_cast_fp16 = transpose(perm = value_25_perm_0, x = v_23_cast_fp16)[name = tensor<string, []>("transpose_232")];
            tensor<fp16, [1, 8, 188, 128]> x_255_cast_fp16 = matmul(transpose_x = x_255_transpose_x_0, transpose_y = x_255_transpose_y_0, x = input_605_cast_fp16, y = value_25_cast_fp16)[name = tensor<string, []>("x_255_cast_fp16")];
            tensor<int32, [4]> var_2098_perm_0 = const()[name = tensor<string, []>("op_2098_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_2099 = const()[name = tensor<string, []>("op_2099"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_2098_cast_fp16 = transpose(perm = var_2098_perm_0, x = x_255_cast_fp16)[name = tensor<string, []>("transpose_231")];
            tensor<fp16, [1, 188, 1024]> input_607_cast_fp16 = reshape(shape = var_2099, x = var_2098_cast_fp16)[name = tensor<string, []>("input_607_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_11_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(214766272))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(215552768))), name = tensor<string, []>("module_layers_11_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_106_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_11_self_attn_linear_out_weight_to_fp16_palettized, x = input_607_cast_fp16)[name = tensor<string, []>("linear_106_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_611_cast_fp16 = add(x = input_603_cast_fp16, y = linear_106_cast_fp16)[name = tensor<string, []>("input_611_cast_fp16")];
            tensor<int32, [1]> x_259_axes_0 = const()[name = tensor<string, []>("x_259_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_11_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_11_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(215552960)))];
            tensor<fp16, [1024]> module_layers_11_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_11_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(215555072)))];
            tensor<fp16, [1, 188, 1024]> x_259_cast_fp16 = layer_norm(axes = x_259_axes_0, beta = module_layers_11_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_11_norm_conv_weight_to_fp16, x = input_611_cast_fp16)[name = tensor<string, []>("x_259_cast_fp16")];
            tensor<int32, [3]> input_613_perm_0 = const()[name = tensor<string, []>("input_613_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_615_pad_type_0 = const()[name = tensor<string, []>("input_615_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_615_strides_0 = const()[name = tensor<string, []>("input_615_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_615_pad_0 = const()[name = tensor<string, []>("input_615_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_615_dilations_0 = const()[name = tensor<string, []>("input_615_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_615_groups_0 = const()[name = tensor<string, []>("input_615_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_11_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(215557184))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(217130112))), name = tensor<string, []>("module_layers_11_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_613_cast_fp16 = transpose(perm = input_613_perm_0, x = x_259_cast_fp16)[name = tensor<string, []>("transpose_230")];
            tensor<fp16, [1, 2048, 188]> input_615_cast_fp16 = conv(dilations = input_615_dilations_0, groups = input_615_groups_0, pad = input_615_pad_0, pad_type = input_615_pad_type_0, strides = input_615_strides_0, weight = module_layers_11_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_613_cast_fp16)[name = tensor<string, []>("input_615_cast_fp16")];
            tensor<int32, []> x_261_split_num_splits_0 = const()[name = tensor<string, []>("x_261_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_261_split_axis_0 = const()[name = tensor<string, []>("x_261_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_261_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_261_split_cast_fp16_1 = split(axis = x_261_split_axis_0, num_splits = x_261_split_num_splits_0, x = input_615_cast_fp16)[name = tensor<string, []>("x_261_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_261_split_1_sigmoid_cast_fp16 = sigmoid(x = x_261_split_cast_fp16_1)[name = tensor<string, []>("x_261_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_261_cast_fp16 = mul(x = x_261_split_cast_fp16_0, y = x_261_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_261_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_617_cast_fp16 = select(a = var_11_to_fp16, b = x_261_cast_fp16, cond = var_328)[name = tensor<string, []>("input_617_cast_fp16")];
            tensor<int32, [6]> input_619_pad_0 = const()[name = tensor<string, []>("input_619_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_619_mode_0 = const()[name = tensor<string, []>("input_619_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_127_to_fp16 = const()[name = tensor<string, []>("const_127_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_619_cast_fp16 = pad(constant_val = const_127_to_fp16, mode = input_619_mode_0, pad = input_619_pad_0, x = input_617_cast_fp16)[name = tensor<string, []>("input_619_cast_fp16")];
            tensor<string, []> input_621_pad_type_0 = const()[name = tensor<string, []>("input_621_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_621_groups_0 = const()[name = tensor<string, []>("input_621_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_621_strides_0 = const()[name = tensor<string, []>("input_621_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_621_pad_0 = const()[name = tensor<string, []>("input_621_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_621_dilations_0 = const()[name = tensor<string, []>("input_621_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_270_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(217130304))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(217137280))), name = tensor<string, []>("const_270_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_271_to_fp16 = const()[name = tensor<string, []>("const_271_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(217137472)))];
            tensor<fp16, [1, 1024, 188]> input_623_cast_fp16 = conv(bias = const_271_to_fp16, dilations = input_621_dilations_0, groups = input_621_groups_0, pad = input_621_pad_0, pad_type = input_621_pad_type_0, strides = input_621_strides_0, weight = const_270_to_fp16_palettized, x = input_619_cast_fp16)[name = tensor<string, []>("input_623_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_625_cast_fp16 = silu(x = input_623_cast_fp16)[name = tensor<string, []>("input_625_cast_fp16")];
            tensor<string, []> x_263_pad_type_0 = const()[name = tensor<string, []>("x_263_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_263_strides_0 = const()[name = tensor<string, []>("x_263_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_263_pad_0 = const()[name = tensor<string, []>("x_263_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_263_dilations_0 = const()[name = tensor<string, []>("x_263_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_263_groups_0 = const()[name = tensor<string, []>("x_263_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_11_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(217139584))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(217926080))), name = tensor<string, []>("module_layers_11_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_263_cast_fp16 = conv(dilations = x_263_dilations_0, groups = x_263_groups_0, pad = x_263_pad_0, pad_type = x_263_pad_type_0, strides = x_263_strides_0, weight = module_layers_11_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_625_cast_fp16)[name = tensor<string, []>("x_263_cast_fp16")];
            tensor<int32, [3]> input_627_perm_0 = const()[name = tensor<string, []>("input_627_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_627_cast_fp16 = transpose(perm = input_627_perm_0, x = x_263_cast_fp16)[name = tensor<string, []>("transpose_229")];
            tensor<fp16, [1, 188, 1024]> input_629_cast_fp16 = add(x = input_611_cast_fp16, y = input_627_cast_fp16)[name = tensor<string, []>("input_629_cast_fp16")];
            tensor<int32, [1]> input_631_axes_0 = const()[name = tensor<string, []>("input_631_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_11_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_11_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(217926272)))];
            tensor<fp16, [1024]> module_layers_11_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_11_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(217928384)))];
            tensor<fp16, [1, 188, 1024]> input_631_cast_fp16 = layer_norm(axes = input_631_axes_0, beta = module_layers_11_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_11_norm_feed_forward2_weight_to_fp16, x = input_629_cast_fp16)[name = tensor<string, []>("input_631_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_11_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(217930496))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(221076288))), name = tensor<string, []>("module_layers_11_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_107_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_11_feed_forward2_linear1_weight_to_fp16_palettized, x = input_631_cast_fp16)[name = tensor<string, []>("linear_107_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_635_cast_fp16 = silu(x = linear_107_cast_fp16)[name = tensor<string, []>("input_635_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_11_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(221076480))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(224222272))), name = tensor<string, []>("module_layers_11_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_108_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_11_feed_forward2_linear2_weight_to_fp16_palettized, x = input_635_cast_fp16)[name = tensor<string, []>("linear_108_cast_fp16")];
            tensor<fp16, []> var_2159_to_fp16 = const()[name = tensor<string, []>("op_2159_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2160_cast_fp16 = mul(x = linear_108_cast_fp16, y = var_2159_to_fp16)[name = tensor<string, []>("op_2160_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_641_cast_fp16 = add(x = input_629_cast_fp16, y = var_2160_cast_fp16)[name = tensor<string, []>("input_641_cast_fp16")];
            tensor<int32, [1]> input_643_axes_0 = const()[name = tensor<string, []>("input_643_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_11_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_11_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(224222464)))];
            tensor<fp16, [1024]> module_layers_11_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_11_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(224224576)))];
            tensor<fp16, [1, 188, 1024]> input_643_cast_fp16 = layer_norm(axes = input_643_axes_0, beta = module_layers_11_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_11_norm_out_weight_to_fp16, x = input_641_cast_fp16)[name = tensor<string, []>("input_643_cast_fp16")];
            tensor<int32, [1]> input_645_axes_0 = const()[name = tensor<string, []>("input_645_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_12_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_12_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(224226688)))];
            tensor<fp16, [1024]> module_layers_12_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_12_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(224228800)))];
            tensor<fp16, [1, 188, 1024]> input_645_cast_fp16 = layer_norm(axes = input_645_axes_0, beta = module_layers_12_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_12_norm_feed_forward1_weight_to_fp16, x = input_643_cast_fp16)[name = tensor<string, []>("input_645_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_12_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(224230912))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(227376704))), name = tensor<string, []>("module_layers_12_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_109_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_12_feed_forward1_linear1_weight_to_fp16_palettized, x = input_645_cast_fp16)[name = tensor<string, []>("linear_109_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_649_cast_fp16 = silu(x = linear_109_cast_fp16)[name = tensor<string, []>("input_649_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_12_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(227376896))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(230522688))), name = tensor<string, []>("module_layers_12_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_110_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_12_feed_forward1_linear2_weight_to_fp16_palettized, x = input_649_cast_fp16)[name = tensor<string, []>("linear_110_cast_fp16")];
            tensor<fp16, []> var_2188_to_fp16 = const()[name = tensor<string, []>("op_2188_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2189_cast_fp16 = mul(x = linear_110_cast_fp16, y = var_2188_to_fp16)[name = tensor<string, []>("op_2189_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_655_cast_fp16 = add(x = input_643_cast_fp16, y = var_2189_cast_fp16)[name = tensor<string, []>("input_655_cast_fp16")];
            tensor<int32, [1]> query_25_axes_0 = const()[name = tensor<string, []>("query_25_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_12_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_12_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(230522880)))];
            tensor<fp16, [1024]> module_layers_12_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_12_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(230524992)))];
            tensor<fp16, [1, 188, 1024]> query_25_cast_fp16 = layer_norm(axes = query_25_axes_0, beta = module_layers_12_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_12_norm_self_att_weight_to_fp16, x = input_655_cast_fp16)[name = tensor<string, []>("query_25_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_12_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(230527104))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(231313600))), name = tensor<string, []>("module_layers_12_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_111_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_12_self_attn_linear_q_weight_to_fp16_palettized, x = query_25_cast_fp16)[name = tensor<string, []>("linear_111_cast_fp16")];
            tensor<int32, [4]> var_2205 = const()[name = tensor<string, []>("op_2205"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_73_cast_fp16 = reshape(shape = var_2205, x = linear_111_cast_fp16)[name = tensor<string, []>("q_73_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_12_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(231313792))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(232100288))), name = tensor<string, []>("module_layers_12_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_112_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_12_self_attn_linear_k_weight_to_fp16_palettized, x = query_25_cast_fp16)[name = tensor<string, []>("linear_112_cast_fp16")];
            tensor<int32, [4]> var_2209 = const()[name = tensor<string, []>("op_2209"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_49_cast_fp16 = reshape(shape = var_2209, x = linear_112_cast_fp16)[name = tensor<string, []>("k_49_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_12_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(232100480))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(232886976))), name = tensor<string, []>("module_layers_12_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_113_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_12_self_attn_linear_v_weight_to_fp16_palettized, x = query_25_cast_fp16)[name = tensor<string, []>("linear_113_cast_fp16")];
            tensor<int32, [4]> var_2213 = const()[name = tensor<string, []>("op_2213"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_25_cast_fp16 = reshape(shape = var_2213, x = linear_113_cast_fp16)[name = tensor<string, []>("v_25_cast_fp16")];
            tensor<int32, [4]> value_27_perm_0 = const()[name = tensor<string, []>("value_27_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_12_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_12_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(232887168)))];
            tensor<fp16, [1, 188, 8, 128]> var_2225_cast_fp16 = add(x = q_73_cast_fp16, y = module_layers_12_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_2225_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_12_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_12_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(232889280)))];
            tensor<fp16, [1, 188, 8, 128]> var_2227_cast_fp16 = add(x = q_73_cast_fp16, y = module_layers_12_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_2227_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_25_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_25_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_271_transpose_x_0 = const()[name = tensor<string, []>("x_271_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_271_transpose_y_0 = const()[name = tensor<string, []>("x_271_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_2229_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(232891392))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(233179456))), name = tensor<string, []>("op_2229_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_25_cast_fp16 = transpose(perm = q_with_bias_v_25_perm_0, x = var_2227_cast_fp16)[name = tensor<string, []>("transpose_228")];
            tensor<fp16, [1, 8, 188, 375]> x_271_cast_fp16 = matmul(transpose_x = x_271_transpose_x_0, transpose_y = x_271_transpose_y_0, x = q_with_bias_v_25_cast_fp16, y = op_2229_to_fp16_palettized)[name = tensor<string, []>("x_271_cast_fp16")];
            tensor<int32, [8]> x_273_pad_0 = const()[name = tensor<string, []>("x_273_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_273_mode_0 = const()[name = tensor<string, []>("x_273_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_134_to_fp16 = const()[name = tensor<string, []>("const_134_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_273_cast_fp16 = pad(constant_val = const_134_to_fp16, mode = x_273_mode_0, pad = x_273_pad_0, x = x_271_cast_fp16)[name = tensor<string, []>("x_273_cast_fp16")];
            tensor<int32, [4]> var_2237 = const()[name = tensor<string, []>("op_2237"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_275_cast_fp16 = reshape(shape = var_2237, x = x_273_cast_fp16)[name = tensor<string, []>("x_275_cast_fp16")];
            tensor<int32, [4]> var_2241_begin_0 = const()[name = tensor<string, []>("op_2241_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_2241_end_0 = const()[name = tensor<string, []>("op_2241_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_2241_end_mask_0 = const()[name = tensor<string, []>("op_2241_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_2241_cast_fp16 = slice_by_index(begin = var_2241_begin_0, end = var_2241_end_0, end_mask = var_2241_end_mask_0, x = x_275_cast_fp16)[name = tensor<string, []>("op_2241_cast_fp16")];
            tensor<int32, [4]> var_2242 = const()[name = tensor<string, []>("op_2242"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_49_cast_fp16 = reshape(shape = var_2242, x = var_2241_cast_fp16)[name = tensor<string, []>("matrix_bd_49_cast_fp16")];
            tensor<bool, []> matrix_ac_25_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_25_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_25_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_25_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_120_perm_0 = const()[name = tensor<string, []>("transpose_120_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_121_perm_0 = const()[name = tensor<string, []>("transpose_121_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_121 = transpose(perm = transpose_121_perm_0, x = k_49_cast_fp16)[name = tensor<string, []>("transpose_226")];
            tensor<fp16, [1, 8, 188, 128]> transpose_120 = transpose(perm = transpose_120_perm_0, x = var_2225_cast_fp16)[name = tensor<string, []>("transpose_227")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_25_cast_fp16 = matmul(transpose_x = matrix_ac_25_transpose_x_0, transpose_y = matrix_ac_25_transpose_y_0, x = transpose_120, y = transpose_121)[name = tensor<string, []>("matrix_ac_25_cast_fp16")];
            tensor<int32, [4]> matrix_bd_51_begin_0 = const()[name = tensor<string, []>("matrix_bd_51_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_51_end_0 = const()[name = tensor<string, []>("matrix_bd_51_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_51_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_51_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_51_cast_fp16 = slice_by_index(begin = matrix_bd_51_begin_0, end = matrix_bd_51_end_0, end_mask = matrix_bd_51_end_mask_0, x = matrix_bd_49_cast_fp16)[name = tensor<string, []>("matrix_bd_51_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2251_cast_fp16 = add(x = matrix_ac_25_cast_fp16, y = matrix_bd_51_cast_fp16)[name = tensor<string, []>("op_2251_cast_fp16")];
            tensor<fp16, []> _inversed_scores_49_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_49_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_49_cast_fp16 = mul(x = var_2251_cast_fp16, y = _inversed_scores_49_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_49_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_51_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_49_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_51_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2257_cast_fp16 = softmax(axis = var_30, x = scores_51_cast_fp16)[name = tensor<string, []>("op_2257_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_657_cast_fp16 = select(a = var_11_to_fp16, b = var_2257_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_657_cast_fp16")];
            tensor<bool, []> x_277_transpose_x_0 = const()[name = tensor<string, []>("x_277_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_277_transpose_y_0 = const()[name = tensor<string, []>("x_277_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_27_cast_fp16 = transpose(perm = value_27_perm_0, x = v_25_cast_fp16)[name = tensor<string, []>("transpose_225")];
            tensor<fp16, [1, 8, 188, 128]> x_277_cast_fp16 = matmul(transpose_x = x_277_transpose_x_0, transpose_y = x_277_transpose_y_0, x = input_657_cast_fp16, y = value_27_cast_fp16)[name = tensor<string, []>("x_277_cast_fp16")];
            tensor<int32, [4]> var_2261_perm_0 = const()[name = tensor<string, []>("op_2261_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_2262 = const()[name = tensor<string, []>("op_2262"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_2261_cast_fp16 = transpose(perm = var_2261_perm_0, x = x_277_cast_fp16)[name = tensor<string, []>("transpose_224")];
            tensor<fp16, [1, 188, 1024]> input_659_cast_fp16 = reshape(shape = var_2262, x = var_2261_cast_fp16)[name = tensor<string, []>("input_659_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_12_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(233179648))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(233966144))), name = tensor<string, []>("module_layers_12_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_115_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_12_self_attn_linear_out_weight_to_fp16_palettized, x = input_659_cast_fp16)[name = tensor<string, []>("linear_115_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_663_cast_fp16 = add(x = input_655_cast_fp16, y = linear_115_cast_fp16)[name = tensor<string, []>("input_663_cast_fp16")];
            tensor<int32, [1]> x_281_axes_0 = const()[name = tensor<string, []>("x_281_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_12_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_12_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(233966336)))];
            tensor<fp16, [1024]> module_layers_12_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_12_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(233968448)))];
            tensor<fp16, [1, 188, 1024]> x_281_cast_fp16 = layer_norm(axes = x_281_axes_0, beta = module_layers_12_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_12_norm_conv_weight_to_fp16, x = input_663_cast_fp16)[name = tensor<string, []>("x_281_cast_fp16")];
            tensor<int32, [3]> input_665_perm_0 = const()[name = tensor<string, []>("input_665_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_667_pad_type_0 = const()[name = tensor<string, []>("input_667_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_667_strides_0 = const()[name = tensor<string, []>("input_667_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_667_pad_0 = const()[name = tensor<string, []>("input_667_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_667_dilations_0 = const()[name = tensor<string, []>("input_667_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_667_groups_0 = const()[name = tensor<string, []>("input_667_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_12_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(233970560))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(235543488))), name = tensor<string, []>("module_layers_12_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_665_cast_fp16 = transpose(perm = input_665_perm_0, x = x_281_cast_fp16)[name = tensor<string, []>("transpose_223")];
            tensor<fp16, [1, 2048, 188]> input_667_cast_fp16 = conv(dilations = input_667_dilations_0, groups = input_667_groups_0, pad = input_667_pad_0, pad_type = input_667_pad_type_0, strides = input_667_strides_0, weight = module_layers_12_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_665_cast_fp16)[name = tensor<string, []>("input_667_cast_fp16")];
            tensor<int32, []> x_283_split_num_splits_0 = const()[name = tensor<string, []>("x_283_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_283_split_axis_0 = const()[name = tensor<string, []>("x_283_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_283_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_283_split_cast_fp16_1 = split(axis = x_283_split_axis_0, num_splits = x_283_split_num_splits_0, x = input_667_cast_fp16)[name = tensor<string, []>("x_283_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_283_split_1_sigmoid_cast_fp16 = sigmoid(x = x_283_split_cast_fp16_1)[name = tensor<string, []>("x_283_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_283_cast_fp16 = mul(x = x_283_split_cast_fp16_0, y = x_283_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_283_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_669_cast_fp16 = select(a = var_11_to_fp16, b = x_283_cast_fp16, cond = var_328)[name = tensor<string, []>("input_669_cast_fp16")];
            tensor<int32, [6]> input_671_pad_0 = const()[name = tensor<string, []>("input_671_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_671_mode_0 = const()[name = tensor<string, []>("input_671_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_137_to_fp16 = const()[name = tensor<string, []>("const_137_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_671_cast_fp16 = pad(constant_val = const_137_to_fp16, mode = input_671_mode_0, pad = input_671_pad_0, x = input_669_cast_fp16)[name = tensor<string, []>("input_671_cast_fp16")];
            tensor<string, []> input_673_pad_type_0 = const()[name = tensor<string, []>("input_673_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_673_groups_0 = const()[name = tensor<string, []>("input_673_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_673_strides_0 = const()[name = tensor<string, []>("input_673_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_673_pad_0 = const()[name = tensor<string, []>("input_673_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_673_dilations_0 = const()[name = tensor<string, []>("input_673_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_272_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(235543680))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(235550656))), name = tensor<string, []>("const_272_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_273_to_fp16 = const()[name = tensor<string, []>("const_273_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(235550848)))];
            tensor<fp16, [1, 1024, 188]> input_675_cast_fp16 = conv(bias = const_273_to_fp16, dilations = input_673_dilations_0, groups = input_673_groups_0, pad = input_673_pad_0, pad_type = input_673_pad_type_0, strides = input_673_strides_0, weight = const_272_to_fp16_palettized, x = input_671_cast_fp16)[name = tensor<string, []>("input_675_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_677_cast_fp16 = silu(x = input_675_cast_fp16)[name = tensor<string, []>("input_677_cast_fp16")];
            tensor<string, []> x_285_pad_type_0 = const()[name = tensor<string, []>("x_285_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_285_strides_0 = const()[name = tensor<string, []>("x_285_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_285_pad_0 = const()[name = tensor<string, []>("x_285_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_285_dilations_0 = const()[name = tensor<string, []>("x_285_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_285_groups_0 = const()[name = tensor<string, []>("x_285_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_12_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(235552960))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(236339456))), name = tensor<string, []>("module_layers_12_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_285_cast_fp16 = conv(dilations = x_285_dilations_0, groups = x_285_groups_0, pad = x_285_pad_0, pad_type = x_285_pad_type_0, strides = x_285_strides_0, weight = module_layers_12_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_677_cast_fp16)[name = tensor<string, []>("x_285_cast_fp16")];
            tensor<int32, [3]> input_679_perm_0 = const()[name = tensor<string, []>("input_679_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_679_cast_fp16 = transpose(perm = input_679_perm_0, x = x_285_cast_fp16)[name = tensor<string, []>("transpose_222")];
            tensor<fp16, [1, 188, 1024]> input_681_cast_fp16 = add(x = input_663_cast_fp16, y = input_679_cast_fp16)[name = tensor<string, []>("input_681_cast_fp16")];
            tensor<int32, [1]> input_683_axes_0 = const()[name = tensor<string, []>("input_683_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_12_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_12_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(236339648)))];
            tensor<fp16, [1024]> module_layers_12_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_12_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(236341760)))];
            tensor<fp16, [1, 188, 1024]> input_683_cast_fp16 = layer_norm(axes = input_683_axes_0, beta = module_layers_12_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_12_norm_feed_forward2_weight_to_fp16, x = input_681_cast_fp16)[name = tensor<string, []>("input_683_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_12_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(236343872))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(239489664))), name = tensor<string, []>("module_layers_12_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_116_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_12_feed_forward2_linear1_weight_to_fp16_palettized, x = input_683_cast_fp16)[name = tensor<string, []>("linear_116_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_687_cast_fp16 = silu(x = linear_116_cast_fp16)[name = tensor<string, []>("input_687_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_12_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(239489856))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(242635648))), name = tensor<string, []>("module_layers_12_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_117_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_12_feed_forward2_linear2_weight_to_fp16_palettized, x = input_687_cast_fp16)[name = tensor<string, []>("linear_117_cast_fp16")];
            tensor<fp16, []> var_2322_to_fp16 = const()[name = tensor<string, []>("op_2322_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2323_cast_fp16 = mul(x = linear_117_cast_fp16, y = var_2322_to_fp16)[name = tensor<string, []>("op_2323_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_693_cast_fp16 = add(x = input_681_cast_fp16, y = var_2323_cast_fp16)[name = tensor<string, []>("input_693_cast_fp16")];
            tensor<int32, [1]> input_695_axes_0 = const()[name = tensor<string, []>("input_695_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_12_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_12_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(242635840)))];
            tensor<fp16, [1024]> module_layers_12_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_12_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(242637952)))];
            tensor<fp16, [1, 188, 1024]> input_695_cast_fp16 = layer_norm(axes = input_695_axes_0, beta = module_layers_12_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_12_norm_out_weight_to_fp16, x = input_693_cast_fp16)[name = tensor<string, []>("input_695_cast_fp16")];
            tensor<int32, [1]> input_697_axes_0 = const()[name = tensor<string, []>("input_697_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_13_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_13_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(242640064)))];
            tensor<fp16, [1024]> module_layers_13_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_13_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(242642176)))];
            tensor<fp16, [1, 188, 1024]> input_697_cast_fp16 = layer_norm(axes = input_697_axes_0, beta = module_layers_13_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_13_norm_feed_forward1_weight_to_fp16, x = input_695_cast_fp16)[name = tensor<string, []>("input_697_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_13_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(242644288))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(245790080))), name = tensor<string, []>("module_layers_13_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_118_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_13_feed_forward1_linear1_weight_to_fp16_palettized, x = input_697_cast_fp16)[name = tensor<string, []>("linear_118_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_701_cast_fp16 = silu(x = linear_118_cast_fp16)[name = tensor<string, []>("input_701_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_13_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(245790272))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(248936064))), name = tensor<string, []>("module_layers_13_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_119_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_13_feed_forward1_linear2_weight_to_fp16_palettized, x = input_701_cast_fp16)[name = tensor<string, []>("linear_119_cast_fp16")];
            tensor<fp16, []> var_2351_to_fp16 = const()[name = tensor<string, []>("op_2351_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2352_cast_fp16 = mul(x = linear_119_cast_fp16, y = var_2351_to_fp16)[name = tensor<string, []>("op_2352_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_707_cast_fp16 = add(x = input_695_cast_fp16, y = var_2352_cast_fp16)[name = tensor<string, []>("input_707_cast_fp16")];
            tensor<int32, [1]> query_27_axes_0 = const()[name = tensor<string, []>("query_27_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_13_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_13_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(248936256)))];
            tensor<fp16, [1024]> module_layers_13_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_13_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(248938368)))];
            tensor<fp16, [1, 188, 1024]> query_27_cast_fp16 = layer_norm(axes = query_27_axes_0, beta = module_layers_13_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_13_norm_self_att_weight_to_fp16, x = input_707_cast_fp16)[name = tensor<string, []>("query_27_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_13_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(248940480))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(249726976))), name = tensor<string, []>("module_layers_13_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_120_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_13_self_attn_linear_q_weight_to_fp16_palettized, x = query_27_cast_fp16)[name = tensor<string, []>("linear_120_cast_fp16")];
            tensor<int32, [4]> var_2368 = const()[name = tensor<string, []>("op_2368"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_79_cast_fp16 = reshape(shape = var_2368, x = linear_120_cast_fp16)[name = tensor<string, []>("q_79_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_13_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(249727168))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(250513664))), name = tensor<string, []>("module_layers_13_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_121_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_13_self_attn_linear_k_weight_to_fp16_palettized, x = query_27_cast_fp16)[name = tensor<string, []>("linear_121_cast_fp16")];
            tensor<int32, [4]> var_2372 = const()[name = tensor<string, []>("op_2372"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_53_cast_fp16 = reshape(shape = var_2372, x = linear_121_cast_fp16)[name = tensor<string, []>("k_53_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_13_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(250513856))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(251300352))), name = tensor<string, []>("module_layers_13_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_122_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_13_self_attn_linear_v_weight_to_fp16_palettized, x = query_27_cast_fp16)[name = tensor<string, []>("linear_122_cast_fp16")];
            tensor<int32, [4]> var_2376 = const()[name = tensor<string, []>("op_2376"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_27_cast_fp16 = reshape(shape = var_2376, x = linear_122_cast_fp16)[name = tensor<string, []>("v_27_cast_fp16")];
            tensor<int32, [4]> value_29_perm_0 = const()[name = tensor<string, []>("value_29_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_13_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_13_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(251300544)))];
            tensor<fp16, [1, 188, 8, 128]> var_2388_cast_fp16 = add(x = q_79_cast_fp16, y = module_layers_13_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_2388_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_13_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_13_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(251302656)))];
            tensor<fp16, [1, 188, 8, 128]> var_2390_cast_fp16 = add(x = q_79_cast_fp16, y = module_layers_13_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_2390_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_27_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_27_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_293_transpose_x_0 = const()[name = tensor<string, []>("x_293_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_293_transpose_y_0 = const()[name = tensor<string, []>("x_293_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_2392_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(251304768))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(251592832))), name = tensor<string, []>("op_2392_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_27_cast_fp16 = transpose(perm = q_with_bias_v_27_perm_0, x = var_2390_cast_fp16)[name = tensor<string, []>("transpose_221")];
            tensor<fp16, [1, 8, 188, 375]> x_293_cast_fp16 = matmul(transpose_x = x_293_transpose_x_0, transpose_y = x_293_transpose_y_0, x = q_with_bias_v_27_cast_fp16, y = op_2392_to_fp16_palettized)[name = tensor<string, []>("x_293_cast_fp16")];
            tensor<int32, [8]> x_295_pad_0 = const()[name = tensor<string, []>("x_295_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_295_mode_0 = const()[name = tensor<string, []>("x_295_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_144_to_fp16 = const()[name = tensor<string, []>("const_144_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_295_cast_fp16 = pad(constant_val = const_144_to_fp16, mode = x_295_mode_0, pad = x_295_pad_0, x = x_293_cast_fp16)[name = tensor<string, []>("x_295_cast_fp16")];
            tensor<int32, [4]> var_2400 = const()[name = tensor<string, []>("op_2400"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_297_cast_fp16 = reshape(shape = var_2400, x = x_295_cast_fp16)[name = tensor<string, []>("x_297_cast_fp16")];
            tensor<int32, [4]> var_2404_begin_0 = const()[name = tensor<string, []>("op_2404_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_2404_end_0 = const()[name = tensor<string, []>("op_2404_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_2404_end_mask_0 = const()[name = tensor<string, []>("op_2404_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_2404_cast_fp16 = slice_by_index(begin = var_2404_begin_0, end = var_2404_end_0, end_mask = var_2404_end_mask_0, x = x_297_cast_fp16)[name = tensor<string, []>("op_2404_cast_fp16")];
            tensor<int32, [4]> var_2405 = const()[name = tensor<string, []>("op_2405"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_53_cast_fp16 = reshape(shape = var_2405, x = var_2404_cast_fp16)[name = tensor<string, []>("matrix_bd_53_cast_fp16")];
            tensor<bool, []> matrix_ac_27_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_27_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_27_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_27_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_122_perm_0 = const()[name = tensor<string, []>("transpose_122_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_123_perm_0 = const()[name = tensor<string, []>("transpose_123_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_123 = transpose(perm = transpose_123_perm_0, x = k_53_cast_fp16)[name = tensor<string, []>("transpose_219")];
            tensor<fp16, [1, 8, 188, 128]> transpose_122 = transpose(perm = transpose_122_perm_0, x = var_2388_cast_fp16)[name = tensor<string, []>("transpose_220")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_27_cast_fp16 = matmul(transpose_x = matrix_ac_27_transpose_x_0, transpose_y = matrix_ac_27_transpose_y_0, x = transpose_122, y = transpose_123)[name = tensor<string, []>("matrix_ac_27_cast_fp16")];
            tensor<int32, [4]> matrix_bd_55_begin_0 = const()[name = tensor<string, []>("matrix_bd_55_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_55_end_0 = const()[name = tensor<string, []>("matrix_bd_55_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_55_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_55_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_55_cast_fp16 = slice_by_index(begin = matrix_bd_55_begin_0, end = matrix_bd_55_end_0, end_mask = matrix_bd_55_end_mask_0, x = matrix_bd_53_cast_fp16)[name = tensor<string, []>("matrix_bd_55_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2414_cast_fp16 = add(x = matrix_ac_27_cast_fp16, y = matrix_bd_55_cast_fp16)[name = tensor<string, []>("op_2414_cast_fp16")];
            tensor<fp16, []> _inversed_scores_53_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_53_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_53_cast_fp16 = mul(x = var_2414_cast_fp16, y = _inversed_scores_53_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_53_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_55_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_53_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_55_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2420_cast_fp16 = softmax(axis = var_30, x = scores_55_cast_fp16)[name = tensor<string, []>("op_2420_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_709_cast_fp16 = select(a = var_11_to_fp16, b = var_2420_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_709_cast_fp16")];
            tensor<bool, []> x_299_transpose_x_0 = const()[name = tensor<string, []>("x_299_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_299_transpose_y_0 = const()[name = tensor<string, []>("x_299_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_29_cast_fp16 = transpose(perm = value_29_perm_0, x = v_27_cast_fp16)[name = tensor<string, []>("transpose_218")];
            tensor<fp16, [1, 8, 188, 128]> x_299_cast_fp16 = matmul(transpose_x = x_299_transpose_x_0, transpose_y = x_299_transpose_y_0, x = input_709_cast_fp16, y = value_29_cast_fp16)[name = tensor<string, []>("x_299_cast_fp16")];
            tensor<int32, [4]> var_2424_perm_0 = const()[name = tensor<string, []>("op_2424_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_2425 = const()[name = tensor<string, []>("op_2425"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_2424_cast_fp16 = transpose(perm = var_2424_perm_0, x = x_299_cast_fp16)[name = tensor<string, []>("transpose_217")];
            tensor<fp16, [1, 188, 1024]> input_711_cast_fp16 = reshape(shape = var_2425, x = var_2424_cast_fp16)[name = tensor<string, []>("input_711_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_13_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(251593024))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(252379520))), name = tensor<string, []>("module_layers_13_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_124_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_13_self_attn_linear_out_weight_to_fp16_palettized, x = input_711_cast_fp16)[name = tensor<string, []>("linear_124_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_715_cast_fp16 = add(x = input_707_cast_fp16, y = linear_124_cast_fp16)[name = tensor<string, []>("input_715_cast_fp16")];
            tensor<int32, [1]> x_303_axes_0 = const()[name = tensor<string, []>("x_303_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_13_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_13_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(252379712)))];
            tensor<fp16, [1024]> module_layers_13_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_13_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(252381824)))];
            tensor<fp16, [1, 188, 1024]> x_303_cast_fp16 = layer_norm(axes = x_303_axes_0, beta = module_layers_13_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_13_norm_conv_weight_to_fp16, x = input_715_cast_fp16)[name = tensor<string, []>("x_303_cast_fp16")];
            tensor<int32, [3]> input_717_perm_0 = const()[name = tensor<string, []>("input_717_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_719_pad_type_0 = const()[name = tensor<string, []>("input_719_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_719_strides_0 = const()[name = tensor<string, []>("input_719_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_719_pad_0 = const()[name = tensor<string, []>("input_719_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_719_dilations_0 = const()[name = tensor<string, []>("input_719_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_719_groups_0 = const()[name = tensor<string, []>("input_719_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_13_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(252383936))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(253956864))), name = tensor<string, []>("module_layers_13_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_717_cast_fp16 = transpose(perm = input_717_perm_0, x = x_303_cast_fp16)[name = tensor<string, []>("transpose_216")];
            tensor<fp16, [1, 2048, 188]> input_719_cast_fp16 = conv(dilations = input_719_dilations_0, groups = input_719_groups_0, pad = input_719_pad_0, pad_type = input_719_pad_type_0, strides = input_719_strides_0, weight = module_layers_13_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_717_cast_fp16)[name = tensor<string, []>("input_719_cast_fp16")];
            tensor<int32, []> x_305_split_num_splits_0 = const()[name = tensor<string, []>("x_305_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_305_split_axis_0 = const()[name = tensor<string, []>("x_305_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_305_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_305_split_cast_fp16_1 = split(axis = x_305_split_axis_0, num_splits = x_305_split_num_splits_0, x = input_719_cast_fp16)[name = tensor<string, []>("x_305_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_305_split_1_sigmoid_cast_fp16 = sigmoid(x = x_305_split_cast_fp16_1)[name = tensor<string, []>("x_305_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_305_cast_fp16 = mul(x = x_305_split_cast_fp16_0, y = x_305_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_305_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_721_cast_fp16 = select(a = var_11_to_fp16, b = x_305_cast_fp16, cond = var_328)[name = tensor<string, []>("input_721_cast_fp16")];
            tensor<int32, [6]> input_723_pad_0 = const()[name = tensor<string, []>("input_723_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_723_mode_0 = const()[name = tensor<string, []>("input_723_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_147_to_fp16 = const()[name = tensor<string, []>("const_147_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_723_cast_fp16 = pad(constant_val = const_147_to_fp16, mode = input_723_mode_0, pad = input_723_pad_0, x = input_721_cast_fp16)[name = tensor<string, []>("input_723_cast_fp16")];
            tensor<string, []> input_725_pad_type_0 = const()[name = tensor<string, []>("input_725_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_725_groups_0 = const()[name = tensor<string, []>("input_725_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_725_strides_0 = const()[name = tensor<string, []>("input_725_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_725_pad_0 = const()[name = tensor<string, []>("input_725_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_725_dilations_0 = const()[name = tensor<string, []>("input_725_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_274_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(253957056))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(253964032))), name = tensor<string, []>("const_274_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_275_to_fp16 = const()[name = tensor<string, []>("const_275_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(253964224)))];
            tensor<fp16, [1, 1024, 188]> input_727_cast_fp16 = conv(bias = const_275_to_fp16, dilations = input_725_dilations_0, groups = input_725_groups_0, pad = input_725_pad_0, pad_type = input_725_pad_type_0, strides = input_725_strides_0, weight = const_274_to_fp16_palettized, x = input_723_cast_fp16)[name = tensor<string, []>("input_727_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_729_cast_fp16 = silu(x = input_727_cast_fp16)[name = tensor<string, []>("input_729_cast_fp16")];
            tensor<string, []> x_307_pad_type_0 = const()[name = tensor<string, []>("x_307_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_307_strides_0 = const()[name = tensor<string, []>("x_307_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_307_pad_0 = const()[name = tensor<string, []>("x_307_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_307_dilations_0 = const()[name = tensor<string, []>("x_307_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_307_groups_0 = const()[name = tensor<string, []>("x_307_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_13_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(253966336))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(254752832))), name = tensor<string, []>("module_layers_13_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_307_cast_fp16 = conv(dilations = x_307_dilations_0, groups = x_307_groups_0, pad = x_307_pad_0, pad_type = x_307_pad_type_0, strides = x_307_strides_0, weight = module_layers_13_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_729_cast_fp16)[name = tensor<string, []>("x_307_cast_fp16")];
            tensor<int32, [3]> input_731_perm_0 = const()[name = tensor<string, []>("input_731_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_731_cast_fp16 = transpose(perm = input_731_perm_0, x = x_307_cast_fp16)[name = tensor<string, []>("transpose_215")];
            tensor<fp16, [1, 188, 1024]> input_733_cast_fp16 = add(x = input_715_cast_fp16, y = input_731_cast_fp16)[name = tensor<string, []>("input_733_cast_fp16")];
            tensor<int32, [1]> input_735_axes_0 = const()[name = tensor<string, []>("input_735_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_13_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_13_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(254753024)))];
            tensor<fp16, [1024]> module_layers_13_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_13_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(254755136)))];
            tensor<fp16, [1, 188, 1024]> input_735_cast_fp16 = layer_norm(axes = input_735_axes_0, beta = module_layers_13_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_13_norm_feed_forward2_weight_to_fp16, x = input_733_cast_fp16)[name = tensor<string, []>("input_735_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_13_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(254757248))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(257903040))), name = tensor<string, []>("module_layers_13_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_125_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_13_feed_forward2_linear1_weight_to_fp16_palettized, x = input_735_cast_fp16)[name = tensor<string, []>("linear_125_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_739_cast_fp16 = silu(x = linear_125_cast_fp16)[name = tensor<string, []>("input_739_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_13_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(257903232))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(261049024))), name = tensor<string, []>("module_layers_13_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_126_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_13_feed_forward2_linear2_weight_to_fp16_palettized, x = input_739_cast_fp16)[name = tensor<string, []>("linear_126_cast_fp16")];
            tensor<fp16, []> var_2485_to_fp16 = const()[name = tensor<string, []>("op_2485_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2486_cast_fp16 = mul(x = linear_126_cast_fp16, y = var_2485_to_fp16)[name = tensor<string, []>("op_2486_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_745_cast_fp16 = add(x = input_733_cast_fp16, y = var_2486_cast_fp16)[name = tensor<string, []>("input_745_cast_fp16")];
            tensor<int32, [1]> input_747_axes_0 = const()[name = tensor<string, []>("input_747_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_13_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_13_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(261049216)))];
            tensor<fp16, [1024]> module_layers_13_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_13_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(261051328)))];
            tensor<fp16, [1, 188, 1024]> input_747_cast_fp16 = layer_norm(axes = input_747_axes_0, beta = module_layers_13_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_13_norm_out_weight_to_fp16, x = input_745_cast_fp16)[name = tensor<string, []>("input_747_cast_fp16")];
            tensor<int32, [1]> input_749_axes_0 = const()[name = tensor<string, []>("input_749_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_14_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_14_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(261053440)))];
            tensor<fp16, [1024]> module_layers_14_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_14_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(261055552)))];
            tensor<fp16, [1, 188, 1024]> input_749_cast_fp16 = layer_norm(axes = input_749_axes_0, beta = module_layers_14_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_14_norm_feed_forward1_weight_to_fp16, x = input_747_cast_fp16)[name = tensor<string, []>("input_749_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_14_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(261057664))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(264203456))), name = tensor<string, []>("module_layers_14_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_127_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_14_feed_forward1_linear1_weight_to_fp16_palettized, x = input_749_cast_fp16)[name = tensor<string, []>("linear_127_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_753_cast_fp16 = silu(x = linear_127_cast_fp16)[name = tensor<string, []>("input_753_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_14_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(264203648))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(267349440))), name = tensor<string, []>("module_layers_14_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_128_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_14_feed_forward1_linear2_weight_to_fp16_palettized, x = input_753_cast_fp16)[name = tensor<string, []>("linear_128_cast_fp16")];
            tensor<fp16, []> var_2514_to_fp16 = const()[name = tensor<string, []>("op_2514_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2515_cast_fp16 = mul(x = linear_128_cast_fp16, y = var_2514_to_fp16)[name = tensor<string, []>("op_2515_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_759_cast_fp16 = add(x = input_747_cast_fp16, y = var_2515_cast_fp16)[name = tensor<string, []>("input_759_cast_fp16")];
            tensor<int32, [1]> query_29_axes_0 = const()[name = tensor<string, []>("query_29_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_14_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_14_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(267349632)))];
            tensor<fp16, [1024]> module_layers_14_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_14_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(267351744)))];
            tensor<fp16, [1, 188, 1024]> query_29_cast_fp16 = layer_norm(axes = query_29_axes_0, beta = module_layers_14_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_14_norm_self_att_weight_to_fp16, x = input_759_cast_fp16)[name = tensor<string, []>("query_29_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_14_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(267353856))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(268140352))), name = tensor<string, []>("module_layers_14_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_129_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_14_self_attn_linear_q_weight_to_fp16_palettized, x = query_29_cast_fp16)[name = tensor<string, []>("linear_129_cast_fp16")];
            tensor<int32, [4]> var_2531 = const()[name = tensor<string, []>("op_2531"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_85_cast_fp16 = reshape(shape = var_2531, x = linear_129_cast_fp16)[name = tensor<string, []>("q_85_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_14_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(268140544))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(268927040))), name = tensor<string, []>("module_layers_14_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_130_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_14_self_attn_linear_k_weight_to_fp16_palettized, x = query_29_cast_fp16)[name = tensor<string, []>("linear_130_cast_fp16")];
            tensor<int32, [4]> var_2535 = const()[name = tensor<string, []>("op_2535"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_57_cast_fp16 = reshape(shape = var_2535, x = linear_130_cast_fp16)[name = tensor<string, []>("k_57_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_14_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(268927232))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(269713728))), name = tensor<string, []>("module_layers_14_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_131_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_14_self_attn_linear_v_weight_to_fp16_palettized, x = query_29_cast_fp16)[name = tensor<string, []>("linear_131_cast_fp16")];
            tensor<int32, [4]> var_2539 = const()[name = tensor<string, []>("op_2539"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_29_cast_fp16 = reshape(shape = var_2539, x = linear_131_cast_fp16)[name = tensor<string, []>("v_29_cast_fp16")];
            tensor<int32, [4]> value_31_perm_0 = const()[name = tensor<string, []>("value_31_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_14_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_14_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(269713920)))];
            tensor<fp16, [1, 188, 8, 128]> var_2551_cast_fp16 = add(x = q_85_cast_fp16, y = module_layers_14_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_2551_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_14_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_14_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(269716032)))];
            tensor<fp16, [1, 188, 8, 128]> var_2553_cast_fp16 = add(x = q_85_cast_fp16, y = module_layers_14_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_2553_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_29_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_29_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_315_transpose_x_0 = const()[name = tensor<string, []>("x_315_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_315_transpose_y_0 = const()[name = tensor<string, []>("x_315_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_2555_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(269718144))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(270006208))), name = tensor<string, []>("op_2555_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_29_cast_fp16 = transpose(perm = q_with_bias_v_29_perm_0, x = var_2553_cast_fp16)[name = tensor<string, []>("transpose_214")];
            tensor<fp16, [1, 8, 188, 375]> x_315_cast_fp16 = matmul(transpose_x = x_315_transpose_x_0, transpose_y = x_315_transpose_y_0, x = q_with_bias_v_29_cast_fp16, y = op_2555_to_fp16_palettized)[name = tensor<string, []>("x_315_cast_fp16")];
            tensor<int32, [8]> x_317_pad_0 = const()[name = tensor<string, []>("x_317_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_317_mode_0 = const()[name = tensor<string, []>("x_317_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_154_to_fp16 = const()[name = tensor<string, []>("const_154_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_317_cast_fp16 = pad(constant_val = const_154_to_fp16, mode = x_317_mode_0, pad = x_317_pad_0, x = x_315_cast_fp16)[name = tensor<string, []>("x_317_cast_fp16")];
            tensor<int32, [4]> var_2563 = const()[name = tensor<string, []>("op_2563"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_319_cast_fp16 = reshape(shape = var_2563, x = x_317_cast_fp16)[name = tensor<string, []>("x_319_cast_fp16")];
            tensor<int32, [4]> var_2567_begin_0 = const()[name = tensor<string, []>("op_2567_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_2567_end_0 = const()[name = tensor<string, []>("op_2567_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_2567_end_mask_0 = const()[name = tensor<string, []>("op_2567_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_2567_cast_fp16 = slice_by_index(begin = var_2567_begin_0, end = var_2567_end_0, end_mask = var_2567_end_mask_0, x = x_319_cast_fp16)[name = tensor<string, []>("op_2567_cast_fp16")];
            tensor<int32, [4]> var_2568 = const()[name = tensor<string, []>("op_2568"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_57_cast_fp16 = reshape(shape = var_2568, x = var_2567_cast_fp16)[name = tensor<string, []>("matrix_bd_57_cast_fp16")];
            tensor<bool, []> matrix_ac_29_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_29_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_29_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_29_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_124_perm_0 = const()[name = tensor<string, []>("transpose_124_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_125_perm_0 = const()[name = tensor<string, []>("transpose_125_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_125 = transpose(perm = transpose_125_perm_0, x = k_57_cast_fp16)[name = tensor<string, []>("transpose_212")];
            tensor<fp16, [1, 8, 188, 128]> transpose_124 = transpose(perm = transpose_124_perm_0, x = var_2551_cast_fp16)[name = tensor<string, []>("transpose_213")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_29_cast_fp16 = matmul(transpose_x = matrix_ac_29_transpose_x_0, transpose_y = matrix_ac_29_transpose_y_0, x = transpose_124, y = transpose_125)[name = tensor<string, []>("matrix_ac_29_cast_fp16")];
            tensor<int32, [4]> matrix_bd_59_begin_0 = const()[name = tensor<string, []>("matrix_bd_59_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_59_end_0 = const()[name = tensor<string, []>("matrix_bd_59_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_59_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_59_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_59_cast_fp16 = slice_by_index(begin = matrix_bd_59_begin_0, end = matrix_bd_59_end_0, end_mask = matrix_bd_59_end_mask_0, x = matrix_bd_57_cast_fp16)[name = tensor<string, []>("matrix_bd_59_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2577_cast_fp16 = add(x = matrix_ac_29_cast_fp16, y = matrix_bd_59_cast_fp16)[name = tensor<string, []>("op_2577_cast_fp16")];
            tensor<fp16, []> _inversed_scores_57_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_57_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_57_cast_fp16 = mul(x = var_2577_cast_fp16, y = _inversed_scores_57_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_57_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_59_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_57_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_59_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2583_cast_fp16 = softmax(axis = var_30, x = scores_59_cast_fp16)[name = tensor<string, []>("op_2583_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_761_cast_fp16 = select(a = var_11_to_fp16, b = var_2583_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_761_cast_fp16")];
            tensor<bool, []> x_321_transpose_x_0 = const()[name = tensor<string, []>("x_321_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_321_transpose_y_0 = const()[name = tensor<string, []>("x_321_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_31_cast_fp16 = transpose(perm = value_31_perm_0, x = v_29_cast_fp16)[name = tensor<string, []>("transpose_211")];
            tensor<fp16, [1, 8, 188, 128]> x_321_cast_fp16 = matmul(transpose_x = x_321_transpose_x_0, transpose_y = x_321_transpose_y_0, x = input_761_cast_fp16, y = value_31_cast_fp16)[name = tensor<string, []>("x_321_cast_fp16")];
            tensor<int32, [4]> var_2587_perm_0 = const()[name = tensor<string, []>("op_2587_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_2588 = const()[name = tensor<string, []>("op_2588"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_2587_cast_fp16 = transpose(perm = var_2587_perm_0, x = x_321_cast_fp16)[name = tensor<string, []>("transpose_210")];
            tensor<fp16, [1, 188, 1024]> input_763_cast_fp16 = reshape(shape = var_2588, x = var_2587_cast_fp16)[name = tensor<string, []>("input_763_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_14_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(270006400))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(270792896))), name = tensor<string, []>("module_layers_14_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_133_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_14_self_attn_linear_out_weight_to_fp16_palettized, x = input_763_cast_fp16)[name = tensor<string, []>("linear_133_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_767_cast_fp16 = add(x = input_759_cast_fp16, y = linear_133_cast_fp16)[name = tensor<string, []>("input_767_cast_fp16")];
            tensor<int32, [1]> x_325_axes_0 = const()[name = tensor<string, []>("x_325_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_14_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_14_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(270793088)))];
            tensor<fp16, [1024]> module_layers_14_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_14_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(270795200)))];
            tensor<fp16, [1, 188, 1024]> x_325_cast_fp16 = layer_norm(axes = x_325_axes_0, beta = module_layers_14_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_14_norm_conv_weight_to_fp16, x = input_767_cast_fp16)[name = tensor<string, []>("x_325_cast_fp16")];
            tensor<int32, [3]> input_769_perm_0 = const()[name = tensor<string, []>("input_769_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_771_pad_type_0 = const()[name = tensor<string, []>("input_771_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_771_strides_0 = const()[name = tensor<string, []>("input_771_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_771_pad_0 = const()[name = tensor<string, []>("input_771_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_771_dilations_0 = const()[name = tensor<string, []>("input_771_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_771_groups_0 = const()[name = tensor<string, []>("input_771_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_14_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(270797312))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(272370240))), name = tensor<string, []>("module_layers_14_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_769_cast_fp16 = transpose(perm = input_769_perm_0, x = x_325_cast_fp16)[name = tensor<string, []>("transpose_209")];
            tensor<fp16, [1, 2048, 188]> input_771_cast_fp16 = conv(dilations = input_771_dilations_0, groups = input_771_groups_0, pad = input_771_pad_0, pad_type = input_771_pad_type_0, strides = input_771_strides_0, weight = module_layers_14_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_769_cast_fp16)[name = tensor<string, []>("input_771_cast_fp16")];
            tensor<int32, []> x_327_split_num_splits_0 = const()[name = tensor<string, []>("x_327_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_327_split_axis_0 = const()[name = tensor<string, []>("x_327_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_327_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_327_split_cast_fp16_1 = split(axis = x_327_split_axis_0, num_splits = x_327_split_num_splits_0, x = input_771_cast_fp16)[name = tensor<string, []>("x_327_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_327_split_1_sigmoid_cast_fp16 = sigmoid(x = x_327_split_cast_fp16_1)[name = tensor<string, []>("x_327_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_327_cast_fp16 = mul(x = x_327_split_cast_fp16_0, y = x_327_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_327_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_773_cast_fp16 = select(a = var_11_to_fp16, b = x_327_cast_fp16, cond = var_328)[name = tensor<string, []>("input_773_cast_fp16")];
            tensor<int32, [6]> input_775_pad_0 = const()[name = tensor<string, []>("input_775_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_775_mode_0 = const()[name = tensor<string, []>("input_775_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_157_to_fp16 = const()[name = tensor<string, []>("const_157_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_775_cast_fp16 = pad(constant_val = const_157_to_fp16, mode = input_775_mode_0, pad = input_775_pad_0, x = input_773_cast_fp16)[name = tensor<string, []>("input_775_cast_fp16")];
            tensor<string, []> input_777_pad_type_0 = const()[name = tensor<string, []>("input_777_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_777_groups_0 = const()[name = tensor<string, []>("input_777_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_777_strides_0 = const()[name = tensor<string, []>("input_777_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_777_pad_0 = const()[name = tensor<string, []>("input_777_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_777_dilations_0 = const()[name = tensor<string, []>("input_777_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_276_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(272370432))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(272377408))), name = tensor<string, []>("const_276_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_277_to_fp16 = const()[name = tensor<string, []>("const_277_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(272377600)))];
            tensor<fp16, [1, 1024, 188]> input_779_cast_fp16 = conv(bias = const_277_to_fp16, dilations = input_777_dilations_0, groups = input_777_groups_0, pad = input_777_pad_0, pad_type = input_777_pad_type_0, strides = input_777_strides_0, weight = const_276_to_fp16_palettized, x = input_775_cast_fp16)[name = tensor<string, []>("input_779_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_781_cast_fp16 = silu(x = input_779_cast_fp16)[name = tensor<string, []>("input_781_cast_fp16")];
            tensor<string, []> x_329_pad_type_0 = const()[name = tensor<string, []>("x_329_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_329_strides_0 = const()[name = tensor<string, []>("x_329_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_329_pad_0 = const()[name = tensor<string, []>("x_329_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_329_dilations_0 = const()[name = tensor<string, []>("x_329_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_329_groups_0 = const()[name = tensor<string, []>("x_329_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_14_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(272379712))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(273166208))), name = tensor<string, []>("module_layers_14_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_329_cast_fp16 = conv(dilations = x_329_dilations_0, groups = x_329_groups_0, pad = x_329_pad_0, pad_type = x_329_pad_type_0, strides = x_329_strides_0, weight = module_layers_14_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_781_cast_fp16)[name = tensor<string, []>("x_329_cast_fp16")];
            tensor<int32, [3]> input_783_perm_0 = const()[name = tensor<string, []>("input_783_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_783_cast_fp16 = transpose(perm = input_783_perm_0, x = x_329_cast_fp16)[name = tensor<string, []>("transpose_208")];
            tensor<fp16, [1, 188, 1024]> input_785_cast_fp16 = add(x = input_767_cast_fp16, y = input_783_cast_fp16)[name = tensor<string, []>("input_785_cast_fp16")];
            tensor<int32, [1]> input_787_axes_0 = const()[name = tensor<string, []>("input_787_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_14_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_14_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(273166400)))];
            tensor<fp16, [1024]> module_layers_14_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_14_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(273168512)))];
            tensor<fp16, [1, 188, 1024]> input_787_cast_fp16 = layer_norm(axes = input_787_axes_0, beta = module_layers_14_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_14_norm_feed_forward2_weight_to_fp16, x = input_785_cast_fp16)[name = tensor<string, []>("input_787_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_14_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(273170624))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(276316416))), name = tensor<string, []>("module_layers_14_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_134_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_14_feed_forward2_linear1_weight_to_fp16_palettized, x = input_787_cast_fp16)[name = tensor<string, []>("linear_134_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_791_cast_fp16 = silu(x = linear_134_cast_fp16)[name = tensor<string, []>("input_791_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_14_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(276316608))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(279462400))), name = tensor<string, []>("module_layers_14_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_135_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_14_feed_forward2_linear2_weight_to_fp16_palettized, x = input_791_cast_fp16)[name = tensor<string, []>("linear_135_cast_fp16")];
            tensor<fp16, []> var_2648_to_fp16 = const()[name = tensor<string, []>("op_2648_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2649_cast_fp16 = mul(x = linear_135_cast_fp16, y = var_2648_to_fp16)[name = tensor<string, []>("op_2649_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_797_cast_fp16 = add(x = input_785_cast_fp16, y = var_2649_cast_fp16)[name = tensor<string, []>("input_797_cast_fp16")];
            tensor<int32, [1]> input_799_axes_0 = const()[name = tensor<string, []>("input_799_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_14_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_14_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(279462592)))];
            tensor<fp16, [1024]> module_layers_14_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_14_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(279464704)))];
            tensor<fp16, [1, 188, 1024]> input_799_cast_fp16 = layer_norm(axes = input_799_axes_0, beta = module_layers_14_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_14_norm_out_weight_to_fp16, x = input_797_cast_fp16)[name = tensor<string, []>("input_799_cast_fp16")];
            tensor<int32, [1]> input_801_axes_0 = const()[name = tensor<string, []>("input_801_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_15_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_15_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(279466816)))];
            tensor<fp16, [1024]> module_layers_15_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_15_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(279468928)))];
            tensor<fp16, [1, 188, 1024]> input_801_cast_fp16 = layer_norm(axes = input_801_axes_0, beta = module_layers_15_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_15_norm_feed_forward1_weight_to_fp16, x = input_799_cast_fp16)[name = tensor<string, []>("input_801_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_15_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(279471040))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(282616832))), name = tensor<string, []>("module_layers_15_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_136_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_15_feed_forward1_linear1_weight_to_fp16_palettized, x = input_801_cast_fp16)[name = tensor<string, []>("linear_136_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_805_cast_fp16 = silu(x = linear_136_cast_fp16)[name = tensor<string, []>("input_805_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_15_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(282617024))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(285762816))), name = tensor<string, []>("module_layers_15_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_137_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_15_feed_forward1_linear2_weight_to_fp16_palettized, x = input_805_cast_fp16)[name = tensor<string, []>("linear_137_cast_fp16")];
            tensor<fp16, []> var_2677_to_fp16 = const()[name = tensor<string, []>("op_2677_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2678_cast_fp16 = mul(x = linear_137_cast_fp16, y = var_2677_to_fp16)[name = tensor<string, []>("op_2678_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_811_cast_fp16 = add(x = input_799_cast_fp16, y = var_2678_cast_fp16)[name = tensor<string, []>("input_811_cast_fp16")];
            tensor<int32, [1]> query_31_axes_0 = const()[name = tensor<string, []>("query_31_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_15_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_15_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(285763008)))];
            tensor<fp16, [1024]> module_layers_15_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_15_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(285765120)))];
            tensor<fp16, [1, 188, 1024]> query_31_cast_fp16 = layer_norm(axes = query_31_axes_0, beta = module_layers_15_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_15_norm_self_att_weight_to_fp16, x = input_811_cast_fp16)[name = tensor<string, []>("query_31_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_15_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(285767232))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(286553728))), name = tensor<string, []>("module_layers_15_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_138_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_15_self_attn_linear_q_weight_to_fp16_palettized, x = query_31_cast_fp16)[name = tensor<string, []>("linear_138_cast_fp16")];
            tensor<int32, [4]> var_2694 = const()[name = tensor<string, []>("op_2694"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_91_cast_fp16 = reshape(shape = var_2694, x = linear_138_cast_fp16)[name = tensor<string, []>("q_91_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_15_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(286553920))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(287340416))), name = tensor<string, []>("module_layers_15_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_139_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_15_self_attn_linear_k_weight_to_fp16_palettized, x = query_31_cast_fp16)[name = tensor<string, []>("linear_139_cast_fp16")];
            tensor<int32, [4]> var_2698 = const()[name = tensor<string, []>("op_2698"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_61_cast_fp16 = reshape(shape = var_2698, x = linear_139_cast_fp16)[name = tensor<string, []>("k_61_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_15_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(287340608))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(288127104))), name = tensor<string, []>("module_layers_15_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_140_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_15_self_attn_linear_v_weight_to_fp16_palettized, x = query_31_cast_fp16)[name = tensor<string, []>("linear_140_cast_fp16")];
            tensor<int32, [4]> var_2702 = const()[name = tensor<string, []>("op_2702"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_31_cast_fp16 = reshape(shape = var_2702, x = linear_140_cast_fp16)[name = tensor<string, []>("v_31_cast_fp16")];
            tensor<int32, [4]> value_33_perm_0 = const()[name = tensor<string, []>("value_33_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_15_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_15_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(288127296)))];
            tensor<fp16, [1, 188, 8, 128]> var_2714_cast_fp16 = add(x = q_91_cast_fp16, y = module_layers_15_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_2714_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_15_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_15_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(288129408)))];
            tensor<fp16, [1, 188, 8, 128]> var_2716_cast_fp16 = add(x = q_91_cast_fp16, y = module_layers_15_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_2716_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_31_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_31_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_337_transpose_x_0 = const()[name = tensor<string, []>("x_337_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_337_transpose_y_0 = const()[name = tensor<string, []>("x_337_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_2718_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(288131520))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(288419584))), name = tensor<string, []>("op_2718_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_31_cast_fp16 = transpose(perm = q_with_bias_v_31_perm_0, x = var_2716_cast_fp16)[name = tensor<string, []>("transpose_207")];
            tensor<fp16, [1, 8, 188, 375]> x_337_cast_fp16 = matmul(transpose_x = x_337_transpose_x_0, transpose_y = x_337_transpose_y_0, x = q_with_bias_v_31_cast_fp16, y = op_2718_to_fp16_palettized)[name = tensor<string, []>("x_337_cast_fp16")];
            tensor<int32, [8]> x_339_pad_0 = const()[name = tensor<string, []>("x_339_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_339_mode_0 = const()[name = tensor<string, []>("x_339_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_164_to_fp16 = const()[name = tensor<string, []>("const_164_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_339_cast_fp16 = pad(constant_val = const_164_to_fp16, mode = x_339_mode_0, pad = x_339_pad_0, x = x_337_cast_fp16)[name = tensor<string, []>("x_339_cast_fp16")];
            tensor<int32, [4]> var_2726 = const()[name = tensor<string, []>("op_2726"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_341_cast_fp16 = reshape(shape = var_2726, x = x_339_cast_fp16)[name = tensor<string, []>("x_341_cast_fp16")];
            tensor<int32, [4]> var_2730_begin_0 = const()[name = tensor<string, []>("op_2730_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_2730_end_0 = const()[name = tensor<string, []>("op_2730_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_2730_end_mask_0 = const()[name = tensor<string, []>("op_2730_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_2730_cast_fp16 = slice_by_index(begin = var_2730_begin_0, end = var_2730_end_0, end_mask = var_2730_end_mask_0, x = x_341_cast_fp16)[name = tensor<string, []>("op_2730_cast_fp16")];
            tensor<int32, [4]> var_2731 = const()[name = tensor<string, []>("op_2731"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_61_cast_fp16 = reshape(shape = var_2731, x = var_2730_cast_fp16)[name = tensor<string, []>("matrix_bd_61_cast_fp16")];
            tensor<bool, []> matrix_ac_31_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_31_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_31_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_31_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_126_perm_0 = const()[name = tensor<string, []>("transpose_126_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_127_perm_0 = const()[name = tensor<string, []>("transpose_127_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_127 = transpose(perm = transpose_127_perm_0, x = k_61_cast_fp16)[name = tensor<string, []>("transpose_205")];
            tensor<fp16, [1, 8, 188, 128]> transpose_126 = transpose(perm = transpose_126_perm_0, x = var_2714_cast_fp16)[name = tensor<string, []>("transpose_206")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_31_cast_fp16 = matmul(transpose_x = matrix_ac_31_transpose_x_0, transpose_y = matrix_ac_31_transpose_y_0, x = transpose_126, y = transpose_127)[name = tensor<string, []>("matrix_ac_31_cast_fp16")];
            tensor<int32, [4]> matrix_bd_63_begin_0 = const()[name = tensor<string, []>("matrix_bd_63_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_63_end_0 = const()[name = tensor<string, []>("matrix_bd_63_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_63_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_63_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_63_cast_fp16 = slice_by_index(begin = matrix_bd_63_begin_0, end = matrix_bd_63_end_0, end_mask = matrix_bd_63_end_mask_0, x = matrix_bd_61_cast_fp16)[name = tensor<string, []>("matrix_bd_63_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2740_cast_fp16 = add(x = matrix_ac_31_cast_fp16, y = matrix_bd_63_cast_fp16)[name = tensor<string, []>("op_2740_cast_fp16")];
            tensor<fp16, []> _inversed_scores_61_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_61_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_61_cast_fp16 = mul(x = var_2740_cast_fp16, y = _inversed_scores_61_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_61_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_63_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_61_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_63_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2746_cast_fp16 = softmax(axis = var_30, x = scores_63_cast_fp16)[name = tensor<string, []>("op_2746_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_813_cast_fp16 = select(a = var_11_to_fp16, b = var_2746_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_813_cast_fp16")];
            tensor<bool, []> x_343_transpose_x_0 = const()[name = tensor<string, []>("x_343_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_343_transpose_y_0 = const()[name = tensor<string, []>("x_343_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_33_cast_fp16 = transpose(perm = value_33_perm_0, x = v_31_cast_fp16)[name = tensor<string, []>("transpose_204")];
            tensor<fp16, [1, 8, 188, 128]> x_343_cast_fp16 = matmul(transpose_x = x_343_transpose_x_0, transpose_y = x_343_transpose_y_0, x = input_813_cast_fp16, y = value_33_cast_fp16)[name = tensor<string, []>("x_343_cast_fp16")];
            tensor<int32, [4]> var_2750_perm_0 = const()[name = tensor<string, []>("op_2750_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_2751 = const()[name = tensor<string, []>("op_2751"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_2750_cast_fp16 = transpose(perm = var_2750_perm_0, x = x_343_cast_fp16)[name = tensor<string, []>("transpose_203")];
            tensor<fp16, [1, 188, 1024]> input_815_cast_fp16 = reshape(shape = var_2751, x = var_2750_cast_fp16)[name = tensor<string, []>("input_815_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_15_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(288419776))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(289206272))), name = tensor<string, []>("module_layers_15_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_142_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_15_self_attn_linear_out_weight_to_fp16_palettized, x = input_815_cast_fp16)[name = tensor<string, []>("linear_142_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_819_cast_fp16 = add(x = input_811_cast_fp16, y = linear_142_cast_fp16)[name = tensor<string, []>("input_819_cast_fp16")];
            tensor<int32, [1]> x_347_axes_0 = const()[name = tensor<string, []>("x_347_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_15_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_15_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(289206464)))];
            tensor<fp16, [1024]> module_layers_15_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_15_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(289208576)))];
            tensor<fp16, [1, 188, 1024]> x_347_cast_fp16 = layer_norm(axes = x_347_axes_0, beta = module_layers_15_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_15_norm_conv_weight_to_fp16, x = input_819_cast_fp16)[name = tensor<string, []>("x_347_cast_fp16")];
            tensor<int32, [3]> input_821_perm_0 = const()[name = tensor<string, []>("input_821_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_823_pad_type_0 = const()[name = tensor<string, []>("input_823_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_823_strides_0 = const()[name = tensor<string, []>("input_823_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_823_pad_0 = const()[name = tensor<string, []>("input_823_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_823_dilations_0 = const()[name = tensor<string, []>("input_823_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_823_groups_0 = const()[name = tensor<string, []>("input_823_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_15_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(289210688))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(290783616))), name = tensor<string, []>("module_layers_15_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_821_cast_fp16 = transpose(perm = input_821_perm_0, x = x_347_cast_fp16)[name = tensor<string, []>("transpose_202")];
            tensor<fp16, [1, 2048, 188]> input_823_cast_fp16 = conv(dilations = input_823_dilations_0, groups = input_823_groups_0, pad = input_823_pad_0, pad_type = input_823_pad_type_0, strides = input_823_strides_0, weight = module_layers_15_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_821_cast_fp16)[name = tensor<string, []>("input_823_cast_fp16")];
            tensor<int32, []> x_349_split_num_splits_0 = const()[name = tensor<string, []>("x_349_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_349_split_axis_0 = const()[name = tensor<string, []>("x_349_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_349_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_349_split_cast_fp16_1 = split(axis = x_349_split_axis_0, num_splits = x_349_split_num_splits_0, x = input_823_cast_fp16)[name = tensor<string, []>("x_349_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_349_split_1_sigmoid_cast_fp16 = sigmoid(x = x_349_split_cast_fp16_1)[name = tensor<string, []>("x_349_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_349_cast_fp16 = mul(x = x_349_split_cast_fp16_0, y = x_349_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_349_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_825_cast_fp16 = select(a = var_11_to_fp16, b = x_349_cast_fp16, cond = var_328)[name = tensor<string, []>("input_825_cast_fp16")];
            tensor<int32, [6]> input_827_pad_0 = const()[name = tensor<string, []>("input_827_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_827_mode_0 = const()[name = tensor<string, []>("input_827_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_167_to_fp16 = const()[name = tensor<string, []>("const_167_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_827_cast_fp16 = pad(constant_val = const_167_to_fp16, mode = input_827_mode_0, pad = input_827_pad_0, x = input_825_cast_fp16)[name = tensor<string, []>("input_827_cast_fp16")];
            tensor<string, []> input_829_pad_type_0 = const()[name = tensor<string, []>("input_829_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_829_groups_0 = const()[name = tensor<string, []>("input_829_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_829_strides_0 = const()[name = tensor<string, []>("input_829_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_829_pad_0 = const()[name = tensor<string, []>("input_829_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_829_dilations_0 = const()[name = tensor<string, []>("input_829_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_278_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(290783808))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(290790784))), name = tensor<string, []>("const_278_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_279_to_fp16 = const()[name = tensor<string, []>("const_279_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(290790976)))];
            tensor<fp16, [1, 1024, 188]> input_831_cast_fp16 = conv(bias = const_279_to_fp16, dilations = input_829_dilations_0, groups = input_829_groups_0, pad = input_829_pad_0, pad_type = input_829_pad_type_0, strides = input_829_strides_0, weight = const_278_to_fp16_palettized, x = input_827_cast_fp16)[name = tensor<string, []>("input_831_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_833_cast_fp16 = silu(x = input_831_cast_fp16)[name = tensor<string, []>("input_833_cast_fp16")];
            tensor<string, []> x_351_pad_type_0 = const()[name = tensor<string, []>("x_351_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_351_strides_0 = const()[name = tensor<string, []>("x_351_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_351_pad_0 = const()[name = tensor<string, []>("x_351_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_351_dilations_0 = const()[name = tensor<string, []>("x_351_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_351_groups_0 = const()[name = tensor<string, []>("x_351_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_15_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(290793088))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(291579584))), name = tensor<string, []>("module_layers_15_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_351_cast_fp16 = conv(dilations = x_351_dilations_0, groups = x_351_groups_0, pad = x_351_pad_0, pad_type = x_351_pad_type_0, strides = x_351_strides_0, weight = module_layers_15_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_833_cast_fp16)[name = tensor<string, []>("x_351_cast_fp16")];
            tensor<int32, [3]> input_835_perm_0 = const()[name = tensor<string, []>("input_835_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_835_cast_fp16 = transpose(perm = input_835_perm_0, x = x_351_cast_fp16)[name = tensor<string, []>("transpose_201")];
            tensor<fp16, [1, 188, 1024]> input_837_cast_fp16 = add(x = input_819_cast_fp16, y = input_835_cast_fp16)[name = tensor<string, []>("input_837_cast_fp16")];
            tensor<int32, [1]> input_839_axes_0 = const()[name = tensor<string, []>("input_839_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_15_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_15_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(291579776)))];
            tensor<fp16, [1024]> module_layers_15_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_15_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(291581888)))];
            tensor<fp16, [1, 188, 1024]> input_839_cast_fp16 = layer_norm(axes = input_839_axes_0, beta = module_layers_15_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_15_norm_feed_forward2_weight_to_fp16, x = input_837_cast_fp16)[name = tensor<string, []>("input_839_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_15_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(291584000))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(294729792))), name = tensor<string, []>("module_layers_15_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_143_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_15_feed_forward2_linear1_weight_to_fp16_palettized, x = input_839_cast_fp16)[name = tensor<string, []>("linear_143_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_843_cast_fp16 = silu(x = linear_143_cast_fp16)[name = tensor<string, []>("input_843_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_15_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(294729984))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(297875776))), name = tensor<string, []>("module_layers_15_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_144_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_15_feed_forward2_linear2_weight_to_fp16_palettized, x = input_843_cast_fp16)[name = tensor<string, []>("linear_144_cast_fp16")];
            tensor<fp16, []> var_2811_to_fp16 = const()[name = tensor<string, []>("op_2811_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2812_cast_fp16 = mul(x = linear_144_cast_fp16, y = var_2811_to_fp16)[name = tensor<string, []>("op_2812_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_849_cast_fp16 = add(x = input_837_cast_fp16, y = var_2812_cast_fp16)[name = tensor<string, []>("input_849_cast_fp16")];
            tensor<int32, [1]> input_851_axes_0 = const()[name = tensor<string, []>("input_851_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_15_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_15_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(297875968)))];
            tensor<fp16, [1024]> module_layers_15_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_15_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(297878080)))];
            tensor<fp16, [1, 188, 1024]> input_851_cast_fp16 = layer_norm(axes = input_851_axes_0, beta = module_layers_15_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_15_norm_out_weight_to_fp16, x = input_849_cast_fp16)[name = tensor<string, []>("input_851_cast_fp16")];
            tensor<int32, [1]> input_853_axes_0 = const()[name = tensor<string, []>("input_853_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_16_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_16_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(297880192)))];
            tensor<fp16, [1024]> module_layers_16_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_16_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(297882304)))];
            tensor<fp16, [1, 188, 1024]> input_853_cast_fp16 = layer_norm(axes = input_853_axes_0, beta = module_layers_16_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_16_norm_feed_forward1_weight_to_fp16, x = input_851_cast_fp16)[name = tensor<string, []>("input_853_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_16_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(297884416))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(301030208))), name = tensor<string, []>("module_layers_16_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_145_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_16_feed_forward1_linear1_weight_to_fp16_palettized, x = input_853_cast_fp16)[name = tensor<string, []>("linear_145_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_857_cast_fp16 = silu(x = linear_145_cast_fp16)[name = tensor<string, []>("input_857_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_16_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(301030400))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(304176192))), name = tensor<string, []>("module_layers_16_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_146_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_16_feed_forward1_linear2_weight_to_fp16_palettized, x = input_857_cast_fp16)[name = tensor<string, []>("linear_146_cast_fp16")];
            tensor<fp16, []> var_2840_to_fp16 = const()[name = tensor<string, []>("op_2840_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2841_cast_fp16 = mul(x = linear_146_cast_fp16, y = var_2840_to_fp16)[name = tensor<string, []>("op_2841_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_863_cast_fp16 = add(x = input_851_cast_fp16, y = var_2841_cast_fp16)[name = tensor<string, []>("input_863_cast_fp16")];
            tensor<int32, [1]> query_33_axes_0 = const()[name = tensor<string, []>("query_33_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_16_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_16_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(304176384)))];
            tensor<fp16, [1024]> module_layers_16_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_16_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(304178496)))];
            tensor<fp16, [1, 188, 1024]> query_33_cast_fp16 = layer_norm(axes = query_33_axes_0, beta = module_layers_16_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_16_norm_self_att_weight_to_fp16, x = input_863_cast_fp16)[name = tensor<string, []>("query_33_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_16_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(304180608))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(304967104))), name = tensor<string, []>("module_layers_16_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_147_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_16_self_attn_linear_q_weight_to_fp16_palettized, x = query_33_cast_fp16)[name = tensor<string, []>("linear_147_cast_fp16")];
            tensor<int32, [4]> var_2857 = const()[name = tensor<string, []>("op_2857"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_97_cast_fp16 = reshape(shape = var_2857, x = linear_147_cast_fp16)[name = tensor<string, []>("q_97_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_16_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(304967296))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(305753792))), name = tensor<string, []>("module_layers_16_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_148_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_16_self_attn_linear_k_weight_to_fp16_palettized, x = query_33_cast_fp16)[name = tensor<string, []>("linear_148_cast_fp16")];
            tensor<int32, [4]> var_2861 = const()[name = tensor<string, []>("op_2861"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_65_cast_fp16 = reshape(shape = var_2861, x = linear_148_cast_fp16)[name = tensor<string, []>("k_65_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_16_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(305753984))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(306540480))), name = tensor<string, []>("module_layers_16_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_149_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_16_self_attn_linear_v_weight_to_fp16_palettized, x = query_33_cast_fp16)[name = tensor<string, []>("linear_149_cast_fp16")];
            tensor<int32, [4]> var_2865 = const()[name = tensor<string, []>("op_2865"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_33_cast_fp16 = reshape(shape = var_2865, x = linear_149_cast_fp16)[name = tensor<string, []>("v_33_cast_fp16")];
            tensor<int32, [4]> value_35_perm_0 = const()[name = tensor<string, []>("value_35_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_16_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_16_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(306540672)))];
            tensor<fp16, [1, 188, 8, 128]> var_2877_cast_fp16 = add(x = q_97_cast_fp16, y = module_layers_16_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_2877_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_16_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_16_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(306542784)))];
            tensor<fp16, [1, 188, 8, 128]> var_2879_cast_fp16 = add(x = q_97_cast_fp16, y = module_layers_16_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_2879_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_33_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_33_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_359_transpose_x_0 = const()[name = tensor<string, []>("x_359_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_359_transpose_y_0 = const()[name = tensor<string, []>("x_359_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_2881_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(306544896))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(306832960))), name = tensor<string, []>("op_2881_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_33_cast_fp16 = transpose(perm = q_with_bias_v_33_perm_0, x = var_2879_cast_fp16)[name = tensor<string, []>("transpose_200")];
            tensor<fp16, [1, 8, 188, 375]> x_359_cast_fp16 = matmul(transpose_x = x_359_transpose_x_0, transpose_y = x_359_transpose_y_0, x = q_with_bias_v_33_cast_fp16, y = op_2881_to_fp16_palettized)[name = tensor<string, []>("x_359_cast_fp16")];
            tensor<int32, [8]> x_361_pad_0 = const()[name = tensor<string, []>("x_361_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_361_mode_0 = const()[name = tensor<string, []>("x_361_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_174_to_fp16 = const()[name = tensor<string, []>("const_174_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_361_cast_fp16 = pad(constant_val = const_174_to_fp16, mode = x_361_mode_0, pad = x_361_pad_0, x = x_359_cast_fp16)[name = tensor<string, []>("x_361_cast_fp16")];
            tensor<int32, [4]> var_2889 = const()[name = tensor<string, []>("op_2889"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_363_cast_fp16 = reshape(shape = var_2889, x = x_361_cast_fp16)[name = tensor<string, []>("x_363_cast_fp16")];
            tensor<int32, [4]> var_2893_begin_0 = const()[name = tensor<string, []>("op_2893_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_2893_end_0 = const()[name = tensor<string, []>("op_2893_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_2893_end_mask_0 = const()[name = tensor<string, []>("op_2893_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_2893_cast_fp16 = slice_by_index(begin = var_2893_begin_0, end = var_2893_end_0, end_mask = var_2893_end_mask_0, x = x_363_cast_fp16)[name = tensor<string, []>("op_2893_cast_fp16")];
            tensor<int32, [4]> var_2894 = const()[name = tensor<string, []>("op_2894"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_65_cast_fp16 = reshape(shape = var_2894, x = var_2893_cast_fp16)[name = tensor<string, []>("matrix_bd_65_cast_fp16")];
            tensor<bool, []> matrix_ac_33_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_33_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_33_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_33_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_128_perm_0 = const()[name = tensor<string, []>("transpose_128_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_129_perm_0 = const()[name = tensor<string, []>("transpose_129_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_129 = transpose(perm = transpose_129_perm_0, x = k_65_cast_fp16)[name = tensor<string, []>("transpose_198")];
            tensor<fp16, [1, 8, 188, 128]> transpose_128 = transpose(perm = transpose_128_perm_0, x = var_2877_cast_fp16)[name = tensor<string, []>("transpose_199")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_33_cast_fp16 = matmul(transpose_x = matrix_ac_33_transpose_x_0, transpose_y = matrix_ac_33_transpose_y_0, x = transpose_128, y = transpose_129)[name = tensor<string, []>("matrix_ac_33_cast_fp16")];
            tensor<int32, [4]> matrix_bd_67_begin_0 = const()[name = tensor<string, []>("matrix_bd_67_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_67_end_0 = const()[name = tensor<string, []>("matrix_bd_67_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_67_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_67_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_67_cast_fp16 = slice_by_index(begin = matrix_bd_67_begin_0, end = matrix_bd_67_end_0, end_mask = matrix_bd_67_end_mask_0, x = matrix_bd_65_cast_fp16)[name = tensor<string, []>("matrix_bd_67_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2903_cast_fp16 = add(x = matrix_ac_33_cast_fp16, y = matrix_bd_67_cast_fp16)[name = tensor<string, []>("op_2903_cast_fp16")];
            tensor<fp16, []> _inversed_scores_65_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_65_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_65_cast_fp16 = mul(x = var_2903_cast_fp16, y = _inversed_scores_65_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_65_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_67_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_65_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_67_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_2909_cast_fp16 = softmax(axis = var_30, x = scores_67_cast_fp16)[name = tensor<string, []>("op_2909_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_865_cast_fp16 = select(a = var_11_to_fp16, b = var_2909_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_865_cast_fp16")];
            tensor<bool, []> x_365_transpose_x_0 = const()[name = tensor<string, []>("x_365_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_365_transpose_y_0 = const()[name = tensor<string, []>("x_365_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_35_cast_fp16 = transpose(perm = value_35_perm_0, x = v_33_cast_fp16)[name = tensor<string, []>("transpose_197")];
            tensor<fp16, [1, 8, 188, 128]> x_365_cast_fp16 = matmul(transpose_x = x_365_transpose_x_0, transpose_y = x_365_transpose_y_0, x = input_865_cast_fp16, y = value_35_cast_fp16)[name = tensor<string, []>("x_365_cast_fp16")];
            tensor<int32, [4]> var_2913_perm_0 = const()[name = tensor<string, []>("op_2913_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_2914 = const()[name = tensor<string, []>("op_2914"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_2913_cast_fp16 = transpose(perm = var_2913_perm_0, x = x_365_cast_fp16)[name = tensor<string, []>("transpose_196")];
            tensor<fp16, [1, 188, 1024]> input_867_cast_fp16 = reshape(shape = var_2914, x = var_2913_cast_fp16)[name = tensor<string, []>("input_867_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_16_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(306833152))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(307619648))), name = tensor<string, []>("module_layers_16_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_151_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_16_self_attn_linear_out_weight_to_fp16_palettized, x = input_867_cast_fp16)[name = tensor<string, []>("linear_151_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_871_cast_fp16 = add(x = input_863_cast_fp16, y = linear_151_cast_fp16)[name = tensor<string, []>("input_871_cast_fp16")];
            tensor<int32, [1]> x_369_axes_0 = const()[name = tensor<string, []>("x_369_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_16_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_16_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(307619840)))];
            tensor<fp16, [1024]> module_layers_16_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_16_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(307621952)))];
            tensor<fp16, [1, 188, 1024]> x_369_cast_fp16 = layer_norm(axes = x_369_axes_0, beta = module_layers_16_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_16_norm_conv_weight_to_fp16, x = input_871_cast_fp16)[name = tensor<string, []>("x_369_cast_fp16")];
            tensor<int32, [3]> input_873_perm_0 = const()[name = tensor<string, []>("input_873_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_875_pad_type_0 = const()[name = tensor<string, []>("input_875_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_875_strides_0 = const()[name = tensor<string, []>("input_875_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_875_pad_0 = const()[name = tensor<string, []>("input_875_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_875_dilations_0 = const()[name = tensor<string, []>("input_875_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_875_groups_0 = const()[name = tensor<string, []>("input_875_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_16_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(307624064))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(309196992))), name = tensor<string, []>("module_layers_16_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_873_cast_fp16 = transpose(perm = input_873_perm_0, x = x_369_cast_fp16)[name = tensor<string, []>("transpose_195")];
            tensor<fp16, [1, 2048, 188]> input_875_cast_fp16 = conv(dilations = input_875_dilations_0, groups = input_875_groups_0, pad = input_875_pad_0, pad_type = input_875_pad_type_0, strides = input_875_strides_0, weight = module_layers_16_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_873_cast_fp16)[name = tensor<string, []>("input_875_cast_fp16")];
            tensor<int32, []> x_371_split_num_splits_0 = const()[name = tensor<string, []>("x_371_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_371_split_axis_0 = const()[name = tensor<string, []>("x_371_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_371_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_371_split_cast_fp16_1 = split(axis = x_371_split_axis_0, num_splits = x_371_split_num_splits_0, x = input_875_cast_fp16)[name = tensor<string, []>("x_371_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_371_split_1_sigmoid_cast_fp16 = sigmoid(x = x_371_split_cast_fp16_1)[name = tensor<string, []>("x_371_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_371_cast_fp16 = mul(x = x_371_split_cast_fp16_0, y = x_371_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_371_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_877_cast_fp16 = select(a = var_11_to_fp16, b = x_371_cast_fp16, cond = var_328)[name = tensor<string, []>("input_877_cast_fp16")];
            tensor<int32, [6]> input_879_pad_0 = const()[name = tensor<string, []>("input_879_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_879_mode_0 = const()[name = tensor<string, []>("input_879_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_177_to_fp16 = const()[name = tensor<string, []>("const_177_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_879_cast_fp16 = pad(constant_val = const_177_to_fp16, mode = input_879_mode_0, pad = input_879_pad_0, x = input_877_cast_fp16)[name = tensor<string, []>("input_879_cast_fp16")];
            tensor<string, []> input_881_pad_type_0 = const()[name = tensor<string, []>("input_881_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_881_groups_0 = const()[name = tensor<string, []>("input_881_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_881_strides_0 = const()[name = tensor<string, []>("input_881_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_881_pad_0 = const()[name = tensor<string, []>("input_881_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_881_dilations_0 = const()[name = tensor<string, []>("input_881_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_280_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(309197184))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(309204160))), name = tensor<string, []>("const_280_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_281_to_fp16 = const()[name = tensor<string, []>("const_281_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(309204352)))];
            tensor<fp16, [1, 1024, 188]> input_883_cast_fp16 = conv(bias = const_281_to_fp16, dilations = input_881_dilations_0, groups = input_881_groups_0, pad = input_881_pad_0, pad_type = input_881_pad_type_0, strides = input_881_strides_0, weight = const_280_to_fp16_palettized, x = input_879_cast_fp16)[name = tensor<string, []>("input_883_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_885_cast_fp16 = silu(x = input_883_cast_fp16)[name = tensor<string, []>("input_885_cast_fp16")];
            tensor<string, []> x_373_pad_type_0 = const()[name = tensor<string, []>("x_373_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_373_strides_0 = const()[name = tensor<string, []>("x_373_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_373_pad_0 = const()[name = tensor<string, []>("x_373_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_373_dilations_0 = const()[name = tensor<string, []>("x_373_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_373_groups_0 = const()[name = tensor<string, []>("x_373_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_16_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(309206464))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(309992960))), name = tensor<string, []>("module_layers_16_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_373_cast_fp16 = conv(dilations = x_373_dilations_0, groups = x_373_groups_0, pad = x_373_pad_0, pad_type = x_373_pad_type_0, strides = x_373_strides_0, weight = module_layers_16_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_885_cast_fp16)[name = tensor<string, []>("x_373_cast_fp16")];
            tensor<int32, [3]> input_887_perm_0 = const()[name = tensor<string, []>("input_887_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_887_cast_fp16 = transpose(perm = input_887_perm_0, x = x_373_cast_fp16)[name = tensor<string, []>("transpose_194")];
            tensor<fp16, [1, 188, 1024]> input_889_cast_fp16 = add(x = input_871_cast_fp16, y = input_887_cast_fp16)[name = tensor<string, []>("input_889_cast_fp16")];
            tensor<int32, [1]> input_891_axes_0 = const()[name = tensor<string, []>("input_891_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_16_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_16_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(309993152)))];
            tensor<fp16, [1024]> module_layers_16_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_16_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(309995264)))];
            tensor<fp16, [1, 188, 1024]> input_891_cast_fp16 = layer_norm(axes = input_891_axes_0, beta = module_layers_16_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_16_norm_feed_forward2_weight_to_fp16, x = input_889_cast_fp16)[name = tensor<string, []>("input_891_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_16_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(309997376))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(313143168))), name = tensor<string, []>("module_layers_16_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_152_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_16_feed_forward2_linear1_weight_to_fp16_palettized, x = input_891_cast_fp16)[name = tensor<string, []>("linear_152_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_895_cast_fp16 = silu(x = linear_152_cast_fp16)[name = tensor<string, []>("input_895_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_16_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(313143360))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(316289152))), name = tensor<string, []>("module_layers_16_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_153_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_16_feed_forward2_linear2_weight_to_fp16_palettized, x = input_895_cast_fp16)[name = tensor<string, []>("linear_153_cast_fp16")];
            tensor<fp16, []> var_2974_to_fp16 = const()[name = tensor<string, []>("op_2974_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_2975_cast_fp16 = mul(x = linear_153_cast_fp16, y = var_2974_to_fp16)[name = tensor<string, []>("op_2975_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_901_cast_fp16 = add(x = input_889_cast_fp16, y = var_2975_cast_fp16)[name = tensor<string, []>("input_901_cast_fp16")];
            tensor<int32, [1]> input_903_axes_0 = const()[name = tensor<string, []>("input_903_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_16_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_16_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(316289344)))];
            tensor<fp16, [1024]> module_layers_16_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_16_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(316291456)))];
            tensor<fp16, [1, 188, 1024]> input_903_cast_fp16 = layer_norm(axes = input_903_axes_0, beta = module_layers_16_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_16_norm_out_weight_to_fp16, x = input_901_cast_fp16)[name = tensor<string, []>("input_903_cast_fp16")];
            tensor<int32, [1]> input_905_axes_0 = const()[name = tensor<string, []>("input_905_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_17_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_17_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(316293568)))];
            tensor<fp16, [1024]> module_layers_17_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_17_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(316295680)))];
            tensor<fp16, [1, 188, 1024]> input_905_cast_fp16 = layer_norm(axes = input_905_axes_0, beta = module_layers_17_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_17_norm_feed_forward1_weight_to_fp16, x = input_903_cast_fp16)[name = tensor<string, []>("input_905_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_17_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(316297792))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(319443584))), name = tensor<string, []>("module_layers_17_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_154_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_17_feed_forward1_linear1_weight_to_fp16_palettized, x = input_905_cast_fp16)[name = tensor<string, []>("linear_154_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_909_cast_fp16 = silu(x = linear_154_cast_fp16)[name = tensor<string, []>("input_909_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_17_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(319443776))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(322589568))), name = tensor<string, []>("module_layers_17_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_155_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_17_feed_forward1_linear2_weight_to_fp16_palettized, x = input_909_cast_fp16)[name = tensor<string, []>("linear_155_cast_fp16")];
            tensor<fp16, []> var_3003_to_fp16 = const()[name = tensor<string, []>("op_3003_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3004_cast_fp16 = mul(x = linear_155_cast_fp16, y = var_3003_to_fp16)[name = tensor<string, []>("op_3004_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_915_cast_fp16 = add(x = input_903_cast_fp16, y = var_3004_cast_fp16)[name = tensor<string, []>("input_915_cast_fp16")];
            tensor<int32, [1]> query_35_axes_0 = const()[name = tensor<string, []>("query_35_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_17_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_17_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(322589760)))];
            tensor<fp16, [1024]> module_layers_17_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_17_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(322591872)))];
            tensor<fp16, [1, 188, 1024]> query_35_cast_fp16 = layer_norm(axes = query_35_axes_0, beta = module_layers_17_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_17_norm_self_att_weight_to_fp16, x = input_915_cast_fp16)[name = tensor<string, []>("query_35_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_17_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(322593984))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(323380480))), name = tensor<string, []>("module_layers_17_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_156_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_17_self_attn_linear_q_weight_to_fp16_palettized, x = query_35_cast_fp16)[name = tensor<string, []>("linear_156_cast_fp16")];
            tensor<int32, [4]> var_3020 = const()[name = tensor<string, []>("op_3020"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_103_cast_fp16 = reshape(shape = var_3020, x = linear_156_cast_fp16)[name = tensor<string, []>("q_103_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_17_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(323380672))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(324167168))), name = tensor<string, []>("module_layers_17_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_157_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_17_self_attn_linear_k_weight_to_fp16_palettized, x = query_35_cast_fp16)[name = tensor<string, []>("linear_157_cast_fp16")];
            tensor<int32, [4]> var_3024 = const()[name = tensor<string, []>("op_3024"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_69_cast_fp16 = reshape(shape = var_3024, x = linear_157_cast_fp16)[name = tensor<string, []>("k_69_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_17_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(324167360))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(324953856))), name = tensor<string, []>("module_layers_17_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_158_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_17_self_attn_linear_v_weight_to_fp16_palettized, x = query_35_cast_fp16)[name = tensor<string, []>("linear_158_cast_fp16")];
            tensor<int32, [4]> var_3028 = const()[name = tensor<string, []>("op_3028"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_35_cast_fp16 = reshape(shape = var_3028, x = linear_158_cast_fp16)[name = tensor<string, []>("v_35_cast_fp16")];
            tensor<int32, [4]> value_37_perm_0 = const()[name = tensor<string, []>("value_37_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_17_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_17_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(324954048)))];
            tensor<fp16, [1, 188, 8, 128]> var_3040_cast_fp16 = add(x = q_103_cast_fp16, y = module_layers_17_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_3040_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_17_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_17_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(324956160)))];
            tensor<fp16, [1, 188, 8, 128]> var_3042_cast_fp16 = add(x = q_103_cast_fp16, y = module_layers_17_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_3042_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_35_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_35_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_381_transpose_x_0 = const()[name = tensor<string, []>("x_381_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_381_transpose_y_0 = const()[name = tensor<string, []>("x_381_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_3044_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(324958272))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(325246336))), name = tensor<string, []>("op_3044_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_35_cast_fp16 = transpose(perm = q_with_bias_v_35_perm_0, x = var_3042_cast_fp16)[name = tensor<string, []>("transpose_193")];
            tensor<fp16, [1, 8, 188, 375]> x_381_cast_fp16 = matmul(transpose_x = x_381_transpose_x_0, transpose_y = x_381_transpose_y_0, x = q_with_bias_v_35_cast_fp16, y = op_3044_to_fp16_palettized)[name = tensor<string, []>("x_381_cast_fp16")];
            tensor<int32, [8]> x_383_pad_0 = const()[name = tensor<string, []>("x_383_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_383_mode_0 = const()[name = tensor<string, []>("x_383_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_184_to_fp16 = const()[name = tensor<string, []>("const_184_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_383_cast_fp16 = pad(constant_val = const_184_to_fp16, mode = x_383_mode_0, pad = x_383_pad_0, x = x_381_cast_fp16)[name = tensor<string, []>("x_383_cast_fp16")];
            tensor<int32, [4]> var_3052 = const()[name = tensor<string, []>("op_3052"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_385_cast_fp16 = reshape(shape = var_3052, x = x_383_cast_fp16)[name = tensor<string, []>("x_385_cast_fp16")];
            tensor<int32, [4]> var_3056_begin_0 = const()[name = tensor<string, []>("op_3056_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_3056_end_0 = const()[name = tensor<string, []>("op_3056_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_3056_end_mask_0 = const()[name = tensor<string, []>("op_3056_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_3056_cast_fp16 = slice_by_index(begin = var_3056_begin_0, end = var_3056_end_0, end_mask = var_3056_end_mask_0, x = x_385_cast_fp16)[name = tensor<string, []>("op_3056_cast_fp16")];
            tensor<int32, [4]> var_3057 = const()[name = tensor<string, []>("op_3057"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_69_cast_fp16 = reshape(shape = var_3057, x = var_3056_cast_fp16)[name = tensor<string, []>("matrix_bd_69_cast_fp16")];
            tensor<bool, []> matrix_ac_35_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_35_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_35_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_35_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_130_perm_0 = const()[name = tensor<string, []>("transpose_130_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_131_perm_0 = const()[name = tensor<string, []>("transpose_131_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_131 = transpose(perm = transpose_131_perm_0, x = k_69_cast_fp16)[name = tensor<string, []>("transpose_191")];
            tensor<fp16, [1, 8, 188, 128]> transpose_130 = transpose(perm = transpose_130_perm_0, x = var_3040_cast_fp16)[name = tensor<string, []>("transpose_192")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_35_cast_fp16 = matmul(transpose_x = matrix_ac_35_transpose_x_0, transpose_y = matrix_ac_35_transpose_y_0, x = transpose_130, y = transpose_131)[name = tensor<string, []>("matrix_ac_35_cast_fp16")];
            tensor<int32, [4]> matrix_bd_71_begin_0 = const()[name = tensor<string, []>("matrix_bd_71_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_71_end_0 = const()[name = tensor<string, []>("matrix_bd_71_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_71_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_71_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_71_cast_fp16 = slice_by_index(begin = matrix_bd_71_begin_0, end = matrix_bd_71_end_0, end_mask = matrix_bd_71_end_mask_0, x = matrix_bd_69_cast_fp16)[name = tensor<string, []>("matrix_bd_71_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3066_cast_fp16 = add(x = matrix_ac_35_cast_fp16, y = matrix_bd_71_cast_fp16)[name = tensor<string, []>("op_3066_cast_fp16")];
            tensor<fp16, []> _inversed_scores_69_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_69_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_69_cast_fp16 = mul(x = var_3066_cast_fp16, y = _inversed_scores_69_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_69_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_71_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_69_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_71_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3072_cast_fp16 = softmax(axis = var_30, x = scores_71_cast_fp16)[name = tensor<string, []>("op_3072_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_917_cast_fp16 = select(a = var_11_to_fp16, b = var_3072_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_917_cast_fp16")];
            tensor<bool, []> x_387_transpose_x_0 = const()[name = tensor<string, []>("x_387_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_387_transpose_y_0 = const()[name = tensor<string, []>("x_387_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_37_cast_fp16 = transpose(perm = value_37_perm_0, x = v_35_cast_fp16)[name = tensor<string, []>("transpose_190")];
            tensor<fp16, [1, 8, 188, 128]> x_387_cast_fp16 = matmul(transpose_x = x_387_transpose_x_0, transpose_y = x_387_transpose_y_0, x = input_917_cast_fp16, y = value_37_cast_fp16)[name = tensor<string, []>("x_387_cast_fp16")];
            tensor<int32, [4]> var_3076_perm_0 = const()[name = tensor<string, []>("op_3076_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_3077 = const()[name = tensor<string, []>("op_3077"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_3076_cast_fp16 = transpose(perm = var_3076_perm_0, x = x_387_cast_fp16)[name = tensor<string, []>("transpose_189")];
            tensor<fp16, [1, 188, 1024]> input_919_cast_fp16 = reshape(shape = var_3077, x = var_3076_cast_fp16)[name = tensor<string, []>("input_919_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_17_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(325246528))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(326033024))), name = tensor<string, []>("module_layers_17_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_160_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_17_self_attn_linear_out_weight_to_fp16_palettized, x = input_919_cast_fp16)[name = tensor<string, []>("linear_160_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_923_cast_fp16 = add(x = input_915_cast_fp16, y = linear_160_cast_fp16)[name = tensor<string, []>("input_923_cast_fp16")];
            tensor<int32, [1]> x_391_axes_0 = const()[name = tensor<string, []>("x_391_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_17_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_17_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(326033216)))];
            tensor<fp16, [1024]> module_layers_17_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_17_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(326035328)))];
            tensor<fp16, [1, 188, 1024]> x_391_cast_fp16 = layer_norm(axes = x_391_axes_0, beta = module_layers_17_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_17_norm_conv_weight_to_fp16, x = input_923_cast_fp16)[name = tensor<string, []>("x_391_cast_fp16")];
            tensor<int32, [3]> input_925_perm_0 = const()[name = tensor<string, []>("input_925_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_927_pad_type_0 = const()[name = tensor<string, []>("input_927_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_927_strides_0 = const()[name = tensor<string, []>("input_927_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_927_pad_0 = const()[name = tensor<string, []>("input_927_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_927_dilations_0 = const()[name = tensor<string, []>("input_927_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_927_groups_0 = const()[name = tensor<string, []>("input_927_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_17_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(326037440))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(327610368))), name = tensor<string, []>("module_layers_17_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_925_cast_fp16 = transpose(perm = input_925_perm_0, x = x_391_cast_fp16)[name = tensor<string, []>("transpose_188")];
            tensor<fp16, [1, 2048, 188]> input_927_cast_fp16 = conv(dilations = input_927_dilations_0, groups = input_927_groups_0, pad = input_927_pad_0, pad_type = input_927_pad_type_0, strides = input_927_strides_0, weight = module_layers_17_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_925_cast_fp16)[name = tensor<string, []>("input_927_cast_fp16")];
            tensor<int32, []> x_393_split_num_splits_0 = const()[name = tensor<string, []>("x_393_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_393_split_axis_0 = const()[name = tensor<string, []>("x_393_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_393_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_393_split_cast_fp16_1 = split(axis = x_393_split_axis_0, num_splits = x_393_split_num_splits_0, x = input_927_cast_fp16)[name = tensor<string, []>("x_393_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_393_split_1_sigmoid_cast_fp16 = sigmoid(x = x_393_split_cast_fp16_1)[name = tensor<string, []>("x_393_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_393_cast_fp16 = mul(x = x_393_split_cast_fp16_0, y = x_393_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_393_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_929_cast_fp16 = select(a = var_11_to_fp16, b = x_393_cast_fp16, cond = var_328)[name = tensor<string, []>("input_929_cast_fp16")];
            tensor<int32, [6]> input_931_pad_0 = const()[name = tensor<string, []>("input_931_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_931_mode_0 = const()[name = tensor<string, []>("input_931_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_187_to_fp16 = const()[name = tensor<string, []>("const_187_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_931_cast_fp16 = pad(constant_val = const_187_to_fp16, mode = input_931_mode_0, pad = input_931_pad_0, x = input_929_cast_fp16)[name = tensor<string, []>("input_931_cast_fp16")];
            tensor<string, []> input_933_pad_type_0 = const()[name = tensor<string, []>("input_933_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_933_groups_0 = const()[name = tensor<string, []>("input_933_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_933_strides_0 = const()[name = tensor<string, []>("input_933_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_933_pad_0 = const()[name = tensor<string, []>("input_933_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_933_dilations_0 = const()[name = tensor<string, []>("input_933_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_282_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(327610560))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(327617536))), name = tensor<string, []>("const_282_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_283_to_fp16 = const()[name = tensor<string, []>("const_283_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(327617728)))];
            tensor<fp16, [1, 1024, 188]> input_935_cast_fp16 = conv(bias = const_283_to_fp16, dilations = input_933_dilations_0, groups = input_933_groups_0, pad = input_933_pad_0, pad_type = input_933_pad_type_0, strides = input_933_strides_0, weight = const_282_to_fp16_palettized, x = input_931_cast_fp16)[name = tensor<string, []>("input_935_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_937_cast_fp16 = silu(x = input_935_cast_fp16)[name = tensor<string, []>("input_937_cast_fp16")];
            tensor<string, []> x_395_pad_type_0 = const()[name = tensor<string, []>("x_395_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_395_strides_0 = const()[name = tensor<string, []>("x_395_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_395_pad_0 = const()[name = tensor<string, []>("x_395_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_395_dilations_0 = const()[name = tensor<string, []>("x_395_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_395_groups_0 = const()[name = tensor<string, []>("x_395_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_17_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(327619840))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(328406336))), name = tensor<string, []>("module_layers_17_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_395_cast_fp16 = conv(dilations = x_395_dilations_0, groups = x_395_groups_0, pad = x_395_pad_0, pad_type = x_395_pad_type_0, strides = x_395_strides_0, weight = module_layers_17_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_937_cast_fp16)[name = tensor<string, []>("x_395_cast_fp16")];
            tensor<int32, [3]> input_939_perm_0 = const()[name = tensor<string, []>("input_939_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_939_cast_fp16 = transpose(perm = input_939_perm_0, x = x_395_cast_fp16)[name = tensor<string, []>("transpose_187")];
            tensor<fp16, [1, 188, 1024]> input_941_cast_fp16 = add(x = input_923_cast_fp16, y = input_939_cast_fp16)[name = tensor<string, []>("input_941_cast_fp16")];
            tensor<int32, [1]> input_943_axes_0 = const()[name = tensor<string, []>("input_943_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_17_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_17_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(328406528)))];
            tensor<fp16, [1024]> module_layers_17_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_17_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(328408640)))];
            tensor<fp16, [1, 188, 1024]> input_943_cast_fp16 = layer_norm(axes = input_943_axes_0, beta = module_layers_17_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_17_norm_feed_forward2_weight_to_fp16, x = input_941_cast_fp16)[name = tensor<string, []>("input_943_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_17_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(328410752))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(331556544))), name = tensor<string, []>("module_layers_17_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_161_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_17_feed_forward2_linear1_weight_to_fp16_palettized, x = input_943_cast_fp16)[name = tensor<string, []>("linear_161_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_947_cast_fp16 = silu(x = linear_161_cast_fp16)[name = tensor<string, []>("input_947_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_17_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(331556736))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(334702528))), name = tensor<string, []>("module_layers_17_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_162_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_17_feed_forward2_linear2_weight_to_fp16_palettized, x = input_947_cast_fp16)[name = tensor<string, []>("linear_162_cast_fp16")];
            tensor<fp16, []> var_3137_to_fp16 = const()[name = tensor<string, []>("op_3137_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3138_cast_fp16 = mul(x = linear_162_cast_fp16, y = var_3137_to_fp16)[name = tensor<string, []>("op_3138_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_953_cast_fp16 = add(x = input_941_cast_fp16, y = var_3138_cast_fp16)[name = tensor<string, []>("input_953_cast_fp16")];
            tensor<int32, [1]> input_955_axes_0 = const()[name = tensor<string, []>("input_955_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_17_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_17_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(334702720)))];
            tensor<fp16, [1024]> module_layers_17_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_17_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(334704832)))];
            tensor<fp16, [1, 188, 1024]> input_955_cast_fp16 = layer_norm(axes = input_955_axes_0, beta = module_layers_17_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_17_norm_out_weight_to_fp16, x = input_953_cast_fp16)[name = tensor<string, []>("input_955_cast_fp16")];
            tensor<int32, [1]> input_957_axes_0 = const()[name = tensor<string, []>("input_957_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_18_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_18_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(334706944)))];
            tensor<fp16, [1024]> module_layers_18_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_18_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(334709056)))];
            tensor<fp16, [1, 188, 1024]> input_957_cast_fp16 = layer_norm(axes = input_957_axes_0, beta = module_layers_18_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_18_norm_feed_forward1_weight_to_fp16, x = input_955_cast_fp16)[name = tensor<string, []>("input_957_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_18_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(334711168))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(337856960))), name = tensor<string, []>("module_layers_18_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_163_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_18_feed_forward1_linear1_weight_to_fp16_palettized, x = input_957_cast_fp16)[name = tensor<string, []>("linear_163_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_961_cast_fp16 = silu(x = linear_163_cast_fp16)[name = tensor<string, []>("input_961_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_18_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(337857152))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(341002944))), name = tensor<string, []>("module_layers_18_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_164_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_18_feed_forward1_linear2_weight_to_fp16_palettized, x = input_961_cast_fp16)[name = tensor<string, []>("linear_164_cast_fp16")];
            tensor<fp16, []> var_3166_to_fp16 = const()[name = tensor<string, []>("op_3166_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3167_cast_fp16 = mul(x = linear_164_cast_fp16, y = var_3166_to_fp16)[name = tensor<string, []>("op_3167_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_967_cast_fp16 = add(x = input_955_cast_fp16, y = var_3167_cast_fp16)[name = tensor<string, []>("input_967_cast_fp16")];
            tensor<int32, [1]> query_37_axes_0 = const()[name = tensor<string, []>("query_37_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_18_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_18_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(341003136)))];
            tensor<fp16, [1024]> module_layers_18_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_18_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(341005248)))];
            tensor<fp16, [1, 188, 1024]> query_37_cast_fp16 = layer_norm(axes = query_37_axes_0, beta = module_layers_18_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_18_norm_self_att_weight_to_fp16, x = input_967_cast_fp16)[name = tensor<string, []>("query_37_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_18_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(341007360))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(341793856))), name = tensor<string, []>("module_layers_18_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_165_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_18_self_attn_linear_q_weight_to_fp16_palettized, x = query_37_cast_fp16)[name = tensor<string, []>("linear_165_cast_fp16")];
            tensor<int32, [4]> var_3183 = const()[name = tensor<string, []>("op_3183"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_109_cast_fp16 = reshape(shape = var_3183, x = linear_165_cast_fp16)[name = tensor<string, []>("q_109_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_18_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(341794048))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(342580544))), name = tensor<string, []>("module_layers_18_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_166_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_18_self_attn_linear_k_weight_to_fp16_palettized, x = query_37_cast_fp16)[name = tensor<string, []>("linear_166_cast_fp16")];
            tensor<int32, [4]> var_3187 = const()[name = tensor<string, []>("op_3187"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_73_cast_fp16 = reshape(shape = var_3187, x = linear_166_cast_fp16)[name = tensor<string, []>("k_73_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_18_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(342580736))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(343367232))), name = tensor<string, []>("module_layers_18_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_167_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_18_self_attn_linear_v_weight_to_fp16_palettized, x = query_37_cast_fp16)[name = tensor<string, []>("linear_167_cast_fp16")];
            tensor<int32, [4]> var_3191 = const()[name = tensor<string, []>("op_3191"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_37_cast_fp16 = reshape(shape = var_3191, x = linear_167_cast_fp16)[name = tensor<string, []>("v_37_cast_fp16")];
            tensor<int32, [4]> value_39_perm_0 = const()[name = tensor<string, []>("value_39_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_18_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_18_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(343367424)))];
            tensor<fp16, [1, 188, 8, 128]> var_3203_cast_fp16 = add(x = q_109_cast_fp16, y = module_layers_18_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_3203_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_18_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_18_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(343369536)))];
            tensor<fp16, [1, 188, 8, 128]> var_3205_cast_fp16 = add(x = q_109_cast_fp16, y = module_layers_18_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_3205_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_37_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_37_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_403_transpose_x_0 = const()[name = tensor<string, []>("x_403_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_403_transpose_y_0 = const()[name = tensor<string, []>("x_403_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_3207_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(343371648))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(343659712))), name = tensor<string, []>("op_3207_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_37_cast_fp16 = transpose(perm = q_with_bias_v_37_perm_0, x = var_3205_cast_fp16)[name = tensor<string, []>("transpose_186")];
            tensor<fp16, [1, 8, 188, 375]> x_403_cast_fp16 = matmul(transpose_x = x_403_transpose_x_0, transpose_y = x_403_transpose_y_0, x = q_with_bias_v_37_cast_fp16, y = op_3207_to_fp16_palettized)[name = tensor<string, []>("x_403_cast_fp16")];
            tensor<int32, [8]> x_405_pad_0 = const()[name = tensor<string, []>("x_405_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_405_mode_0 = const()[name = tensor<string, []>("x_405_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_194_to_fp16 = const()[name = tensor<string, []>("const_194_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_405_cast_fp16 = pad(constant_val = const_194_to_fp16, mode = x_405_mode_0, pad = x_405_pad_0, x = x_403_cast_fp16)[name = tensor<string, []>("x_405_cast_fp16")];
            tensor<int32, [4]> var_3215 = const()[name = tensor<string, []>("op_3215"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_407_cast_fp16 = reshape(shape = var_3215, x = x_405_cast_fp16)[name = tensor<string, []>("x_407_cast_fp16")];
            tensor<int32, [4]> var_3219_begin_0 = const()[name = tensor<string, []>("op_3219_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_3219_end_0 = const()[name = tensor<string, []>("op_3219_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_3219_end_mask_0 = const()[name = tensor<string, []>("op_3219_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_3219_cast_fp16 = slice_by_index(begin = var_3219_begin_0, end = var_3219_end_0, end_mask = var_3219_end_mask_0, x = x_407_cast_fp16)[name = tensor<string, []>("op_3219_cast_fp16")];
            tensor<int32, [4]> var_3220 = const()[name = tensor<string, []>("op_3220"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_73_cast_fp16 = reshape(shape = var_3220, x = var_3219_cast_fp16)[name = tensor<string, []>("matrix_bd_73_cast_fp16")];
            tensor<bool, []> matrix_ac_37_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_37_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_37_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_37_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_132_perm_0 = const()[name = tensor<string, []>("transpose_132_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_133_perm_0 = const()[name = tensor<string, []>("transpose_133_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_133 = transpose(perm = transpose_133_perm_0, x = k_73_cast_fp16)[name = tensor<string, []>("transpose_184")];
            tensor<fp16, [1, 8, 188, 128]> transpose_132 = transpose(perm = transpose_132_perm_0, x = var_3203_cast_fp16)[name = tensor<string, []>("transpose_185")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_37_cast_fp16 = matmul(transpose_x = matrix_ac_37_transpose_x_0, transpose_y = matrix_ac_37_transpose_y_0, x = transpose_132, y = transpose_133)[name = tensor<string, []>("matrix_ac_37_cast_fp16")];
            tensor<int32, [4]> matrix_bd_75_begin_0 = const()[name = tensor<string, []>("matrix_bd_75_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_75_end_0 = const()[name = tensor<string, []>("matrix_bd_75_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_75_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_75_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_75_cast_fp16 = slice_by_index(begin = matrix_bd_75_begin_0, end = matrix_bd_75_end_0, end_mask = matrix_bd_75_end_mask_0, x = matrix_bd_73_cast_fp16)[name = tensor<string, []>("matrix_bd_75_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3229_cast_fp16 = add(x = matrix_ac_37_cast_fp16, y = matrix_bd_75_cast_fp16)[name = tensor<string, []>("op_3229_cast_fp16")];
            tensor<fp16, []> _inversed_scores_73_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_73_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_73_cast_fp16 = mul(x = var_3229_cast_fp16, y = _inversed_scores_73_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_73_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_75_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_73_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_75_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3235_cast_fp16 = softmax(axis = var_30, x = scores_75_cast_fp16)[name = tensor<string, []>("op_3235_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_969_cast_fp16 = select(a = var_11_to_fp16, b = var_3235_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_969_cast_fp16")];
            tensor<bool, []> x_409_transpose_x_0 = const()[name = tensor<string, []>("x_409_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_409_transpose_y_0 = const()[name = tensor<string, []>("x_409_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_39_cast_fp16 = transpose(perm = value_39_perm_0, x = v_37_cast_fp16)[name = tensor<string, []>("transpose_183")];
            tensor<fp16, [1, 8, 188, 128]> x_409_cast_fp16 = matmul(transpose_x = x_409_transpose_x_0, transpose_y = x_409_transpose_y_0, x = input_969_cast_fp16, y = value_39_cast_fp16)[name = tensor<string, []>("x_409_cast_fp16")];
            tensor<int32, [4]> var_3239_perm_0 = const()[name = tensor<string, []>("op_3239_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_3240 = const()[name = tensor<string, []>("op_3240"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_3239_cast_fp16 = transpose(perm = var_3239_perm_0, x = x_409_cast_fp16)[name = tensor<string, []>("transpose_182")];
            tensor<fp16, [1, 188, 1024]> input_971_cast_fp16 = reshape(shape = var_3240, x = var_3239_cast_fp16)[name = tensor<string, []>("input_971_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_18_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(343659904))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(344446400))), name = tensor<string, []>("module_layers_18_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_169_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_18_self_attn_linear_out_weight_to_fp16_palettized, x = input_971_cast_fp16)[name = tensor<string, []>("linear_169_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_975_cast_fp16 = add(x = input_967_cast_fp16, y = linear_169_cast_fp16)[name = tensor<string, []>("input_975_cast_fp16")];
            tensor<int32, [1]> x_413_axes_0 = const()[name = tensor<string, []>("x_413_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_18_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_18_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(344446592)))];
            tensor<fp16, [1024]> module_layers_18_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_18_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(344448704)))];
            tensor<fp16, [1, 188, 1024]> x_413_cast_fp16 = layer_norm(axes = x_413_axes_0, beta = module_layers_18_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_18_norm_conv_weight_to_fp16, x = input_975_cast_fp16)[name = tensor<string, []>("x_413_cast_fp16")];
            tensor<int32, [3]> input_977_perm_0 = const()[name = tensor<string, []>("input_977_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_979_pad_type_0 = const()[name = tensor<string, []>("input_979_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_979_strides_0 = const()[name = tensor<string, []>("input_979_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_979_pad_0 = const()[name = tensor<string, []>("input_979_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_979_dilations_0 = const()[name = tensor<string, []>("input_979_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_979_groups_0 = const()[name = tensor<string, []>("input_979_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_18_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(344450816))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(346023744))), name = tensor<string, []>("module_layers_18_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_977_cast_fp16 = transpose(perm = input_977_perm_0, x = x_413_cast_fp16)[name = tensor<string, []>("transpose_181")];
            tensor<fp16, [1, 2048, 188]> input_979_cast_fp16 = conv(dilations = input_979_dilations_0, groups = input_979_groups_0, pad = input_979_pad_0, pad_type = input_979_pad_type_0, strides = input_979_strides_0, weight = module_layers_18_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_977_cast_fp16)[name = tensor<string, []>("input_979_cast_fp16")];
            tensor<int32, []> x_415_split_num_splits_0 = const()[name = tensor<string, []>("x_415_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_415_split_axis_0 = const()[name = tensor<string, []>("x_415_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_415_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_415_split_cast_fp16_1 = split(axis = x_415_split_axis_0, num_splits = x_415_split_num_splits_0, x = input_979_cast_fp16)[name = tensor<string, []>("x_415_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_415_split_1_sigmoid_cast_fp16 = sigmoid(x = x_415_split_cast_fp16_1)[name = tensor<string, []>("x_415_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_415_cast_fp16 = mul(x = x_415_split_cast_fp16_0, y = x_415_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_415_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_981_cast_fp16 = select(a = var_11_to_fp16, b = x_415_cast_fp16, cond = var_328)[name = tensor<string, []>("input_981_cast_fp16")];
            tensor<int32, [6]> input_983_pad_0 = const()[name = tensor<string, []>("input_983_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_983_mode_0 = const()[name = tensor<string, []>("input_983_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_197_to_fp16 = const()[name = tensor<string, []>("const_197_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_983_cast_fp16 = pad(constant_val = const_197_to_fp16, mode = input_983_mode_0, pad = input_983_pad_0, x = input_981_cast_fp16)[name = tensor<string, []>("input_983_cast_fp16")];
            tensor<string, []> input_985_pad_type_0 = const()[name = tensor<string, []>("input_985_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_985_groups_0 = const()[name = tensor<string, []>("input_985_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_985_strides_0 = const()[name = tensor<string, []>("input_985_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_985_pad_0 = const()[name = tensor<string, []>("input_985_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_985_dilations_0 = const()[name = tensor<string, []>("input_985_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_284_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(346023936))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(346030912))), name = tensor<string, []>("const_284_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_285_to_fp16 = const()[name = tensor<string, []>("const_285_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(346031104)))];
            tensor<fp16, [1, 1024, 188]> input_987_cast_fp16 = conv(bias = const_285_to_fp16, dilations = input_985_dilations_0, groups = input_985_groups_0, pad = input_985_pad_0, pad_type = input_985_pad_type_0, strides = input_985_strides_0, weight = const_284_to_fp16_palettized, x = input_983_cast_fp16)[name = tensor<string, []>("input_987_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_989_cast_fp16 = silu(x = input_987_cast_fp16)[name = tensor<string, []>("input_989_cast_fp16")];
            tensor<string, []> x_417_pad_type_0 = const()[name = tensor<string, []>("x_417_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_417_strides_0 = const()[name = tensor<string, []>("x_417_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_417_pad_0 = const()[name = tensor<string, []>("x_417_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_417_dilations_0 = const()[name = tensor<string, []>("x_417_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_417_groups_0 = const()[name = tensor<string, []>("x_417_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_18_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(346033216))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(346819712))), name = tensor<string, []>("module_layers_18_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_417_cast_fp16 = conv(dilations = x_417_dilations_0, groups = x_417_groups_0, pad = x_417_pad_0, pad_type = x_417_pad_type_0, strides = x_417_strides_0, weight = module_layers_18_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_989_cast_fp16)[name = tensor<string, []>("x_417_cast_fp16")];
            tensor<int32, [3]> input_991_perm_0 = const()[name = tensor<string, []>("input_991_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_991_cast_fp16 = transpose(perm = input_991_perm_0, x = x_417_cast_fp16)[name = tensor<string, []>("transpose_180")];
            tensor<fp16, [1, 188, 1024]> input_993_cast_fp16 = add(x = input_975_cast_fp16, y = input_991_cast_fp16)[name = tensor<string, []>("input_993_cast_fp16")];
            tensor<int32, [1]> input_995_axes_0 = const()[name = tensor<string, []>("input_995_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_18_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_18_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(346819904)))];
            tensor<fp16, [1024]> module_layers_18_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_18_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(346822016)))];
            tensor<fp16, [1, 188, 1024]> input_995_cast_fp16 = layer_norm(axes = input_995_axes_0, beta = module_layers_18_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_18_norm_feed_forward2_weight_to_fp16, x = input_993_cast_fp16)[name = tensor<string, []>("input_995_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_18_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(346824128))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(349969920))), name = tensor<string, []>("module_layers_18_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_170_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_18_feed_forward2_linear1_weight_to_fp16_palettized, x = input_995_cast_fp16)[name = tensor<string, []>("linear_170_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_999_cast_fp16 = silu(x = linear_170_cast_fp16)[name = tensor<string, []>("input_999_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_18_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(349970112))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(353115904))), name = tensor<string, []>("module_layers_18_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_171_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_18_feed_forward2_linear2_weight_to_fp16_palettized, x = input_999_cast_fp16)[name = tensor<string, []>("linear_171_cast_fp16")];
            tensor<fp16, []> var_3300_to_fp16 = const()[name = tensor<string, []>("op_3300_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3301_cast_fp16 = mul(x = linear_171_cast_fp16, y = var_3300_to_fp16)[name = tensor<string, []>("op_3301_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1005_cast_fp16 = add(x = input_993_cast_fp16, y = var_3301_cast_fp16)[name = tensor<string, []>("input_1005_cast_fp16")];
            tensor<int32, [1]> input_1007_axes_0 = const()[name = tensor<string, []>("input_1007_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_18_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_18_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(353116096)))];
            tensor<fp16, [1024]> module_layers_18_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_18_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(353118208)))];
            tensor<fp16, [1, 188, 1024]> input_1007_cast_fp16 = layer_norm(axes = input_1007_axes_0, beta = module_layers_18_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_18_norm_out_weight_to_fp16, x = input_1005_cast_fp16)[name = tensor<string, []>("input_1007_cast_fp16")];
            tensor<int32, [1]> input_1009_axes_0 = const()[name = tensor<string, []>("input_1009_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_19_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_19_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(353120320)))];
            tensor<fp16, [1024]> module_layers_19_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_19_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(353122432)))];
            tensor<fp16, [1, 188, 1024]> input_1009_cast_fp16 = layer_norm(axes = input_1009_axes_0, beta = module_layers_19_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_19_norm_feed_forward1_weight_to_fp16, x = input_1007_cast_fp16)[name = tensor<string, []>("input_1009_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_19_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(353124544))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(356270336))), name = tensor<string, []>("module_layers_19_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_172_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_19_feed_forward1_linear1_weight_to_fp16_palettized, x = input_1009_cast_fp16)[name = tensor<string, []>("linear_172_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_1013_cast_fp16 = silu(x = linear_172_cast_fp16)[name = tensor<string, []>("input_1013_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_19_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(356270528))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(359416320))), name = tensor<string, []>("module_layers_19_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_173_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_19_feed_forward1_linear2_weight_to_fp16_palettized, x = input_1013_cast_fp16)[name = tensor<string, []>("linear_173_cast_fp16")];
            tensor<fp16, []> var_3329_to_fp16 = const()[name = tensor<string, []>("op_3329_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3330_cast_fp16 = mul(x = linear_173_cast_fp16, y = var_3329_to_fp16)[name = tensor<string, []>("op_3330_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1019_cast_fp16 = add(x = input_1007_cast_fp16, y = var_3330_cast_fp16)[name = tensor<string, []>("input_1019_cast_fp16")];
            tensor<int32, [1]> query_39_axes_0 = const()[name = tensor<string, []>("query_39_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_19_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_19_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(359416512)))];
            tensor<fp16, [1024]> module_layers_19_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_19_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(359418624)))];
            tensor<fp16, [1, 188, 1024]> query_39_cast_fp16 = layer_norm(axes = query_39_axes_0, beta = module_layers_19_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_19_norm_self_att_weight_to_fp16, x = input_1019_cast_fp16)[name = tensor<string, []>("query_39_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_19_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(359420736))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(360207232))), name = tensor<string, []>("module_layers_19_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_174_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_19_self_attn_linear_q_weight_to_fp16_palettized, x = query_39_cast_fp16)[name = tensor<string, []>("linear_174_cast_fp16")];
            tensor<int32, [4]> var_3346 = const()[name = tensor<string, []>("op_3346"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_115_cast_fp16 = reshape(shape = var_3346, x = linear_174_cast_fp16)[name = tensor<string, []>("q_115_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_19_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(360207424))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(360993920))), name = tensor<string, []>("module_layers_19_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_175_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_19_self_attn_linear_k_weight_to_fp16_palettized, x = query_39_cast_fp16)[name = tensor<string, []>("linear_175_cast_fp16")];
            tensor<int32, [4]> var_3350 = const()[name = tensor<string, []>("op_3350"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_77_cast_fp16 = reshape(shape = var_3350, x = linear_175_cast_fp16)[name = tensor<string, []>("k_77_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_19_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(360994112))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(361780608))), name = tensor<string, []>("module_layers_19_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_176_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_19_self_attn_linear_v_weight_to_fp16_palettized, x = query_39_cast_fp16)[name = tensor<string, []>("linear_176_cast_fp16")];
            tensor<int32, [4]> var_3354 = const()[name = tensor<string, []>("op_3354"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_39_cast_fp16 = reshape(shape = var_3354, x = linear_176_cast_fp16)[name = tensor<string, []>("v_39_cast_fp16")];
            tensor<int32, [4]> value_41_perm_0 = const()[name = tensor<string, []>("value_41_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_19_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_19_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(361780800)))];
            tensor<fp16, [1, 188, 8, 128]> var_3366_cast_fp16 = add(x = q_115_cast_fp16, y = module_layers_19_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_3366_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_19_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_19_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(361782912)))];
            tensor<fp16, [1, 188, 8, 128]> var_3368_cast_fp16 = add(x = q_115_cast_fp16, y = module_layers_19_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_3368_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_39_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_39_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_425_transpose_x_0 = const()[name = tensor<string, []>("x_425_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_425_transpose_y_0 = const()[name = tensor<string, []>("x_425_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_3370_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(361785024))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(362073088))), name = tensor<string, []>("op_3370_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_39_cast_fp16 = transpose(perm = q_with_bias_v_39_perm_0, x = var_3368_cast_fp16)[name = tensor<string, []>("transpose_179")];
            tensor<fp16, [1, 8, 188, 375]> x_425_cast_fp16 = matmul(transpose_x = x_425_transpose_x_0, transpose_y = x_425_transpose_y_0, x = q_with_bias_v_39_cast_fp16, y = op_3370_to_fp16_palettized)[name = tensor<string, []>("x_425_cast_fp16")];
            tensor<int32, [8]> x_427_pad_0 = const()[name = tensor<string, []>("x_427_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_427_mode_0 = const()[name = tensor<string, []>("x_427_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_204_to_fp16 = const()[name = tensor<string, []>("const_204_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_427_cast_fp16 = pad(constant_val = const_204_to_fp16, mode = x_427_mode_0, pad = x_427_pad_0, x = x_425_cast_fp16)[name = tensor<string, []>("x_427_cast_fp16")];
            tensor<int32, [4]> var_3378 = const()[name = tensor<string, []>("op_3378"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_429_cast_fp16 = reshape(shape = var_3378, x = x_427_cast_fp16)[name = tensor<string, []>("x_429_cast_fp16")];
            tensor<int32, [4]> var_3382_begin_0 = const()[name = tensor<string, []>("op_3382_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_3382_end_0 = const()[name = tensor<string, []>("op_3382_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_3382_end_mask_0 = const()[name = tensor<string, []>("op_3382_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_3382_cast_fp16 = slice_by_index(begin = var_3382_begin_0, end = var_3382_end_0, end_mask = var_3382_end_mask_0, x = x_429_cast_fp16)[name = tensor<string, []>("op_3382_cast_fp16")];
            tensor<int32, [4]> var_3383 = const()[name = tensor<string, []>("op_3383"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_77_cast_fp16 = reshape(shape = var_3383, x = var_3382_cast_fp16)[name = tensor<string, []>("matrix_bd_77_cast_fp16")];
            tensor<bool, []> matrix_ac_39_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_39_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_39_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_39_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_134_perm_0 = const()[name = tensor<string, []>("transpose_134_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_135_perm_0 = const()[name = tensor<string, []>("transpose_135_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_135 = transpose(perm = transpose_135_perm_0, x = k_77_cast_fp16)[name = tensor<string, []>("transpose_177")];
            tensor<fp16, [1, 8, 188, 128]> transpose_134 = transpose(perm = transpose_134_perm_0, x = var_3366_cast_fp16)[name = tensor<string, []>("transpose_178")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_39_cast_fp16 = matmul(transpose_x = matrix_ac_39_transpose_x_0, transpose_y = matrix_ac_39_transpose_y_0, x = transpose_134, y = transpose_135)[name = tensor<string, []>("matrix_ac_39_cast_fp16")];
            tensor<int32, [4]> matrix_bd_79_begin_0 = const()[name = tensor<string, []>("matrix_bd_79_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_79_end_0 = const()[name = tensor<string, []>("matrix_bd_79_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_79_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_79_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_79_cast_fp16 = slice_by_index(begin = matrix_bd_79_begin_0, end = matrix_bd_79_end_0, end_mask = matrix_bd_79_end_mask_0, x = matrix_bd_77_cast_fp16)[name = tensor<string, []>("matrix_bd_79_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3392_cast_fp16 = add(x = matrix_ac_39_cast_fp16, y = matrix_bd_79_cast_fp16)[name = tensor<string, []>("op_3392_cast_fp16")];
            tensor<fp16, []> _inversed_scores_77_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_77_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_77_cast_fp16 = mul(x = var_3392_cast_fp16, y = _inversed_scores_77_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_77_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_79_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_77_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_79_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3398_cast_fp16 = softmax(axis = var_30, x = scores_79_cast_fp16)[name = tensor<string, []>("op_3398_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_1021_cast_fp16 = select(a = var_11_to_fp16, b = var_3398_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_1021_cast_fp16")];
            tensor<bool, []> x_431_transpose_x_0 = const()[name = tensor<string, []>("x_431_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_431_transpose_y_0 = const()[name = tensor<string, []>("x_431_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_41_cast_fp16 = transpose(perm = value_41_perm_0, x = v_39_cast_fp16)[name = tensor<string, []>("transpose_176")];
            tensor<fp16, [1, 8, 188, 128]> x_431_cast_fp16 = matmul(transpose_x = x_431_transpose_x_0, transpose_y = x_431_transpose_y_0, x = input_1021_cast_fp16, y = value_41_cast_fp16)[name = tensor<string, []>("x_431_cast_fp16")];
            tensor<int32, [4]> var_3402_perm_0 = const()[name = tensor<string, []>("op_3402_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_3403 = const()[name = tensor<string, []>("op_3403"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_3402_cast_fp16 = transpose(perm = var_3402_perm_0, x = x_431_cast_fp16)[name = tensor<string, []>("transpose_175")];
            tensor<fp16, [1, 188, 1024]> input_1023_cast_fp16 = reshape(shape = var_3403, x = var_3402_cast_fp16)[name = tensor<string, []>("input_1023_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_19_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(362073280))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(362859776))), name = tensor<string, []>("module_layers_19_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_178_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_19_self_attn_linear_out_weight_to_fp16_palettized, x = input_1023_cast_fp16)[name = tensor<string, []>("linear_178_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1027_cast_fp16 = add(x = input_1019_cast_fp16, y = linear_178_cast_fp16)[name = tensor<string, []>("input_1027_cast_fp16")];
            tensor<int32, [1]> x_435_axes_0 = const()[name = tensor<string, []>("x_435_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_19_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_19_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(362859968)))];
            tensor<fp16, [1024]> module_layers_19_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_19_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(362862080)))];
            tensor<fp16, [1, 188, 1024]> x_435_cast_fp16 = layer_norm(axes = x_435_axes_0, beta = module_layers_19_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_19_norm_conv_weight_to_fp16, x = input_1027_cast_fp16)[name = tensor<string, []>("x_435_cast_fp16")];
            tensor<int32, [3]> input_1029_perm_0 = const()[name = tensor<string, []>("input_1029_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_1031_pad_type_0 = const()[name = tensor<string, []>("input_1031_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_1031_strides_0 = const()[name = tensor<string, []>("input_1031_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_1031_pad_0 = const()[name = tensor<string, []>("input_1031_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_1031_dilations_0 = const()[name = tensor<string, []>("input_1031_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_1031_groups_0 = const()[name = tensor<string, []>("input_1031_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_19_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(362864192))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(364437120))), name = tensor<string, []>("module_layers_19_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_1029_cast_fp16 = transpose(perm = input_1029_perm_0, x = x_435_cast_fp16)[name = tensor<string, []>("transpose_174")];
            tensor<fp16, [1, 2048, 188]> input_1031_cast_fp16 = conv(dilations = input_1031_dilations_0, groups = input_1031_groups_0, pad = input_1031_pad_0, pad_type = input_1031_pad_type_0, strides = input_1031_strides_0, weight = module_layers_19_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_1029_cast_fp16)[name = tensor<string, []>("input_1031_cast_fp16")];
            tensor<int32, []> x_437_split_num_splits_0 = const()[name = tensor<string, []>("x_437_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_437_split_axis_0 = const()[name = tensor<string, []>("x_437_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_437_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_437_split_cast_fp16_1 = split(axis = x_437_split_axis_0, num_splits = x_437_split_num_splits_0, x = input_1031_cast_fp16)[name = tensor<string, []>("x_437_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_437_split_1_sigmoid_cast_fp16 = sigmoid(x = x_437_split_cast_fp16_1)[name = tensor<string, []>("x_437_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_437_cast_fp16 = mul(x = x_437_split_cast_fp16_0, y = x_437_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_437_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_1033_cast_fp16 = select(a = var_11_to_fp16, b = x_437_cast_fp16, cond = var_328)[name = tensor<string, []>("input_1033_cast_fp16")];
            tensor<int32, [6]> input_1035_pad_0 = const()[name = tensor<string, []>("input_1035_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_1035_mode_0 = const()[name = tensor<string, []>("input_1035_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_207_to_fp16 = const()[name = tensor<string, []>("const_207_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_1035_cast_fp16 = pad(constant_val = const_207_to_fp16, mode = input_1035_mode_0, pad = input_1035_pad_0, x = input_1033_cast_fp16)[name = tensor<string, []>("input_1035_cast_fp16")];
            tensor<string, []> input_1037_pad_type_0 = const()[name = tensor<string, []>("input_1037_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_1037_groups_0 = const()[name = tensor<string, []>("input_1037_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_1037_strides_0 = const()[name = tensor<string, []>("input_1037_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_1037_pad_0 = const()[name = tensor<string, []>("input_1037_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_1037_dilations_0 = const()[name = tensor<string, []>("input_1037_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_286_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(364437312))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(364444288))), name = tensor<string, []>("const_286_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_287_to_fp16 = const()[name = tensor<string, []>("const_287_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(364444480)))];
            tensor<fp16, [1, 1024, 188]> input_1039_cast_fp16 = conv(bias = const_287_to_fp16, dilations = input_1037_dilations_0, groups = input_1037_groups_0, pad = input_1037_pad_0, pad_type = input_1037_pad_type_0, strides = input_1037_strides_0, weight = const_286_to_fp16_palettized, x = input_1035_cast_fp16)[name = tensor<string, []>("input_1039_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_1041_cast_fp16 = silu(x = input_1039_cast_fp16)[name = tensor<string, []>("input_1041_cast_fp16")];
            tensor<string, []> x_439_pad_type_0 = const()[name = tensor<string, []>("x_439_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_439_strides_0 = const()[name = tensor<string, []>("x_439_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_439_pad_0 = const()[name = tensor<string, []>("x_439_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_439_dilations_0 = const()[name = tensor<string, []>("x_439_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_439_groups_0 = const()[name = tensor<string, []>("x_439_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_19_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(364446592))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(365233088))), name = tensor<string, []>("module_layers_19_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_439_cast_fp16 = conv(dilations = x_439_dilations_0, groups = x_439_groups_0, pad = x_439_pad_0, pad_type = x_439_pad_type_0, strides = x_439_strides_0, weight = module_layers_19_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_1041_cast_fp16)[name = tensor<string, []>("x_439_cast_fp16")];
            tensor<int32, [3]> input_1043_perm_0 = const()[name = tensor<string, []>("input_1043_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_1043_cast_fp16 = transpose(perm = input_1043_perm_0, x = x_439_cast_fp16)[name = tensor<string, []>("transpose_173")];
            tensor<fp16, [1, 188, 1024]> input_1045_cast_fp16 = add(x = input_1027_cast_fp16, y = input_1043_cast_fp16)[name = tensor<string, []>("input_1045_cast_fp16")];
            tensor<int32, [1]> input_1047_axes_0 = const()[name = tensor<string, []>("input_1047_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_19_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_19_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(365233280)))];
            tensor<fp16, [1024]> module_layers_19_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_19_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(365235392)))];
            tensor<fp16, [1, 188, 1024]> input_1047_cast_fp16 = layer_norm(axes = input_1047_axes_0, beta = module_layers_19_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_19_norm_feed_forward2_weight_to_fp16, x = input_1045_cast_fp16)[name = tensor<string, []>("input_1047_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_19_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(365237504))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(368383296))), name = tensor<string, []>("module_layers_19_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_179_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_19_feed_forward2_linear1_weight_to_fp16_palettized, x = input_1047_cast_fp16)[name = tensor<string, []>("linear_179_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_1051_cast_fp16 = silu(x = linear_179_cast_fp16)[name = tensor<string, []>("input_1051_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_19_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(368383488))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(371529280))), name = tensor<string, []>("module_layers_19_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_180_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_19_feed_forward2_linear2_weight_to_fp16_palettized, x = input_1051_cast_fp16)[name = tensor<string, []>("linear_180_cast_fp16")];
            tensor<fp16, []> var_3463_to_fp16 = const()[name = tensor<string, []>("op_3463_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3464_cast_fp16 = mul(x = linear_180_cast_fp16, y = var_3463_to_fp16)[name = tensor<string, []>("op_3464_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1057_cast_fp16 = add(x = input_1045_cast_fp16, y = var_3464_cast_fp16)[name = tensor<string, []>("input_1057_cast_fp16")];
            tensor<int32, [1]> input_1059_axes_0 = const()[name = tensor<string, []>("input_1059_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_19_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_19_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(371529472)))];
            tensor<fp16, [1024]> module_layers_19_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_19_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(371531584)))];
            tensor<fp16, [1, 188, 1024]> input_1059_cast_fp16 = layer_norm(axes = input_1059_axes_0, beta = module_layers_19_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_19_norm_out_weight_to_fp16, x = input_1057_cast_fp16)[name = tensor<string, []>("input_1059_cast_fp16")];
            tensor<int32, [1]> input_1061_axes_0 = const()[name = tensor<string, []>("input_1061_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_20_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_20_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(371533696)))];
            tensor<fp16, [1024]> module_layers_20_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_20_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(371535808)))];
            tensor<fp16, [1, 188, 1024]> input_1061_cast_fp16 = layer_norm(axes = input_1061_axes_0, beta = module_layers_20_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_20_norm_feed_forward1_weight_to_fp16, x = input_1059_cast_fp16)[name = tensor<string, []>("input_1061_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_20_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(371537920))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(374683712))), name = tensor<string, []>("module_layers_20_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_181_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_20_feed_forward1_linear1_weight_to_fp16_palettized, x = input_1061_cast_fp16)[name = tensor<string, []>("linear_181_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_1065_cast_fp16 = silu(x = linear_181_cast_fp16)[name = tensor<string, []>("input_1065_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_20_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(374683904))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(377829696))), name = tensor<string, []>("module_layers_20_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_182_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_20_feed_forward1_linear2_weight_to_fp16_palettized, x = input_1065_cast_fp16)[name = tensor<string, []>("linear_182_cast_fp16")];
            tensor<fp16, []> var_3492_to_fp16 = const()[name = tensor<string, []>("op_3492_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3493_cast_fp16 = mul(x = linear_182_cast_fp16, y = var_3492_to_fp16)[name = tensor<string, []>("op_3493_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1071_cast_fp16 = add(x = input_1059_cast_fp16, y = var_3493_cast_fp16)[name = tensor<string, []>("input_1071_cast_fp16")];
            tensor<int32, [1]> query_41_axes_0 = const()[name = tensor<string, []>("query_41_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_20_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_20_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(377829888)))];
            tensor<fp16, [1024]> module_layers_20_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_20_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(377832000)))];
            tensor<fp16, [1, 188, 1024]> query_41_cast_fp16 = layer_norm(axes = query_41_axes_0, beta = module_layers_20_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_20_norm_self_att_weight_to_fp16, x = input_1071_cast_fp16)[name = tensor<string, []>("query_41_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_20_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(377834112))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(378620608))), name = tensor<string, []>("module_layers_20_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_183_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_20_self_attn_linear_q_weight_to_fp16_palettized, x = query_41_cast_fp16)[name = tensor<string, []>("linear_183_cast_fp16")];
            tensor<int32, [4]> var_3509 = const()[name = tensor<string, []>("op_3509"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_121_cast_fp16 = reshape(shape = var_3509, x = linear_183_cast_fp16)[name = tensor<string, []>("q_121_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_20_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(378620800))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(379407296))), name = tensor<string, []>("module_layers_20_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_184_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_20_self_attn_linear_k_weight_to_fp16_palettized, x = query_41_cast_fp16)[name = tensor<string, []>("linear_184_cast_fp16")];
            tensor<int32, [4]> var_3513 = const()[name = tensor<string, []>("op_3513"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_81_cast_fp16 = reshape(shape = var_3513, x = linear_184_cast_fp16)[name = tensor<string, []>("k_81_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_20_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(379407488))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(380193984))), name = tensor<string, []>("module_layers_20_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_185_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_20_self_attn_linear_v_weight_to_fp16_palettized, x = query_41_cast_fp16)[name = tensor<string, []>("linear_185_cast_fp16")];
            tensor<int32, [4]> var_3517 = const()[name = tensor<string, []>("op_3517"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_41_cast_fp16 = reshape(shape = var_3517, x = linear_185_cast_fp16)[name = tensor<string, []>("v_41_cast_fp16")];
            tensor<int32, [4]> value_43_perm_0 = const()[name = tensor<string, []>("value_43_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_20_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_20_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(380194176)))];
            tensor<fp16, [1, 188, 8, 128]> var_3529_cast_fp16 = add(x = q_121_cast_fp16, y = module_layers_20_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_3529_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_20_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_20_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(380196288)))];
            tensor<fp16, [1, 188, 8, 128]> var_3531_cast_fp16 = add(x = q_121_cast_fp16, y = module_layers_20_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_3531_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_41_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_41_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_447_transpose_x_0 = const()[name = tensor<string, []>("x_447_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_447_transpose_y_0 = const()[name = tensor<string, []>("x_447_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_3533_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(380198400))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(380486464))), name = tensor<string, []>("op_3533_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_41_cast_fp16 = transpose(perm = q_with_bias_v_41_perm_0, x = var_3531_cast_fp16)[name = tensor<string, []>("transpose_172")];
            tensor<fp16, [1, 8, 188, 375]> x_447_cast_fp16 = matmul(transpose_x = x_447_transpose_x_0, transpose_y = x_447_transpose_y_0, x = q_with_bias_v_41_cast_fp16, y = op_3533_to_fp16_palettized)[name = tensor<string, []>("x_447_cast_fp16")];
            tensor<int32, [8]> x_449_pad_0 = const()[name = tensor<string, []>("x_449_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_449_mode_0 = const()[name = tensor<string, []>("x_449_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_214_to_fp16 = const()[name = tensor<string, []>("const_214_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_449_cast_fp16 = pad(constant_val = const_214_to_fp16, mode = x_449_mode_0, pad = x_449_pad_0, x = x_447_cast_fp16)[name = tensor<string, []>("x_449_cast_fp16")];
            tensor<int32, [4]> var_3541 = const()[name = tensor<string, []>("op_3541"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_451_cast_fp16 = reshape(shape = var_3541, x = x_449_cast_fp16)[name = tensor<string, []>("x_451_cast_fp16")];
            tensor<int32, [4]> var_3545_begin_0 = const()[name = tensor<string, []>("op_3545_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_3545_end_0 = const()[name = tensor<string, []>("op_3545_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_3545_end_mask_0 = const()[name = tensor<string, []>("op_3545_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_3545_cast_fp16 = slice_by_index(begin = var_3545_begin_0, end = var_3545_end_0, end_mask = var_3545_end_mask_0, x = x_451_cast_fp16)[name = tensor<string, []>("op_3545_cast_fp16")];
            tensor<int32, [4]> var_3546 = const()[name = tensor<string, []>("op_3546"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_81_cast_fp16 = reshape(shape = var_3546, x = var_3545_cast_fp16)[name = tensor<string, []>("matrix_bd_81_cast_fp16")];
            tensor<bool, []> matrix_ac_41_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_41_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_41_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_41_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_136_perm_0 = const()[name = tensor<string, []>("transpose_136_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_137_perm_0 = const()[name = tensor<string, []>("transpose_137_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_137 = transpose(perm = transpose_137_perm_0, x = k_81_cast_fp16)[name = tensor<string, []>("transpose_170")];
            tensor<fp16, [1, 8, 188, 128]> transpose_136 = transpose(perm = transpose_136_perm_0, x = var_3529_cast_fp16)[name = tensor<string, []>("transpose_171")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_41_cast_fp16 = matmul(transpose_x = matrix_ac_41_transpose_x_0, transpose_y = matrix_ac_41_transpose_y_0, x = transpose_136, y = transpose_137)[name = tensor<string, []>("matrix_ac_41_cast_fp16")];
            tensor<int32, [4]> matrix_bd_83_begin_0 = const()[name = tensor<string, []>("matrix_bd_83_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_83_end_0 = const()[name = tensor<string, []>("matrix_bd_83_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_83_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_83_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_83_cast_fp16 = slice_by_index(begin = matrix_bd_83_begin_0, end = matrix_bd_83_end_0, end_mask = matrix_bd_83_end_mask_0, x = matrix_bd_81_cast_fp16)[name = tensor<string, []>("matrix_bd_83_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3555_cast_fp16 = add(x = matrix_ac_41_cast_fp16, y = matrix_bd_83_cast_fp16)[name = tensor<string, []>("op_3555_cast_fp16")];
            tensor<fp16, []> _inversed_scores_81_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_81_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_81_cast_fp16 = mul(x = var_3555_cast_fp16, y = _inversed_scores_81_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_81_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_83_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_81_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_83_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3561_cast_fp16 = softmax(axis = var_30, x = scores_83_cast_fp16)[name = tensor<string, []>("op_3561_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_1073_cast_fp16 = select(a = var_11_to_fp16, b = var_3561_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_1073_cast_fp16")];
            tensor<bool, []> x_453_transpose_x_0 = const()[name = tensor<string, []>("x_453_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_453_transpose_y_0 = const()[name = tensor<string, []>("x_453_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_43_cast_fp16 = transpose(perm = value_43_perm_0, x = v_41_cast_fp16)[name = tensor<string, []>("transpose_169")];
            tensor<fp16, [1, 8, 188, 128]> x_453_cast_fp16 = matmul(transpose_x = x_453_transpose_x_0, transpose_y = x_453_transpose_y_0, x = input_1073_cast_fp16, y = value_43_cast_fp16)[name = tensor<string, []>("x_453_cast_fp16")];
            tensor<int32, [4]> var_3565_perm_0 = const()[name = tensor<string, []>("op_3565_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_3566 = const()[name = tensor<string, []>("op_3566"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_3565_cast_fp16 = transpose(perm = var_3565_perm_0, x = x_453_cast_fp16)[name = tensor<string, []>("transpose_168")];
            tensor<fp16, [1, 188, 1024]> input_1075_cast_fp16 = reshape(shape = var_3566, x = var_3565_cast_fp16)[name = tensor<string, []>("input_1075_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_20_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(380486656))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(381273152))), name = tensor<string, []>("module_layers_20_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_187_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_20_self_attn_linear_out_weight_to_fp16_palettized, x = input_1075_cast_fp16)[name = tensor<string, []>("linear_187_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1079_cast_fp16 = add(x = input_1071_cast_fp16, y = linear_187_cast_fp16)[name = tensor<string, []>("input_1079_cast_fp16")];
            tensor<int32, [1]> x_457_axes_0 = const()[name = tensor<string, []>("x_457_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_20_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_20_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(381273344)))];
            tensor<fp16, [1024]> module_layers_20_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_20_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(381275456)))];
            tensor<fp16, [1, 188, 1024]> x_457_cast_fp16 = layer_norm(axes = x_457_axes_0, beta = module_layers_20_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_20_norm_conv_weight_to_fp16, x = input_1079_cast_fp16)[name = tensor<string, []>("x_457_cast_fp16")];
            tensor<int32, [3]> input_1081_perm_0 = const()[name = tensor<string, []>("input_1081_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_1083_pad_type_0 = const()[name = tensor<string, []>("input_1083_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_1083_strides_0 = const()[name = tensor<string, []>("input_1083_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_1083_pad_0 = const()[name = tensor<string, []>("input_1083_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_1083_dilations_0 = const()[name = tensor<string, []>("input_1083_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_1083_groups_0 = const()[name = tensor<string, []>("input_1083_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_20_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(381277568))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(382850496))), name = tensor<string, []>("module_layers_20_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_1081_cast_fp16 = transpose(perm = input_1081_perm_0, x = x_457_cast_fp16)[name = tensor<string, []>("transpose_167")];
            tensor<fp16, [1, 2048, 188]> input_1083_cast_fp16 = conv(dilations = input_1083_dilations_0, groups = input_1083_groups_0, pad = input_1083_pad_0, pad_type = input_1083_pad_type_0, strides = input_1083_strides_0, weight = module_layers_20_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_1081_cast_fp16)[name = tensor<string, []>("input_1083_cast_fp16")];
            tensor<int32, []> x_459_split_num_splits_0 = const()[name = tensor<string, []>("x_459_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_459_split_axis_0 = const()[name = tensor<string, []>("x_459_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_459_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_459_split_cast_fp16_1 = split(axis = x_459_split_axis_0, num_splits = x_459_split_num_splits_0, x = input_1083_cast_fp16)[name = tensor<string, []>("x_459_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_459_split_1_sigmoid_cast_fp16 = sigmoid(x = x_459_split_cast_fp16_1)[name = tensor<string, []>("x_459_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_459_cast_fp16 = mul(x = x_459_split_cast_fp16_0, y = x_459_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_459_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_1085_cast_fp16 = select(a = var_11_to_fp16, b = x_459_cast_fp16, cond = var_328)[name = tensor<string, []>("input_1085_cast_fp16")];
            tensor<int32, [6]> input_1087_pad_0 = const()[name = tensor<string, []>("input_1087_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_1087_mode_0 = const()[name = tensor<string, []>("input_1087_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_217_to_fp16 = const()[name = tensor<string, []>("const_217_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_1087_cast_fp16 = pad(constant_val = const_217_to_fp16, mode = input_1087_mode_0, pad = input_1087_pad_0, x = input_1085_cast_fp16)[name = tensor<string, []>("input_1087_cast_fp16")];
            tensor<string, []> input_1089_pad_type_0 = const()[name = tensor<string, []>("input_1089_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_1089_groups_0 = const()[name = tensor<string, []>("input_1089_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_1089_strides_0 = const()[name = tensor<string, []>("input_1089_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_1089_pad_0 = const()[name = tensor<string, []>("input_1089_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_1089_dilations_0 = const()[name = tensor<string, []>("input_1089_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_288_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(382850688))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(382857664))), name = tensor<string, []>("const_288_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_289_to_fp16 = const()[name = tensor<string, []>("const_289_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(382857856)))];
            tensor<fp16, [1, 1024, 188]> input_1091_cast_fp16 = conv(bias = const_289_to_fp16, dilations = input_1089_dilations_0, groups = input_1089_groups_0, pad = input_1089_pad_0, pad_type = input_1089_pad_type_0, strides = input_1089_strides_0, weight = const_288_to_fp16_palettized, x = input_1087_cast_fp16)[name = tensor<string, []>("input_1091_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_1093_cast_fp16 = silu(x = input_1091_cast_fp16)[name = tensor<string, []>("input_1093_cast_fp16")];
            tensor<string, []> x_461_pad_type_0 = const()[name = tensor<string, []>("x_461_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_461_strides_0 = const()[name = tensor<string, []>("x_461_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_461_pad_0 = const()[name = tensor<string, []>("x_461_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_461_dilations_0 = const()[name = tensor<string, []>("x_461_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_461_groups_0 = const()[name = tensor<string, []>("x_461_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_20_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(382859968))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(383646464))), name = tensor<string, []>("module_layers_20_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_461_cast_fp16 = conv(dilations = x_461_dilations_0, groups = x_461_groups_0, pad = x_461_pad_0, pad_type = x_461_pad_type_0, strides = x_461_strides_0, weight = module_layers_20_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_1093_cast_fp16)[name = tensor<string, []>("x_461_cast_fp16")];
            tensor<int32, [3]> input_1095_perm_0 = const()[name = tensor<string, []>("input_1095_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_1095_cast_fp16 = transpose(perm = input_1095_perm_0, x = x_461_cast_fp16)[name = tensor<string, []>("transpose_166")];
            tensor<fp16, [1, 188, 1024]> input_1097_cast_fp16 = add(x = input_1079_cast_fp16, y = input_1095_cast_fp16)[name = tensor<string, []>("input_1097_cast_fp16")];
            tensor<int32, [1]> input_1099_axes_0 = const()[name = tensor<string, []>("input_1099_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_20_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_20_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(383646656)))];
            tensor<fp16, [1024]> module_layers_20_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_20_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(383648768)))];
            tensor<fp16, [1, 188, 1024]> input_1099_cast_fp16 = layer_norm(axes = input_1099_axes_0, beta = module_layers_20_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_20_norm_feed_forward2_weight_to_fp16, x = input_1097_cast_fp16)[name = tensor<string, []>("input_1099_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_20_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(383650880))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(386796672))), name = tensor<string, []>("module_layers_20_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_188_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_20_feed_forward2_linear1_weight_to_fp16_palettized, x = input_1099_cast_fp16)[name = tensor<string, []>("linear_188_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_1103_cast_fp16 = silu(x = linear_188_cast_fp16)[name = tensor<string, []>("input_1103_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_20_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(386796864))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(389942656))), name = tensor<string, []>("module_layers_20_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_189_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_20_feed_forward2_linear2_weight_to_fp16_palettized, x = input_1103_cast_fp16)[name = tensor<string, []>("linear_189_cast_fp16")];
            tensor<fp16, []> var_3626_to_fp16 = const()[name = tensor<string, []>("op_3626_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3627_cast_fp16 = mul(x = linear_189_cast_fp16, y = var_3626_to_fp16)[name = tensor<string, []>("op_3627_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1109_cast_fp16 = add(x = input_1097_cast_fp16, y = var_3627_cast_fp16)[name = tensor<string, []>("input_1109_cast_fp16")];
            tensor<int32, [1]> input_1111_axes_0 = const()[name = tensor<string, []>("input_1111_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_20_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_20_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(389942848)))];
            tensor<fp16, [1024]> module_layers_20_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_20_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(389944960)))];
            tensor<fp16, [1, 188, 1024]> input_1111_cast_fp16 = layer_norm(axes = input_1111_axes_0, beta = module_layers_20_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_20_norm_out_weight_to_fp16, x = input_1109_cast_fp16)[name = tensor<string, []>("input_1111_cast_fp16")];
            tensor<int32, [1]> input_1113_axes_0 = const()[name = tensor<string, []>("input_1113_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_21_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_21_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(389947072)))];
            tensor<fp16, [1024]> module_layers_21_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_21_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(389949184)))];
            tensor<fp16, [1, 188, 1024]> input_1113_cast_fp16 = layer_norm(axes = input_1113_axes_0, beta = module_layers_21_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_21_norm_feed_forward1_weight_to_fp16, x = input_1111_cast_fp16)[name = tensor<string, []>("input_1113_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_21_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(389951296))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(393097088))), name = tensor<string, []>("module_layers_21_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_190_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_21_feed_forward1_linear1_weight_to_fp16_palettized, x = input_1113_cast_fp16)[name = tensor<string, []>("linear_190_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_1117_cast_fp16 = silu(x = linear_190_cast_fp16)[name = tensor<string, []>("input_1117_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_21_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(393097280))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(396243072))), name = tensor<string, []>("module_layers_21_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_191_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_21_feed_forward1_linear2_weight_to_fp16_palettized, x = input_1117_cast_fp16)[name = tensor<string, []>("linear_191_cast_fp16")];
            tensor<fp16, []> var_3655_to_fp16 = const()[name = tensor<string, []>("op_3655_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3656_cast_fp16 = mul(x = linear_191_cast_fp16, y = var_3655_to_fp16)[name = tensor<string, []>("op_3656_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1123_cast_fp16 = add(x = input_1111_cast_fp16, y = var_3656_cast_fp16)[name = tensor<string, []>("input_1123_cast_fp16")];
            tensor<int32, [1]> query_43_axes_0 = const()[name = tensor<string, []>("query_43_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_21_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_21_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(396243264)))];
            tensor<fp16, [1024]> module_layers_21_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_21_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(396245376)))];
            tensor<fp16, [1, 188, 1024]> query_43_cast_fp16 = layer_norm(axes = query_43_axes_0, beta = module_layers_21_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_21_norm_self_att_weight_to_fp16, x = input_1123_cast_fp16)[name = tensor<string, []>("query_43_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_21_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(396247488))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(397033984))), name = tensor<string, []>("module_layers_21_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_192_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_21_self_attn_linear_q_weight_to_fp16_palettized, x = query_43_cast_fp16)[name = tensor<string, []>("linear_192_cast_fp16")];
            tensor<int32, [4]> var_3672 = const()[name = tensor<string, []>("op_3672"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_127_cast_fp16 = reshape(shape = var_3672, x = linear_192_cast_fp16)[name = tensor<string, []>("q_127_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_21_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(397034176))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(397820672))), name = tensor<string, []>("module_layers_21_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_193_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_21_self_attn_linear_k_weight_to_fp16_palettized, x = query_43_cast_fp16)[name = tensor<string, []>("linear_193_cast_fp16")];
            tensor<int32, [4]> var_3676 = const()[name = tensor<string, []>("op_3676"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_85_cast_fp16 = reshape(shape = var_3676, x = linear_193_cast_fp16)[name = tensor<string, []>("k_85_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_21_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(397820864))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(398607360))), name = tensor<string, []>("module_layers_21_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_194_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_21_self_attn_linear_v_weight_to_fp16_palettized, x = query_43_cast_fp16)[name = tensor<string, []>("linear_194_cast_fp16")];
            tensor<int32, [4]> var_3680 = const()[name = tensor<string, []>("op_3680"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_43_cast_fp16 = reshape(shape = var_3680, x = linear_194_cast_fp16)[name = tensor<string, []>("v_43_cast_fp16")];
            tensor<int32, [4]> value_45_perm_0 = const()[name = tensor<string, []>("value_45_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_21_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_21_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(398607552)))];
            tensor<fp16, [1, 188, 8, 128]> var_3692_cast_fp16 = add(x = q_127_cast_fp16, y = module_layers_21_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_3692_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_21_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_21_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(398609664)))];
            tensor<fp16, [1, 188, 8, 128]> var_3694_cast_fp16 = add(x = q_127_cast_fp16, y = module_layers_21_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_3694_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_43_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_43_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_469_transpose_x_0 = const()[name = tensor<string, []>("x_469_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_469_transpose_y_0 = const()[name = tensor<string, []>("x_469_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_3696_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(398611776))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(398899840))), name = tensor<string, []>("op_3696_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_43_cast_fp16 = transpose(perm = q_with_bias_v_43_perm_0, x = var_3694_cast_fp16)[name = tensor<string, []>("transpose_165")];
            tensor<fp16, [1, 8, 188, 375]> x_469_cast_fp16 = matmul(transpose_x = x_469_transpose_x_0, transpose_y = x_469_transpose_y_0, x = q_with_bias_v_43_cast_fp16, y = op_3696_to_fp16_palettized)[name = tensor<string, []>("x_469_cast_fp16")];
            tensor<int32, [8]> x_471_pad_0 = const()[name = tensor<string, []>("x_471_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_471_mode_0 = const()[name = tensor<string, []>("x_471_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_224_to_fp16 = const()[name = tensor<string, []>("const_224_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_471_cast_fp16 = pad(constant_val = const_224_to_fp16, mode = x_471_mode_0, pad = x_471_pad_0, x = x_469_cast_fp16)[name = tensor<string, []>("x_471_cast_fp16")];
            tensor<int32, [4]> var_3704 = const()[name = tensor<string, []>("op_3704"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_473_cast_fp16 = reshape(shape = var_3704, x = x_471_cast_fp16)[name = tensor<string, []>("x_473_cast_fp16")];
            tensor<int32, [4]> var_3708_begin_0 = const()[name = tensor<string, []>("op_3708_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_3708_end_0 = const()[name = tensor<string, []>("op_3708_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_3708_end_mask_0 = const()[name = tensor<string, []>("op_3708_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_3708_cast_fp16 = slice_by_index(begin = var_3708_begin_0, end = var_3708_end_0, end_mask = var_3708_end_mask_0, x = x_473_cast_fp16)[name = tensor<string, []>("op_3708_cast_fp16")];
            tensor<int32, [4]> var_3709 = const()[name = tensor<string, []>("op_3709"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_85_cast_fp16 = reshape(shape = var_3709, x = var_3708_cast_fp16)[name = tensor<string, []>("matrix_bd_85_cast_fp16")];
            tensor<bool, []> matrix_ac_43_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_43_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_43_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_43_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_138_perm_0 = const()[name = tensor<string, []>("transpose_138_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_139_perm_0 = const()[name = tensor<string, []>("transpose_139_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_139 = transpose(perm = transpose_139_perm_0, x = k_85_cast_fp16)[name = tensor<string, []>("transpose_163")];
            tensor<fp16, [1, 8, 188, 128]> transpose_138 = transpose(perm = transpose_138_perm_0, x = var_3692_cast_fp16)[name = tensor<string, []>("transpose_164")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_43_cast_fp16 = matmul(transpose_x = matrix_ac_43_transpose_x_0, transpose_y = matrix_ac_43_transpose_y_0, x = transpose_138, y = transpose_139)[name = tensor<string, []>("matrix_ac_43_cast_fp16")];
            tensor<int32, [4]> matrix_bd_87_begin_0 = const()[name = tensor<string, []>("matrix_bd_87_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_87_end_0 = const()[name = tensor<string, []>("matrix_bd_87_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_87_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_87_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_87_cast_fp16 = slice_by_index(begin = matrix_bd_87_begin_0, end = matrix_bd_87_end_0, end_mask = matrix_bd_87_end_mask_0, x = matrix_bd_85_cast_fp16)[name = tensor<string, []>("matrix_bd_87_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3718_cast_fp16 = add(x = matrix_ac_43_cast_fp16, y = matrix_bd_87_cast_fp16)[name = tensor<string, []>("op_3718_cast_fp16")];
            tensor<fp16, []> _inversed_scores_85_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_85_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_85_cast_fp16 = mul(x = var_3718_cast_fp16, y = _inversed_scores_85_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_85_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_87_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_85_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_87_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3724_cast_fp16 = softmax(axis = var_30, x = scores_87_cast_fp16)[name = tensor<string, []>("op_3724_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_1125_cast_fp16 = select(a = var_11_to_fp16, b = var_3724_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_1125_cast_fp16")];
            tensor<bool, []> x_475_transpose_x_0 = const()[name = tensor<string, []>("x_475_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_475_transpose_y_0 = const()[name = tensor<string, []>("x_475_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_45_cast_fp16 = transpose(perm = value_45_perm_0, x = v_43_cast_fp16)[name = tensor<string, []>("transpose_162")];
            tensor<fp16, [1, 8, 188, 128]> x_475_cast_fp16 = matmul(transpose_x = x_475_transpose_x_0, transpose_y = x_475_transpose_y_0, x = input_1125_cast_fp16, y = value_45_cast_fp16)[name = tensor<string, []>("x_475_cast_fp16")];
            tensor<int32, [4]> var_3728_perm_0 = const()[name = tensor<string, []>("op_3728_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_3729 = const()[name = tensor<string, []>("op_3729"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_3728_cast_fp16 = transpose(perm = var_3728_perm_0, x = x_475_cast_fp16)[name = tensor<string, []>("transpose_161")];
            tensor<fp16, [1, 188, 1024]> input_1127_cast_fp16 = reshape(shape = var_3729, x = var_3728_cast_fp16)[name = tensor<string, []>("input_1127_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_21_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(398900032))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(399686528))), name = tensor<string, []>("module_layers_21_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_196_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_21_self_attn_linear_out_weight_to_fp16_palettized, x = input_1127_cast_fp16)[name = tensor<string, []>("linear_196_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1131_cast_fp16 = add(x = input_1123_cast_fp16, y = linear_196_cast_fp16)[name = tensor<string, []>("input_1131_cast_fp16")];
            tensor<int32, [1]> x_479_axes_0 = const()[name = tensor<string, []>("x_479_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_21_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_21_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(399686720)))];
            tensor<fp16, [1024]> module_layers_21_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_21_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(399688832)))];
            tensor<fp16, [1, 188, 1024]> x_479_cast_fp16 = layer_norm(axes = x_479_axes_0, beta = module_layers_21_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_21_norm_conv_weight_to_fp16, x = input_1131_cast_fp16)[name = tensor<string, []>("x_479_cast_fp16")];
            tensor<int32, [3]> input_1133_perm_0 = const()[name = tensor<string, []>("input_1133_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_1135_pad_type_0 = const()[name = tensor<string, []>("input_1135_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_1135_strides_0 = const()[name = tensor<string, []>("input_1135_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_1135_pad_0 = const()[name = tensor<string, []>("input_1135_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_1135_dilations_0 = const()[name = tensor<string, []>("input_1135_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_1135_groups_0 = const()[name = tensor<string, []>("input_1135_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_21_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(399690944))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(401263872))), name = tensor<string, []>("module_layers_21_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_1133_cast_fp16 = transpose(perm = input_1133_perm_0, x = x_479_cast_fp16)[name = tensor<string, []>("transpose_160")];
            tensor<fp16, [1, 2048, 188]> input_1135_cast_fp16 = conv(dilations = input_1135_dilations_0, groups = input_1135_groups_0, pad = input_1135_pad_0, pad_type = input_1135_pad_type_0, strides = input_1135_strides_0, weight = module_layers_21_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_1133_cast_fp16)[name = tensor<string, []>("input_1135_cast_fp16")];
            tensor<int32, []> x_481_split_num_splits_0 = const()[name = tensor<string, []>("x_481_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_481_split_axis_0 = const()[name = tensor<string, []>("x_481_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_481_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_481_split_cast_fp16_1 = split(axis = x_481_split_axis_0, num_splits = x_481_split_num_splits_0, x = input_1135_cast_fp16)[name = tensor<string, []>("x_481_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_481_split_1_sigmoid_cast_fp16 = sigmoid(x = x_481_split_cast_fp16_1)[name = tensor<string, []>("x_481_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_481_cast_fp16 = mul(x = x_481_split_cast_fp16_0, y = x_481_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_481_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_1137_cast_fp16 = select(a = var_11_to_fp16, b = x_481_cast_fp16, cond = var_328)[name = tensor<string, []>("input_1137_cast_fp16")];
            tensor<int32, [6]> input_1139_pad_0 = const()[name = tensor<string, []>("input_1139_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_1139_mode_0 = const()[name = tensor<string, []>("input_1139_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_227_to_fp16 = const()[name = tensor<string, []>("const_227_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_1139_cast_fp16 = pad(constant_val = const_227_to_fp16, mode = input_1139_mode_0, pad = input_1139_pad_0, x = input_1137_cast_fp16)[name = tensor<string, []>("input_1139_cast_fp16")];
            tensor<string, []> input_1141_pad_type_0 = const()[name = tensor<string, []>("input_1141_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_1141_groups_0 = const()[name = tensor<string, []>("input_1141_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_1141_strides_0 = const()[name = tensor<string, []>("input_1141_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_1141_pad_0 = const()[name = tensor<string, []>("input_1141_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_1141_dilations_0 = const()[name = tensor<string, []>("input_1141_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_290_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(401264064))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(401271040))), name = tensor<string, []>("const_290_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_291_to_fp16 = const()[name = tensor<string, []>("const_291_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(401271232)))];
            tensor<fp16, [1, 1024, 188]> input_1143_cast_fp16 = conv(bias = const_291_to_fp16, dilations = input_1141_dilations_0, groups = input_1141_groups_0, pad = input_1141_pad_0, pad_type = input_1141_pad_type_0, strides = input_1141_strides_0, weight = const_290_to_fp16_palettized, x = input_1139_cast_fp16)[name = tensor<string, []>("input_1143_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_1145_cast_fp16 = silu(x = input_1143_cast_fp16)[name = tensor<string, []>("input_1145_cast_fp16")];
            tensor<string, []> x_483_pad_type_0 = const()[name = tensor<string, []>("x_483_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_483_strides_0 = const()[name = tensor<string, []>("x_483_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_483_pad_0 = const()[name = tensor<string, []>("x_483_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_483_dilations_0 = const()[name = tensor<string, []>("x_483_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_483_groups_0 = const()[name = tensor<string, []>("x_483_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_21_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(401273344))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(402059840))), name = tensor<string, []>("module_layers_21_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_483_cast_fp16 = conv(dilations = x_483_dilations_0, groups = x_483_groups_0, pad = x_483_pad_0, pad_type = x_483_pad_type_0, strides = x_483_strides_0, weight = module_layers_21_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_1145_cast_fp16)[name = tensor<string, []>("x_483_cast_fp16")];
            tensor<int32, [3]> input_1147_perm_0 = const()[name = tensor<string, []>("input_1147_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_1147_cast_fp16 = transpose(perm = input_1147_perm_0, x = x_483_cast_fp16)[name = tensor<string, []>("transpose_159")];
            tensor<fp16, [1, 188, 1024]> input_1149_cast_fp16 = add(x = input_1131_cast_fp16, y = input_1147_cast_fp16)[name = tensor<string, []>("input_1149_cast_fp16")];
            tensor<int32, [1]> input_1151_axes_0 = const()[name = tensor<string, []>("input_1151_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_21_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_21_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(402060032)))];
            tensor<fp16, [1024]> module_layers_21_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_21_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(402062144)))];
            tensor<fp16, [1, 188, 1024]> input_1151_cast_fp16 = layer_norm(axes = input_1151_axes_0, beta = module_layers_21_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_21_norm_feed_forward2_weight_to_fp16, x = input_1149_cast_fp16)[name = tensor<string, []>("input_1151_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_21_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(402064256))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(405210048))), name = tensor<string, []>("module_layers_21_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_197_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_21_feed_forward2_linear1_weight_to_fp16_palettized, x = input_1151_cast_fp16)[name = tensor<string, []>("linear_197_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_1155_cast_fp16 = silu(x = linear_197_cast_fp16)[name = tensor<string, []>("input_1155_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_21_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(405210240))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(408356032))), name = tensor<string, []>("module_layers_21_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_198_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_21_feed_forward2_linear2_weight_to_fp16_palettized, x = input_1155_cast_fp16)[name = tensor<string, []>("linear_198_cast_fp16")];
            tensor<fp16, []> var_3789_to_fp16 = const()[name = tensor<string, []>("op_3789_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3790_cast_fp16 = mul(x = linear_198_cast_fp16, y = var_3789_to_fp16)[name = tensor<string, []>("op_3790_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1161_cast_fp16 = add(x = input_1149_cast_fp16, y = var_3790_cast_fp16)[name = tensor<string, []>("input_1161_cast_fp16")];
            tensor<int32, [1]> input_1163_axes_0 = const()[name = tensor<string, []>("input_1163_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_21_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_21_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(408356224)))];
            tensor<fp16, [1024]> module_layers_21_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_21_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(408358336)))];
            tensor<fp16, [1, 188, 1024]> input_1163_cast_fp16 = layer_norm(axes = input_1163_axes_0, beta = module_layers_21_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_21_norm_out_weight_to_fp16, x = input_1161_cast_fp16)[name = tensor<string, []>("input_1163_cast_fp16")];
            tensor<int32, [1]> input_1165_axes_0 = const()[name = tensor<string, []>("input_1165_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_22_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_22_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(408360448)))];
            tensor<fp16, [1024]> module_layers_22_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_22_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(408362560)))];
            tensor<fp16, [1, 188, 1024]> input_1165_cast_fp16 = layer_norm(axes = input_1165_axes_0, beta = module_layers_22_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_22_norm_feed_forward1_weight_to_fp16, x = input_1163_cast_fp16)[name = tensor<string, []>("input_1165_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_22_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(408364672))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(411510464))), name = tensor<string, []>("module_layers_22_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_199_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_22_feed_forward1_linear1_weight_to_fp16_palettized, x = input_1165_cast_fp16)[name = tensor<string, []>("linear_199_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_1169_cast_fp16 = silu(x = linear_199_cast_fp16)[name = tensor<string, []>("input_1169_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_22_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(411510656))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(414656448))), name = tensor<string, []>("module_layers_22_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_200_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_22_feed_forward1_linear2_weight_to_fp16_palettized, x = input_1169_cast_fp16)[name = tensor<string, []>("linear_200_cast_fp16")];
            tensor<fp16, []> var_3818_to_fp16 = const()[name = tensor<string, []>("op_3818_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3819_cast_fp16 = mul(x = linear_200_cast_fp16, y = var_3818_to_fp16)[name = tensor<string, []>("op_3819_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1175_cast_fp16 = add(x = input_1163_cast_fp16, y = var_3819_cast_fp16)[name = tensor<string, []>("input_1175_cast_fp16")];
            tensor<int32, [1]> query_45_axes_0 = const()[name = tensor<string, []>("query_45_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_22_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_22_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(414656640)))];
            tensor<fp16, [1024]> module_layers_22_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_22_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(414658752)))];
            tensor<fp16, [1, 188, 1024]> query_45_cast_fp16 = layer_norm(axes = query_45_axes_0, beta = module_layers_22_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_22_norm_self_att_weight_to_fp16, x = input_1175_cast_fp16)[name = tensor<string, []>("query_45_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_22_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(414660864))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(415447360))), name = tensor<string, []>("module_layers_22_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_201_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_22_self_attn_linear_q_weight_to_fp16_palettized, x = query_45_cast_fp16)[name = tensor<string, []>("linear_201_cast_fp16")];
            tensor<int32, [4]> var_3835 = const()[name = tensor<string, []>("op_3835"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_133_cast_fp16 = reshape(shape = var_3835, x = linear_201_cast_fp16)[name = tensor<string, []>("q_133_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_22_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(415447552))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(416234048))), name = tensor<string, []>("module_layers_22_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_202_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_22_self_attn_linear_k_weight_to_fp16_palettized, x = query_45_cast_fp16)[name = tensor<string, []>("linear_202_cast_fp16")];
            tensor<int32, [4]> var_3839 = const()[name = tensor<string, []>("op_3839"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_89_cast_fp16 = reshape(shape = var_3839, x = linear_202_cast_fp16)[name = tensor<string, []>("k_89_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_22_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(416234240))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(417020736))), name = tensor<string, []>("module_layers_22_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_203_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_22_self_attn_linear_v_weight_to_fp16_palettized, x = query_45_cast_fp16)[name = tensor<string, []>("linear_203_cast_fp16")];
            tensor<int32, [4]> var_3843 = const()[name = tensor<string, []>("op_3843"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_45_cast_fp16 = reshape(shape = var_3843, x = linear_203_cast_fp16)[name = tensor<string, []>("v_45_cast_fp16")];
            tensor<int32, [4]> value_47_perm_0 = const()[name = tensor<string, []>("value_47_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_22_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_22_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(417020928)))];
            tensor<fp16, [1, 188, 8, 128]> var_3855_cast_fp16 = add(x = q_133_cast_fp16, y = module_layers_22_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_3855_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_22_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_22_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(417023040)))];
            tensor<fp16, [1, 188, 8, 128]> var_3857_cast_fp16 = add(x = q_133_cast_fp16, y = module_layers_22_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_3857_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_45_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_45_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_491_transpose_x_0 = const()[name = tensor<string, []>("x_491_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_491_transpose_y_0 = const()[name = tensor<string, []>("x_491_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_3859_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(417025152))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(417313216))), name = tensor<string, []>("op_3859_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_45_cast_fp16 = transpose(perm = q_with_bias_v_45_perm_0, x = var_3857_cast_fp16)[name = tensor<string, []>("transpose_158")];
            tensor<fp16, [1, 8, 188, 375]> x_491_cast_fp16 = matmul(transpose_x = x_491_transpose_x_0, transpose_y = x_491_transpose_y_0, x = q_with_bias_v_45_cast_fp16, y = op_3859_to_fp16_palettized)[name = tensor<string, []>("x_491_cast_fp16")];
            tensor<int32, [8]> x_493_pad_0 = const()[name = tensor<string, []>("x_493_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_493_mode_0 = const()[name = tensor<string, []>("x_493_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_234_to_fp16 = const()[name = tensor<string, []>("const_234_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_493_cast_fp16 = pad(constant_val = const_234_to_fp16, mode = x_493_mode_0, pad = x_493_pad_0, x = x_491_cast_fp16)[name = tensor<string, []>("x_493_cast_fp16")];
            tensor<int32, [4]> var_3867 = const()[name = tensor<string, []>("op_3867"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_495_cast_fp16 = reshape(shape = var_3867, x = x_493_cast_fp16)[name = tensor<string, []>("x_495_cast_fp16")];
            tensor<int32, [4]> var_3871_begin_0 = const()[name = tensor<string, []>("op_3871_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_3871_end_0 = const()[name = tensor<string, []>("op_3871_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_3871_end_mask_0 = const()[name = tensor<string, []>("op_3871_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_3871_cast_fp16 = slice_by_index(begin = var_3871_begin_0, end = var_3871_end_0, end_mask = var_3871_end_mask_0, x = x_495_cast_fp16)[name = tensor<string, []>("op_3871_cast_fp16")];
            tensor<int32, [4]> var_3872 = const()[name = tensor<string, []>("op_3872"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_89_cast_fp16 = reshape(shape = var_3872, x = var_3871_cast_fp16)[name = tensor<string, []>("matrix_bd_89_cast_fp16")];
            tensor<bool, []> matrix_ac_45_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_45_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_45_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_45_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_140_perm_0 = const()[name = tensor<string, []>("transpose_140_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_141_perm_0 = const()[name = tensor<string, []>("transpose_141_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_141 = transpose(perm = transpose_141_perm_0, x = k_89_cast_fp16)[name = tensor<string, []>("transpose_156")];
            tensor<fp16, [1, 8, 188, 128]> transpose_140 = transpose(perm = transpose_140_perm_0, x = var_3855_cast_fp16)[name = tensor<string, []>("transpose_157")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_45_cast_fp16 = matmul(transpose_x = matrix_ac_45_transpose_x_0, transpose_y = matrix_ac_45_transpose_y_0, x = transpose_140, y = transpose_141)[name = tensor<string, []>("matrix_ac_45_cast_fp16")];
            tensor<int32, [4]> matrix_bd_91_begin_0 = const()[name = tensor<string, []>("matrix_bd_91_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_91_end_0 = const()[name = tensor<string, []>("matrix_bd_91_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_91_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_91_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_91_cast_fp16 = slice_by_index(begin = matrix_bd_91_begin_0, end = matrix_bd_91_end_0, end_mask = matrix_bd_91_end_mask_0, x = matrix_bd_89_cast_fp16)[name = tensor<string, []>("matrix_bd_91_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3881_cast_fp16 = add(x = matrix_ac_45_cast_fp16, y = matrix_bd_91_cast_fp16)[name = tensor<string, []>("op_3881_cast_fp16")];
            tensor<fp16, []> _inversed_scores_89_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_89_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_89_cast_fp16 = mul(x = var_3881_cast_fp16, y = _inversed_scores_89_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_89_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_91_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_89_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_91_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_3887_cast_fp16 = softmax(axis = var_30, x = scores_91_cast_fp16)[name = tensor<string, []>("op_3887_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_1177_cast_fp16 = select(a = var_11_to_fp16, b = var_3887_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_1177_cast_fp16")];
            tensor<bool, []> x_497_transpose_x_0 = const()[name = tensor<string, []>("x_497_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_497_transpose_y_0 = const()[name = tensor<string, []>("x_497_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_47_cast_fp16 = transpose(perm = value_47_perm_0, x = v_45_cast_fp16)[name = tensor<string, []>("transpose_155")];
            tensor<fp16, [1, 8, 188, 128]> x_497_cast_fp16 = matmul(transpose_x = x_497_transpose_x_0, transpose_y = x_497_transpose_y_0, x = input_1177_cast_fp16, y = value_47_cast_fp16)[name = tensor<string, []>("x_497_cast_fp16")];
            tensor<int32, [4]> var_3891_perm_0 = const()[name = tensor<string, []>("op_3891_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_3892 = const()[name = tensor<string, []>("op_3892"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_3891_cast_fp16 = transpose(perm = var_3891_perm_0, x = x_497_cast_fp16)[name = tensor<string, []>("transpose_154")];
            tensor<fp16, [1, 188, 1024]> input_1179_cast_fp16 = reshape(shape = var_3892, x = var_3891_cast_fp16)[name = tensor<string, []>("input_1179_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_22_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(417313408))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(418099904))), name = tensor<string, []>("module_layers_22_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_205_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_22_self_attn_linear_out_weight_to_fp16_palettized, x = input_1179_cast_fp16)[name = tensor<string, []>("linear_205_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1183_cast_fp16 = add(x = input_1175_cast_fp16, y = linear_205_cast_fp16)[name = tensor<string, []>("input_1183_cast_fp16")];
            tensor<int32, [1]> x_501_axes_0 = const()[name = tensor<string, []>("x_501_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_22_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_22_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(418100096)))];
            tensor<fp16, [1024]> module_layers_22_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_22_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(418102208)))];
            tensor<fp16, [1, 188, 1024]> x_501_cast_fp16 = layer_norm(axes = x_501_axes_0, beta = module_layers_22_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_22_norm_conv_weight_to_fp16, x = input_1183_cast_fp16)[name = tensor<string, []>("x_501_cast_fp16")];
            tensor<int32, [3]> input_1185_perm_0 = const()[name = tensor<string, []>("input_1185_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_1187_pad_type_0 = const()[name = tensor<string, []>("input_1187_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_1187_strides_0 = const()[name = tensor<string, []>("input_1187_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_1187_pad_0 = const()[name = tensor<string, []>("input_1187_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_1187_dilations_0 = const()[name = tensor<string, []>("input_1187_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_1187_groups_0 = const()[name = tensor<string, []>("input_1187_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_22_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(418104320))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(419677248))), name = tensor<string, []>("module_layers_22_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_1185_cast_fp16 = transpose(perm = input_1185_perm_0, x = x_501_cast_fp16)[name = tensor<string, []>("transpose_153")];
            tensor<fp16, [1, 2048, 188]> input_1187_cast_fp16 = conv(dilations = input_1187_dilations_0, groups = input_1187_groups_0, pad = input_1187_pad_0, pad_type = input_1187_pad_type_0, strides = input_1187_strides_0, weight = module_layers_22_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_1185_cast_fp16)[name = tensor<string, []>("input_1187_cast_fp16")];
            tensor<int32, []> x_503_split_num_splits_0 = const()[name = tensor<string, []>("x_503_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_503_split_axis_0 = const()[name = tensor<string, []>("x_503_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_503_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_503_split_cast_fp16_1 = split(axis = x_503_split_axis_0, num_splits = x_503_split_num_splits_0, x = input_1187_cast_fp16)[name = tensor<string, []>("x_503_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_503_split_1_sigmoid_cast_fp16 = sigmoid(x = x_503_split_cast_fp16_1)[name = tensor<string, []>("x_503_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_503_cast_fp16 = mul(x = x_503_split_cast_fp16_0, y = x_503_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_503_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_1189_cast_fp16 = select(a = var_11_to_fp16, b = x_503_cast_fp16, cond = var_328)[name = tensor<string, []>("input_1189_cast_fp16")];
            tensor<int32, [6]> input_1191_pad_0 = const()[name = tensor<string, []>("input_1191_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_1191_mode_0 = const()[name = tensor<string, []>("input_1191_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_237_to_fp16 = const()[name = tensor<string, []>("const_237_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_1191_cast_fp16 = pad(constant_val = const_237_to_fp16, mode = input_1191_mode_0, pad = input_1191_pad_0, x = input_1189_cast_fp16)[name = tensor<string, []>("input_1191_cast_fp16")];
            tensor<string, []> input_1193_pad_type_0 = const()[name = tensor<string, []>("input_1193_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_1193_groups_0 = const()[name = tensor<string, []>("input_1193_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_1193_strides_0 = const()[name = tensor<string, []>("input_1193_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_1193_pad_0 = const()[name = tensor<string, []>("input_1193_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_1193_dilations_0 = const()[name = tensor<string, []>("input_1193_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_292_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(419677440))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(419684416))), name = tensor<string, []>("const_292_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_293_to_fp16 = const()[name = tensor<string, []>("const_293_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(419684608)))];
            tensor<fp16, [1, 1024, 188]> input_1195_cast_fp16 = conv(bias = const_293_to_fp16, dilations = input_1193_dilations_0, groups = input_1193_groups_0, pad = input_1193_pad_0, pad_type = input_1193_pad_type_0, strides = input_1193_strides_0, weight = const_292_to_fp16_palettized, x = input_1191_cast_fp16)[name = tensor<string, []>("input_1195_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_1197_cast_fp16 = silu(x = input_1195_cast_fp16)[name = tensor<string, []>("input_1197_cast_fp16")];
            tensor<string, []> x_505_pad_type_0 = const()[name = tensor<string, []>("x_505_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_505_strides_0 = const()[name = tensor<string, []>("x_505_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_505_pad_0 = const()[name = tensor<string, []>("x_505_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_505_dilations_0 = const()[name = tensor<string, []>("x_505_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_505_groups_0 = const()[name = tensor<string, []>("x_505_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_22_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(419686720))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(420473216))), name = tensor<string, []>("module_layers_22_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_505_cast_fp16 = conv(dilations = x_505_dilations_0, groups = x_505_groups_0, pad = x_505_pad_0, pad_type = x_505_pad_type_0, strides = x_505_strides_0, weight = module_layers_22_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_1197_cast_fp16)[name = tensor<string, []>("x_505_cast_fp16")];
            tensor<int32, [3]> input_1199_perm_0 = const()[name = tensor<string, []>("input_1199_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_1199_cast_fp16 = transpose(perm = input_1199_perm_0, x = x_505_cast_fp16)[name = tensor<string, []>("transpose_152")];
            tensor<fp16, [1, 188, 1024]> input_1201_cast_fp16 = add(x = input_1183_cast_fp16, y = input_1199_cast_fp16)[name = tensor<string, []>("input_1201_cast_fp16")];
            tensor<int32, [1]> input_1203_axes_0 = const()[name = tensor<string, []>("input_1203_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_22_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_22_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(420473408)))];
            tensor<fp16, [1024]> module_layers_22_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_22_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(420475520)))];
            tensor<fp16, [1, 188, 1024]> input_1203_cast_fp16 = layer_norm(axes = input_1203_axes_0, beta = module_layers_22_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_22_norm_feed_forward2_weight_to_fp16, x = input_1201_cast_fp16)[name = tensor<string, []>("input_1203_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_22_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(420477632))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(423623424))), name = tensor<string, []>("module_layers_22_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_206_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_22_feed_forward2_linear1_weight_to_fp16_palettized, x = input_1203_cast_fp16)[name = tensor<string, []>("linear_206_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_1207_cast_fp16 = silu(x = linear_206_cast_fp16)[name = tensor<string, []>("input_1207_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_22_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(423623616))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(426769408))), name = tensor<string, []>("module_layers_22_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_207_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_22_feed_forward2_linear2_weight_to_fp16_palettized, x = input_1207_cast_fp16)[name = tensor<string, []>("linear_207_cast_fp16")];
            tensor<fp16, []> var_3952_to_fp16 = const()[name = tensor<string, []>("op_3952_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3953_cast_fp16 = mul(x = linear_207_cast_fp16, y = var_3952_to_fp16)[name = tensor<string, []>("op_3953_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1213_cast_fp16 = add(x = input_1201_cast_fp16, y = var_3953_cast_fp16)[name = tensor<string, []>("input_1213_cast_fp16")];
            tensor<int32, [1]> input_1215_axes_0 = const()[name = tensor<string, []>("input_1215_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_22_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_22_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(426769600)))];
            tensor<fp16, [1024]> module_layers_22_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_22_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(426771712)))];
            tensor<fp16, [1, 188, 1024]> input_1215_cast_fp16 = layer_norm(axes = input_1215_axes_0, beta = module_layers_22_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_22_norm_out_weight_to_fp16, x = input_1213_cast_fp16)[name = tensor<string, []>("input_1215_cast_fp16")];
            tensor<int32, [1]> input_1217_axes_0 = const()[name = tensor<string, []>("input_1217_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_23_norm_feed_forward1_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_23_norm_feed_forward1_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(426773824)))];
            tensor<fp16, [1024]> module_layers_23_norm_feed_forward1_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_23_norm_feed_forward1_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(426775936)))];
            tensor<fp16, [1, 188, 1024]> input_1217_cast_fp16 = layer_norm(axes = input_1217_axes_0, beta = module_layers_23_norm_feed_forward1_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_23_norm_feed_forward1_weight_to_fp16, x = input_1215_cast_fp16)[name = tensor<string, []>("input_1217_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_23_feed_forward1_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(426778048))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(429923840))), name = tensor<string, []>("module_layers_23_feed_forward1_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_208_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_23_feed_forward1_linear1_weight_to_fp16_palettized, x = input_1217_cast_fp16)[name = tensor<string, []>("linear_208_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_1221_cast_fp16 = silu(x = linear_208_cast_fp16)[name = tensor<string, []>("input_1221_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_23_feed_forward1_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(429924032))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(433069824))), name = tensor<string, []>("module_layers_23_feed_forward1_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_209_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_23_feed_forward1_linear2_weight_to_fp16_palettized, x = input_1221_cast_fp16)[name = tensor<string, []>("linear_209_cast_fp16")];
            tensor<fp16, []> var_3981_to_fp16 = const()[name = tensor<string, []>("op_3981_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_3982_cast_fp16 = mul(x = linear_209_cast_fp16, y = var_3981_to_fp16)[name = tensor<string, []>("op_3982_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1227_cast_fp16 = add(x = input_1215_cast_fp16, y = var_3982_cast_fp16)[name = tensor<string, []>("input_1227_cast_fp16")];
            tensor<int32, [1]> query_axes_0 = const()[name = tensor<string, []>("query_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_23_norm_self_att_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_23_norm_self_att_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(433070016)))];
            tensor<fp16, [1024]> module_layers_23_norm_self_att_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_23_norm_self_att_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(433072128)))];
            tensor<fp16, [1, 188, 1024]> query_cast_fp16 = layer_norm(axes = query_axes_0, beta = module_layers_23_norm_self_att_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_23_norm_self_att_weight_to_fp16, x = input_1227_cast_fp16)[name = tensor<string, []>("query_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_23_self_attn_linear_q_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(433074240))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(433860736))), name = tensor<string, []>("module_layers_23_self_attn_linear_q_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_210_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_23_self_attn_linear_q_weight_to_fp16_palettized, x = query_cast_fp16)[name = tensor<string, []>("linear_210_cast_fp16")];
            tensor<int32, [4]> var_3998 = const()[name = tensor<string, []>("op_3998"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> q_139_cast_fp16 = reshape(shape = var_3998, x = linear_210_cast_fp16)[name = tensor<string, []>("q_139_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_23_self_attn_linear_k_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(433860928))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(434647424))), name = tensor<string, []>("module_layers_23_self_attn_linear_k_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_211_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_23_self_attn_linear_k_weight_to_fp16_palettized, x = query_cast_fp16)[name = tensor<string, []>("linear_211_cast_fp16")];
            tensor<int32, [4]> var_4002 = const()[name = tensor<string, []>("op_4002"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> k_93_cast_fp16 = reshape(shape = var_4002, x = linear_211_cast_fp16)[name = tensor<string, []>("k_93_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_23_self_attn_linear_v_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(434647616))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(435434112))), name = tensor<string, []>("module_layers_23_self_attn_linear_v_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_212_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_23_self_attn_linear_v_weight_to_fp16_palettized, x = query_cast_fp16)[name = tensor<string, []>("linear_212_cast_fp16")];
            tensor<int32, [4]> var_4006 = const()[name = tensor<string, []>("op_4006"), val = tensor<int32, [4]>([1, -1, 8, 128])];
            tensor<fp16, [1, 188, 8, 128]> v_cast_fp16 = reshape(shape = var_4006, x = linear_212_cast_fp16)[name = tensor<string, []>("v_cast_fp16")];
            tensor<int32, [4]> value_perm_0 = const()[name = tensor<string, []>("value_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [8, 128]> module_layers_23_self_attn_pos_bias_u_to_fp16 = const()[name = tensor<string, []>("module_layers_23_self_attn_pos_bias_u_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(435434304)))];
            tensor<fp16, [1, 188, 8, 128]> var_4018_cast_fp16 = add(x = q_139_cast_fp16, y = module_layers_23_self_attn_pos_bias_u_to_fp16)[name = tensor<string, []>("op_4018_cast_fp16")];
            tensor<fp16, [8, 128]> module_layers_23_self_attn_pos_bias_v_to_fp16 = const()[name = tensor<string, []>("module_layers_23_self_attn_pos_bias_v_to_fp16"), val = tensor<fp16, [8, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(435436416)))];
            tensor<fp16, [1, 188, 8, 128]> var_4020_cast_fp16 = add(x = q_139_cast_fp16, y = module_layers_23_self_attn_pos_bias_v_to_fp16)[name = tensor<string, []>("op_4020_cast_fp16")];
            tensor<int32, [4]> q_with_bias_v_perm_0 = const()[name = tensor<string, []>("q_with_bias_v_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<bool, []> x_513_transpose_x_0 = const()[name = tensor<string, []>("x_513_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_513_transpose_y_0 = const()[name = tensor<string, []>("x_513_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 128, 375]> op_4022_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [288000]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(435438528))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(435726592))), name = tensor<string, []>("op_4022_to_fp16_palettized"), shape = tensor<uint32, [4]>([1, 8, 128, 375])];
            tensor<fp16, [1, 8, 188, 128]> q_with_bias_v_cast_fp16 = transpose(perm = q_with_bias_v_perm_0, x = var_4020_cast_fp16)[name = tensor<string, []>("transpose_151")];
            tensor<fp16, [1, 8, 188, 375]> x_513_cast_fp16 = matmul(transpose_x = x_513_transpose_x_0, transpose_y = x_513_transpose_y_0, x = q_with_bias_v_cast_fp16, y = op_4022_to_fp16_palettized)[name = tensor<string, []>("x_513_cast_fp16")];
            tensor<int32, [8]> x_515_pad_0 = const()[name = tensor<string, []>("x_515_pad_0"), val = tensor<int32, [8]>([0, 0, 0, 0, 0, 0, 1, 0])];
            tensor<string, []> x_515_mode_0 = const()[name = tensor<string, []>("x_515_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_244_to_fp16 = const()[name = tensor<string, []>("const_244_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 8, 188, 376]> x_515_cast_fp16 = pad(constant_val = const_244_to_fp16, mode = x_515_mode_0, pad = x_515_pad_0, x = x_513_cast_fp16)[name = tensor<string, []>("x_515_cast_fp16")];
            tensor<int32, [4]> var_4030 = const()[name = tensor<string, []>("op_4030"), val = tensor<int32, [4]>([1, 8, -1, 188])];
            tensor<fp16, [1, 8, 376, 188]> x_517_cast_fp16 = reshape(shape = var_4030, x = x_515_cast_fp16)[name = tensor<string, []>("x_517_cast_fp16")];
            tensor<int32, [4]> var_4034_begin_0 = const()[name = tensor<string, []>("op_4034_begin_0"), val = tensor<int32, [4]>([0, 0, 1, 0])];
            tensor<int32, [4]> var_4034_end_0 = const()[name = tensor<string, []>("op_4034_end_0"), val = tensor<int32, [4]>([1, 8, 376, 188])];
            tensor<bool, [4]> var_4034_end_mask_0 = const()[name = tensor<string, []>("op_4034_end_mask_0"), val = tensor<bool, [4]>([true, true, true, true])];
            tensor<fp16, [1, 8, 375, 188]> var_4034_cast_fp16 = slice_by_index(begin = var_4034_begin_0, end = var_4034_end_0, end_mask = var_4034_end_mask_0, x = x_517_cast_fp16)[name = tensor<string, []>("op_4034_cast_fp16")];
            tensor<int32, [4]> var_4035 = const()[name = tensor<string, []>("op_4035"), val = tensor<int32, [4]>([1, 8, 188, 375])];
            tensor<fp16, [1, 8, 188, 375]> matrix_bd_93_cast_fp16 = reshape(shape = var_4035, x = var_4034_cast_fp16)[name = tensor<string, []>("matrix_bd_93_cast_fp16")];
            tensor<bool, []> matrix_ac_transpose_x_0 = const()[name = tensor<string, []>("matrix_ac_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matrix_ac_transpose_y_0 = const()[name = tensor<string, []>("matrix_ac_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_142_perm_0 = const()[name = tensor<string, []>("transpose_142_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_143_perm_0 = const()[name = tensor<string, []>("transpose_143_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 8, 128, 188]> transpose_143 = transpose(perm = transpose_143_perm_0, x = k_93_cast_fp16)[name = tensor<string, []>("transpose_149")];
            tensor<fp16, [1, 8, 188, 128]> transpose_142 = transpose(perm = transpose_142_perm_0, x = var_4018_cast_fp16)[name = tensor<string, []>("transpose_150")];
            tensor<fp16, [1, 8, 188, 188]> matrix_ac_cast_fp16 = matmul(transpose_x = matrix_ac_transpose_x_0, transpose_y = matrix_ac_transpose_y_0, x = transpose_142, y = transpose_143)[name = tensor<string, []>("matrix_ac_cast_fp16")];
            tensor<int32, [4]> matrix_bd_begin_0 = const()[name = tensor<string, []>("matrix_bd_begin_0"), val = tensor<int32, [4]>([0, 0, 0, 0])];
            tensor<int32, [4]> matrix_bd_end_0 = const()[name = tensor<string, []>("matrix_bd_end_0"), val = tensor<int32, [4]>([1, 8, 188, 188])];
            tensor<bool, [4]> matrix_bd_end_mask_0 = const()[name = tensor<string, []>("matrix_bd_end_mask_0"), val = tensor<bool, [4]>([true, true, true, false])];
            tensor<fp16, [1, 8, 188, 188]> matrix_bd_cast_fp16 = slice_by_index(begin = matrix_bd_begin_0, end = matrix_bd_end_0, end_mask = matrix_bd_end_mask_0, x = matrix_bd_93_cast_fp16)[name = tensor<string, []>("matrix_bd_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_4044_cast_fp16 = add(x = matrix_ac_cast_fp16, y = matrix_bd_cast_fp16)[name = tensor<string, []>("op_4044_cast_fp16")];
            tensor<fp16, []> _inversed_scores_93_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_scores_93_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-4)];
            tensor<fp16, [1, 8, 188, 188]> _inversed_scores_93_cast_fp16 = mul(x = var_4044_cast_fp16, y = _inversed_scores_93_y_0_to_fp16)[name = tensor<string, []>("_inversed_scores_93_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> scores_cast_fp16 = select(a = var_12_to_fp16, b = _inversed_scores_93_cast_fp16, cond = mask_3)[name = tensor<string, []>("scores_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> var_4050_cast_fp16 = softmax(axis = var_30, x = scores_cast_fp16)[name = tensor<string, []>("op_4050_cast_fp16")];
            tensor<fp16, [1, 8, 188, 188]> input_1229_cast_fp16 = select(a = var_11_to_fp16, b = var_4050_cast_fp16, cond = mask_3)[name = tensor<string, []>("input_1229_cast_fp16")];
            tensor<bool, []> x_519_transpose_x_0 = const()[name = tensor<string, []>("x_519_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> x_519_transpose_y_0 = const()[name = tensor<string, []>("x_519_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 8, 188, 128]> value_cast_fp16 = transpose(perm = value_perm_0, x = v_cast_fp16)[name = tensor<string, []>("transpose_148")];
            tensor<fp16, [1, 8, 188, 128]> x_519_cast_fp16 = matmul(transpose_x = x_519_transpose_x_0, transpose_y = x_519_transpose_y_0, x = input_1229_cast_fp16, y = value_cast_fp16)[name = tensor<string, []>("x_519_cast_fp16")];
            tensor<int32, [4]> var_4054_perm_0 = const()[name = tensor<string, []>("op_4054_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_4055 = const()[name = tensor<string, []>("op_4055"), val = tensor<int32, [3]>([1, -1, 1024])];
            tensor<fp16, [1, 188, 8, 128]> var_4054_cast_fp16 = transpose(perm = var_4054_perm_0, x = x_519_cast_fp16)[name = tensor<string, []>("transpose_147")];
            tensor<fp16, [1, 188, 1024]> input_1231_cast_fp16 = reshape(shape = var_4055, x = var_4054_cast_fp16)[name = tensor<string, []>("input_1231_cast_fp16")];
            tensor<fp16, [1024, 1024]> module_layers_23_self_attn_linear_out_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(435726784))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(436513280))), name = tensor<string, []>("module_layers_23_self_attn_linear_out_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 1024])];
            tensor<fp16, [1, 188, 1024]> linear_214_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_23_self_attn_linear_out_weight_to_fp16_palettized, x = input_1231_cast_fp16)[name = tensor<string, []>("linear_214_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_1235_cast_fp16 = add(x = input_1227_cast_fp16, y = linear_214_cast_fp16)[name = tensor<string, []>("input_1235_cast_fp16")];
            tensor<int32, [1]> x_523_axes_0 = const()[name = tensor<string, []>("x_523_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_23_norm_conv_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_23_norm_conv_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(436513472)))];
            tensor<fp16, [1024]> module_layers_23_norm_conv_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_23_norm_conv_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(436515584)))];
            tensor<fp16, [1, 188, 1024]> x_523_cast_fp16 = layer_norm(axes = x_523_axes_0, beta = module_layers_23_norm_conv_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_23_norm_conv_weight_to_fp16, x = input_1235_cast_fp16)[name = tensor<string, []>("x_523_cast_fp16")];
            tensor<int32, [3]> input_1237_perm_0 = const()[name = tensor<string, []>("input_1237_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> input_1239_pad_type_0 = const()[name = tensor<string, []>("input_1239_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> input_1239_strides_0 = const()[name = tensor<string, []>("input_1239_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_1239_pad_0 = const()[name = tensor<string, []>("input_1239_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_1239_dilations_0 = const()[name = tensor<string, []>("input_1239_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> input_1239_groups_0 = const()[name = tensor<string, []>("input_1239_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [2048, 1024, 1]> module_layers_23_conv_pointwise_conv1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [1572864]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(436517696))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(438090624))), name = tensor<string, []>("module_layers_23_conv_pointwise_conv1_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([2048, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> input_1237_cast_fp16 = transpose(perm = input_1237_perm_0, x = x_523_cast_fp16)[name = tensor<string, []>("transpose_146")];
            tensor<fp16, [1, 2048, 188]> input_1239_cast_fp16 = conv(dilations = input_1239_dilations_0, groups = input_1239_groups_0, pad = input_1239_pad_0, pad_type = input_1239_pad_type_0, strides = input_1239_strides_0, weight = module_layers_23_conv_pointwise_conv1_weight_to_fp16_palettized, x = input_1237_cast_fp16)[name = tensor<string, []>("input_1239_cast_fp16")];
            tensor<int32, []> x_525_split_num_splits_0 = const()[name = tensor<string, []>("x_525_split_num_splits_0"), val = tensor<int32, []>(2)];
            tensor<int32, []> x_525_split_axis_0 = const()[name = tensor<string, []>("x_525_split_axis_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1, 1024, 188]> x_525_split_cast_fp16_0, tensor<fp16, [1, 1024, 188]> x_525_split_cast_fp16_1 = split(axis = x_525_split_axis_0, num_splits = x_525_split_num_splits_0, x = input_1239_cast_fp16)[name = tensor<string, []>("x_525_split_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_525_split_1_sigmoid_cast_fp16 = sigmoid(x = x_525_split_cast_fp16_1)[name = tensor<string, []>("x_525_split_1_sigmoid_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> x_525_cast_fp16 = mul(x = x_525_split_cast_fp16_0, y = x_525_split_1_sigmoid_cast_fp16)[name = tensor<string, []>("x_525_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_1241_cast_fp16 = select(a = var_11_to_fp16, b = x_525_cast_fp16, cond = var_328)[name = tensor<string, []>("input_1241_cast_fp16")];
            tensor<int32, [6]> input_1243_pad_0 = const()[name = tensor<string, []>("input_1243_pad_0"), val = tensor<int32, [6]>([0, 0, 0, 0, 4, 4])];
            tensor<string, []> input_1243_mode_0 = const()[name = tensor<string, []>("input_1243_mode_0"), val = tensor<string, []>("constant")];
            tensor<fp16, []> const_247_to_fp16 = const()[name = tensor<string, []>("const_247_to_fp16"), val = tensor<fp16, []>(0x0p+0)];
            tensor<fp16, [1, 1024, 196]> input_1243_cast_fp16 = pad(constant_val = const_247_to_fp16, mode = input_1243_mode_0, pad = input_1243_pad_0, x = input_1241_cast_fp16)[name = tensor<string, []>("input_1243_cast_fp16")];
            tensor<string, []> input_1245_pad_type_0 = const()[name = tensor<string, []>("input_1245_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, []> input_1245_groups_0 = const()[name = tensor<string, []>("input_1245_groups_0"), val = tensor<int32, []>(1024)];
            tensor<int32, [1]> input_1245_strides_0 = const()[name = tensor<string, []>("input_1245_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> input_1245_pad_0 = const()[name = tensor<string, []>("input_1245_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> input_1245_dilations_0 = const()[name = tensor<string, []>("input_1245_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<fp16, [1024, 1, 9]> const_294_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [6912]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(438090816))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(438097792))), name = tensor<string, []>("const_294_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1, 9])];
            tensor<fp16, [1024]> const_295_to_fp16 = const()[name = tensor<string, []>("const_295_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(438097984)))];
            tensor<fp16, [1, 1024, 188]> input_1247_cast_fp16 = conv(bias = const_295_to_fp16, dilations = input_1245_dilations_0, groups = input_1245_groups_0, pad = input_1245_pad_0, pad_type = input_1245_pad_type_0, strides = input_1245_strides_0, weight = const_294_to_fp16_palettized, x = input_1243_cast_fp16)[name = tensor<string, []>("input_1247_cast_fp16")];
            tensor<fp16, [1, 1024, 188]> input_1249_cast_fp16 = silu(x = input_1247_cast_fp16)[name = tensor<string, []>("input_1249_cast_fp16")];
            tensor<string, []> x_527_pad_type_0 = const()[name = tensor<string, []>("x_527_pad_type_0"), val = tensor<string, []>("valid")];
            tensor<int32, [1]> x_527_strides_0 = const()[name = tensor<string, []>("x_527_strides_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [2]> x_527_pad_0 = const()[name = tensor<string, []>("x_527_pad_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [1]> x_527_dilations_0 = const()[name = tensor<string, []>("x_527_dilations_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, []> x_527_groups_0 = const()[name = tensor<string, []>("x_527_groups_0"), val = tensor<int32, []>(1)];
            tensor<fp16, [1024, 1024, 1]> module_layers_23_conv_pointwise_conv2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [786432]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(438100096))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(438886592))), name = tensor<string, []>("module_layers_23_conv_pointwise_conv2_weight_to_fp16_palettized"), shape = tensor<uint32, [3]>([1024, 1024, 1])];
            tensor<fp16, [1, 1024, 188]> x_527_cast_fp16 = conv(dilations = x_527_dilations_0, groups = x_527_groups_0, pad = x_527_pad_0, pad_type = x_527_pad_type_0, strides = x_527_strides_0, weight = module_layers_23_conv_pointwise_conv2_weight_to_fp16_palettized, x = input_1249_cast_fp16)[name = tensor<string, []>("x_527_cast_fp16")];
            tensor<int32, [3]> input_1251_perm_0 = const()[name = tensor<string, []>("input_1251_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<fp16, [1, 188, 1024]> input_1251_cast_fp16 = transpose(perm = input_1251_perm_0, x = x_527_cast_fp16)[name = tensor<string, []>("transpose_145")];
            tensor<fp16, [1, 188, 1024]> input_1253_cast_fp16 = add(x = input_1235_cast_fp16, y = input_1251_cast_fp16)[name = tensor<string, []>("input_1253_cast_fp16")];
            tensor<int32, [1]> input_1255_axes_0 = const()[name = tensor<string, []>("input_1255_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_23_norm_feed_forward2_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_23_norm_feed_forward2_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(438886784)))];
            tensor<fp16, [1024]> module_layers_23_norm_feed_forward2_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_23_norm_feed_forward2_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(438888896)))];
            tensor<fp16, [1, 188, 1024]> input_1255_cast_fp16 = layer_norm(axes = input_1255_axes_0, beta = module_layers_23_norm_feed_forward2_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_23_norm_feed_forward2_weight_to_fp16, x = input_1253_cast_fp16)[name = tensor<string, []>("input_1255_cast_fp16")];
            tensor<fp16, [4096, 1024]> module_layers_23_feed_forward2_linear1_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(438891008))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(442036800))), name = tensor<string, []>("module_layers_23_feed_forward2_linear1_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([4096, 1024])];
            tensor<fp16, [1, 188, 4096]> linear_215_cast_fp16 = linear(bias = linear_1_bias_0_to_fp16, weight = module_layers_23_feed_forward2_linear1_weight_to_fp16_palettized, x = input_1255_cast_fp16)[name = tensor<string, []>("linear_215_cast_fp16")];
            tensor<fp16, [1, 188, 4096]> input_1259_cast_fp16 = silu(x = linear_215_cast_fp16)[name = tensor<string, []>("input_1259_cast_fp16")];
            tensor<fp16, [1024, 4096]> module_layers_23_feed_forward2_linear2_weight_to_fp16_palettized = constexpr_lut_to_dense()[indices = tensor<uint8, [3145728]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(442036992))), lut = tensor<fp16, [64]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(445182784))), name = tensor<string, []>("module_layers_23_feed_forward2_linear2_weight_to_fp16_palettized"), shape = tensor<uint32, [2]>([1024, 4096])];
            tensor<fp16, [1, 188, 1024]> linear_216_cast_fp16 = linear(bias = linear_2_bias_0_to_fp16, weight = module_layers_23_feed_forward2_linear2_weight_to_fp16_palettized, x = input_1259_cast_fp16)[name = tensor<string, []>("linear_216_cast_fp16")];
            tensor<fp16, []> var_4115_to_fp16 = const()[name = tensor<string, []>("op_4115_to_fp16"), val = tensor<fp16, []>(0x1p-1)];
            tensor<fp16, [1, 188, 1024]> var_4116_cast_fp16 = mul(x = linear_216_cast_fp16, y = var_4115_to_fp16)[name = tensor<string, []>("op_4116_cast_fp16")];
            tensor<fp16, [1, 188, 1024]> input_cast_fp16 = add(x = input_1253_cast_fp16, y = var_4116_cast_fp16)[name = tensor<string, []>("input_cast_fp16")];
            tensor<int32, [1]> audio_signal_axes_0 = const()[name = tensor<string, []>("audio_signal_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [1024]> module_layers_23_norm_out_weight_to_fp16 = const()[name = tensor<string, []>("module_layers_23_norm_out_weight_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(445182976)))];
            tensor<fp16, [1024]> module_layers_23_norm_out_bias_to_fp16 = const()[name = tensor<string, []>("module_layers_23_norm_out_bias_to_fp16"), val = tensor<fp16, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(445185088)))];
            tensor<fp16, [1, 188, 1024]> audio_signal_cast_fp16 = layer_norm(axes = audio_signal_axes_0, beta = module_layers_23_norm_out_bias_to_fp16, epsilon = var_9_to_fp16, gamma = module_layers_23_norm_out_weight_to_fp16, x = input_cast_fp16)[name = tensor<string, []>("audio_signal_cast_fp16")];
            tensor<int32, [3]> obj_1_perm_0 = const()[name = tensor<string, []>("obj_1_perm_0"), val = tensor<int32, [3]>([0, 2, 1])];
            tensor<string, []> obj_1_cast_fp16_to_fp32_dtype_0 = const()[name = tensor<string, []>("obj_1_cast_fp16_to_fp32_dtype_0"), val = tensor<string, []>("fp32")];
            tensor<fp16, [1, 1024, 188]> obj_1_cast_fp16 = transpose(perm = obj_1_perm_0, x = audio_signal_cast_fp16)[name = tensor<string, []>("transpose_144")];
            tensor<fp32, [1, 1024, 188]> encoder = cast(dtype = obj_1_cast_fp16_to_fp32_dtype_0, x = obj_1_cast_fp16)[name = tensor<string, []>("cast_0")];
        } -> (encoder, encoder_length);
}